{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exact solution u = H\n",
    "def exact_solution_h(x, y, t):\n",
    "    return -torch.sin(x)*torch.sin(y)*torch.cos(t)\n",
    "\n",
    "def initial_condition_h(x, y):\n",
    "    return -torch.sin(x)*torch.sin(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exact solution p = E\n",
    "def exact_solution_e1(x, y, t):\n",
    "    return -torch.sin(x)*torch.sin(t)*torch.cos(y)\n",
    "\n",
    "def initial_condition_e1(x, y):\n",
    "    return 0.0*torch.sin(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exact solution p = E\n",
    "def exact_solution_e2(x, y, t):\n",
    "    return torch.sin(y)*torch.sin(t)*torch.cos(x)\n",
    "\n",
    "def initial_condition_e2(x, y):\n",
    "    return 0.0*torch.sin(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning number of points\n",
    "initial_pts = 200 \n",
    "left_boundary_pts = 200 \n",
    "right_boundary_pts = 200\n",
    "back_boundary_pts = 200\n",
    "front_boundary_pts = 200\n",
    "residual_pts = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of optimizer (ADAM or LBFGS)\n",
    "opt_type = \"LBFGS\"\n",
    "\n",
    "x_init = torch.rand((initial_pts,1)) # initial pts\n",
    "y_init = torch.rand((initial_pts,1))\n",
    "t_init =  0*x_init\n",
    "init =  torch.cat([x_init, y_init, t_init],1)\n",
    "h_init = initial_condition_h(init[:,0], init[:, 1]).reshape(-1, 1)\n",
    "e1_init = initial_condition_e1(init[:,0], init[:, 1]).reshape(-1, 1)\n",
    "e2_init = initial_condition_e2(init[:,0], init[:, 1]).reshape(-1, 1)\n",
    "w_init = torch.cat([h_init, e1_init, e2_init],1)\n",
    "\n",
    "\n",
    "xb_left = torch.zeros((left_boundary_pts, 1)) # left spatial boundary\n",
    "yb_left = torch.rand((left_boundary_pts, 1)) # left spatial boundary\n",
    "tb_left = torch.rand((left_boundary_pts, 1)) # \n",
    "b_left = torch.cat([xb_left, yb_left, tb_left ],1)\n",
    "h_b_l = exact_solution_h(xb_left, yb_left, tb_left).reshape(-1, 1)\n",
    "e1_b_l = exact_solution_e1(xb_left, yb_left, tb_left).reshape(-1, 1)\n",
    "e2_b_l = exact_solution_e2(xb_left, yb_left, tb_left).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "xb_right = torch.ones((right_boundary_pts, 1)) # right spatial boundary\n",
    "yb_right = torch.rand((right_boundary_pts, 1)) # right spatial boundary\n",
    "tb_right = torch.rand((right_boundary_pts, 1)) # right boundary pts\n",
    "b_right = torch.cat([xb_right, yb_right, tb_right ],1)\n",
    "h_b_r = exact_solution_h(xb_right, yb_right, tb_right).reshape(-1, 1)\n",
    "e1_b_r = exact_solution_e1(xb_right, yb_right, tb_right).reshape(-1, 1)\n",
    "e2_b_r = exact_solution_e2(xb_right, yb_right, tb_right).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "xb_front = torch.rand((front_boundary_pts, 1)) # front spatial boundary\n",
    "yb_front = torch.zeros((front_boundary_pts, 1)) # front spatial boundary\n",
    "tb_front = torch.rand((front_boundary_pts, 1)) # \n",
    "b_front = torch.cat([xb_front, yb_front, tb_front ],1)\n",
    "h_b_f = exact_solution_h(xb_front, yb_front, tb_front).reshape(-1, 1)\n",
    "e1_b_f = exact_solution_e1(xb_front, yb_front, tb_front).reshape(-1, 1)\n",
    "e2_b_f = exact_solution_e2(xb_front, yb_front, tb_front).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "xb_back = torch.rand((back_boundary_pts, 1)) # back spatial boundary\n",
    "yb_back = torch.ones((back_boundary_pts, 1)) # back spatial boundary\n",
    "tb_back = torch.rand((back_boundary_pts, 1)) # back boundary pts\n",
    "b_back = torch.cat([xb_back, yb_back, tb_back ],1)\n",
    "h_b_b = exact_solution_h(xb_back, yb_back, tb_back).reshape(-1, 1)\n",
    "e1_b_b = exact_solution_e1(xb_back, yb_back, tb_back).reshape(-1, 1)\n",
    "e2_b_b = exact_solution_e2(xb_back, yb_back, tb_back).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "x_interior = torch.rand((residual_pts, 1))\n",
    "y_interior = torch.rand((residual_pts, 1))\n",
    "t_interior = torch.rand((residual_pts, 1))\n",
    "interior = torch.cat([x_interior, y_interior, t_interior],1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = DataLoader(torch.utils.data.TensorDataset(init, w_init, b_left,  b_right, b_front, b_back), batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, n_hidden_layers, neurons):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer \n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers \n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function \n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers)])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network \n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "my_network = NeuralNet(input_dimension = init.shape[1], output_dimension = w_init.shape[1], n_hidden_layers=4, neurons=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_xavier(model, retrain_seed):\n",
    "    torch.manual_seed(retrain_seed)\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "            g = nn.init.calculate_gain('tanh')\n",
    "            torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "            #torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "            m.bias.data.fill_(0)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "# Random Seed for weight initialization\n",
    "retrain = 128\n",
    "# Xavier weight initialization\n",
    "init_xavier(my_network, retrain)\n",
    "#print(my_network(init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt_type == \"ADAM\":\n",
    "    optimizer_ = optim.Adam(my_network.parameters(), lr=0.001)\n",
    "elif opt_type == \"LBFGS\":\n",
    "    optimizer_ = optim.LBFGS(my_network.parameters(), lr=0.1, max_iter=1, max_eval=50000, tolerance_change=1.0 * np.finfo(float).eps)\n",
    "else:\n",
    "    raise ValueError(\"Optimizer not recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, training_set, interior, num_epochs, optimizer, p, verbose=True):\n",
    "    history = list()\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "        running_loss = list([0])\n",
    "        \n",
    "        # Loop over batches\n",
    "        for j, (initial, w_initial, bd_left,  bd_right, bd_front, bd_back) in enumerate(training_set):\n",
    "            \n",
    "            def closure():\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # for initial\n",
    "                initial.requires_grad = True\n",
    "                w_initial_pred_ = model(initial)\n",
    "                h_initial_pred_ = w_initial_pred_[:,0].reshape(-1,1)\n",
    "                e1_initial_pred_ = w_initial_pred_[:,1].reshape(-1,1)\n",
    "                e2_initial_pred_ = w_initial_pred_[:,2].reshape(-1,1)\n",
    "                \n",
    "                \n",
    "                # with derivative\n",
    "                inpu = torch.ones(initial_pts, 1 )\n",
    "                \n",
    "                grad_h_ini = torch.autograd.grad(h_initial_pred_, initial, grad_outputs=inpu, create_graph=True, allow_unused=True)[0]\n",
    "                h_initial_t = grad_h_ini[:, 2]\n",
    "                \n",
    "                grad_e1_ini = torch.autograd.grad(e1_initial_pred_, initial, grad_outputs=inpu, create_graph=True, allow_unused=True)[0]\n",
    "                e1_initial_t = grad_e1_ini[:, 2]\n",
    "                \n",
    "                grad_e2_ini = torch.autograd.grad(e2_initial_pred_, initial, grad_outputs=inpu, create_graph=True, allow_unused=True)[0]\n",
    "                e2_initial_t = grad_e2_ini[:, 2]\n",
    "                \n",
    "                \n",
    "                \n",
    "                # for left boundary\n",
    "                w_bd_left_pred_ = model(bd_left)\n",
    "                h_bd_left_pred_ = w_bd_left_pred_[:,0].reshape(-1,1)\n",
    "                e1_bd_left_pred_ = w_bd_left_pred_[:,1].reshape(-1,1)\n",
    "                e2_bd_left_pred_ = w_bd_left_pred_[:,2].reshape(-1,1)\n",
    "                \n",
    "                # for right boundary\n",
    "                w_bd_right_pred_ = model(bd_right)\n",
    "                h_bd_right_pred_ = w_bd_right_pred_[:,0].reshape(-1,1)\n",
    "                e1_bd_right_pred_ = w_bd_right_pred_[:,1].reshape(-1,1)\n",
    "                e2_bd_right_pred_ = w_bd_right_pred_[:,2].reshape(-1,1)\n",
    "                \n",
    "                # for front boundary\n",
    "                w_bd_front_pred_ = model(bd_front)\n",
    "                h_bd_front_pred_ = w_bd_front_pred_[:,0].reshape(-1,1)\n",
    "                e1_bd_front_pred_ = w_bd_front_pred_[:,1].reshape(-1,1)\n",
    "                e2_bd_front_pred_ = w_bd_front_pred_[:,2].reshape(-1,1)\n",
    "                \n",
    "                # for back boundary\n",
    "                w_bd_back_pred_ = model(bd_back)\n",
    "                h_bd_back_pred_ = w_bd_back_pred_[:,0].reshape(-1,1)\n",
    "                e1_bd_back_pred_ = w_bd_back_pred_[:,1].reshape(-1,1)\n",
    "                e2_bd_back_pred_ = w_bd_back_pred_[:,2].reshape(-1,1)\n",
    "                \n",
    "                # residual calculation\n",
    "                interior.requires_grad = True\n",
    "                w_hat = model(interior)\n",
    "                h_hat = w_hat[:,0].reshape(-1,1)\n",
    "                e1_hat = w_hat[:,1].reshape(-1,1)\n",
    "                e2_hat = w_hat[:,2].reshape(-1,1)\n",
    "                \n",
    "                inputs = torch.ones(residual_pts, 1 )\n",
    "                inputs2 = torch.ones(residual_pts, 1)\n",
    "                \n",
    "                grad_h_hat = torch.autograd.grad(h_hat.reshape(-1,1), interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                h_x = grad_h_hat[:, 0].reshape(-1,1)\n",
    "                h_y = grad_h_hat[:, 1].reshape(-1,1)\n",
    "                \n",
    "                grad_e1_hat = torch.autograd.grad(e1_hat, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                e1_x = grad_e1_hat[:, 0].reshape(-1,1)\n",
    "                e1_y = grad_e1_hat[:, 1].reshape(-1,1)\n",
    "                \n",
    "                grad_e2_hat = torch.autograd.grad(e2_hat, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                e2_x = grad_e2_hat[:, 0].reshape(-1,1)\n",
    "                e2_y = grad_e2_hat[:, 1].reshape(-1,1)\n",
    "                \n",
    "                h_t = grad_h_hat[:, 2].reshape(-1,1)\n",
    "                e1_t = grad_e1_hat[:, 2].reshape(-1,1)\n",
    "                e2_t = grad_e2_hat[:, 2].reshape(-1,1)\n",
    "                \n",
    "                \n",
    "                # Item 1. below\n",
    "                loss1 = torch.mean((h_initial_pred_.reshape(-1, ) - w_initial[:,0].reshape(-1, ))**p) + torch.mean((2*h_t.reshape(-1, ) + e2_x.reshape(-1, ) - e1_y.reshape(-1, ))**p)+torch.mean((h_bd_left_pred_.reshape(-1,)- h_b_l.reshape(-1,))**p) + torch.mean((h_bd_right_pred_.reshape(-1,)- h_b_r.reshape(-1,))**p) +torch.mean((h_bd_front_pred_.reshape(-1,)- h_b_f.reshape(-1,))**p) + torch.mean((h_bd_back_pred_.reshape(-1,)- h_b_b.reshape(-1,))**p)\n",
    "                loss2 = torch.mean((e1_initial_pred_.reshape(-1, ) - w_initial[:,1].reshape(-1, ))**p)+ torch.mean((3*e1_t.reshape(-1, )  - h_y.reshape(-1, ) + 2*torch.sin(interior[:, 0])*torch.cos(interior[:, 2])*torch.cos(interior[:, 1]))**p) +torch.mean((e1_bd_left_pred_.reshape(-1,)- e1_b_l.reshape(-1,))**p) + torch.mean((e1_bd_right_pred_.reshape(-1,)- e1_b_r.reshape(-1,))**p) +torch.mean((e1_bd_front_pred_.reshape(-1,)- e1_b_f.reshape(-1,))**p) + torch.mean((e1_bd_back_pred_.reshape(-1,)- e1_b_b.reshape(-1,))**p)\n",
    "                loss3 = torch.mean((e2_initial_pred_.reshape(-1, ) - w_initial[:,2].reshape(-1, ))**p)+ torch.mean((2*e2_t.reshape(-1, )  + h_x.reshape(-1, ) - torch.cos(interior[:, 0])*torch.cos(interior[:, 2])*torch.sin(interior[:, 1]))**p) +torch.mean((e2_bd_left_pred_.reshape(-1,)- e2_b_l.reshape(-1,))**p) + torch.mean((e2_bd_right_pred_.reshape(-1,)- e2_b_r.reshape(-1,))**p) +torch.mean((e2_bd_front_pred_.reshape(-1,)- e2_b_f.reshape(-1,))**p) + torch.mean((e2_bd_back_pred_.reshape(-1,)- e2_b_b.reshape(-1,))**p)\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                \n",
    "                # Item 2. below\n",
    "                loss.backward()\n",
    "                # Compute average training loss over batches for the current epoch\n",
    "                running_loss[0] += loss.item()\n",
    "                return loss\n",
    "            \n",
    "            # Item 3. below\n",
    "            optimizer.step(closure=closure)\n",
    "            \n",
    "        print('Loss: ', (running_loss[0] / len(training_set)))\n",
    "        history.append(running_loss[0])\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  0  ################################\n",
      "Loss:  99.23579406738281\n",
      "################################  1  ################################\n",
      "Loss:  97.35958099365234\n",
      "################################  2  ################################\n",
      "Loss:  83.40977478027344\n",
      "################################  3  ################################\n",
      "Loss:  68.36258697509766\n",
      "################################  4  ################################\n",
      "Loss:  52.6787109375\n",
      "################################  5  ################################\n",
      "Loss:  39.63242721557617\n",
      "################################  6  ################################\n",
      "Loss:  32.06940460205078\n",
      "################################  7  ################################\n",
      "Loss:  27.531774520874023\n",
      "################################  8  ################################\n",
      "Loss:  24.084901809692383\n",
      "################################  9  ################################\n",
      "Loss:  21.137191772460938\n",
      "################################  10  ################################\n",
      "Loss:  18.513675689697266\n",
      "################################  11  ################################\n",
      "Loss:  16.156354904174805\n",
      "################################  12  ################################\n",
      "Loss:  14.047198295593262\n",
      "################################  13  ################################\n",
      "Loss:  12.177654266357422\n",
      "################################  14  ################################\n",
      "Loss:  10.534926414489746\n",
      "################################  15  ################################\n",
      "Loss:  9.099468231201172\n",
      "################################  16  ################################\n",
      "Loss:  7.848513603210449\n",
      "################################  17  ################################\n",
      "Loss:  6.760356903076172\n",
      "################################  18  ################################\n",
      "Loss:  5.81638765335083\n",
      "################################  19  ################################\n",
      "Loss:  5.0009660720825195\n",
      "################################  20  ################################\n",
      "Loss:  4.300351142883301\n",
      "################################  21  ################################\n",
      "Loss:  3.7017765045166016\n",
      "################################  22  ################################\n",
      "Loss:  3.1930878162384033\n",
      "################################  23  ################################\n",
      "Loss:  2.76274037361145\n",
      "################################  24  ################################\n",
      "Loss:  2.399954080581665\n",
      "################################  25  ################################\n",
      "Loss:  2.0947062969207764\n",
      "################################  26  ################################\n",
      "Loss:  1.837664246559143\n",
      "################################  27  ################################\n",
      "Loss:  1.6202112436294556\n",
      "################################  28  ################################\n",
      "Loss:  1.4346811771392822\n",
      "################################  29  ################################\n",
      "Loss:  1.2746682167053223\n",
      "################################  30  ################################\n",
      "Loss:  1.1351821422576904\n",
      "################################  31  ################################\n",
      "Loss:  1.0125789642333984\n",
      "################################  32  ################################\n",
      "Loss:  0.9030930995941162\n",
      "################################  33  ################################\n",
      "Loss:  0.7878071069717407\n",
      "################################  34  ################################\n",
      "Loss:  0.7006081342697144\n",
      "################################  35  ################################\n",
      "Loss:  0.6356480121612549\n",
      "################################  36  ################################\n",
      "Loss:  0.567438006401062\n",
      "################################  37  ################################\n",
      "Loss:  0.5190051794052124\n",
      "################################  38  ################################\n",
      "Loss:  0.477011501789093\n",
      "################################  39  ################################\n",
      "Loss:  0.43280231952667236\n",
      "################################  40  ################################\n",
      "Loss:  0.39621007442474365\n",
      "################################  41  ################################\n",
      "Loss:  0.3659975826740265\n",
      "################################  42  ################################\n",
      "Loss:  0.3345470428466797\n",
      "################################  43  ################################\n",
      "Loss:  0.30978819727897644\n",
      "################################  44  ################################\n",
      "Loss:  0.2884771525859833\n",
      "################################  45  ################################\n",
      "Loss:  0.2670381963253021\n",
      "################################  46  ################################\n",
      "Loss:  0.24890635907649994\n",
      "################################  47  ################################\n",
      "Loss:  0.23420041799545288\n",
      "################################  48  ################################\n",
      "Loss:  0.21758517622947693\n",
      "################################  49  ################################\n",
      "Loss:  0.20564207434654236\n",
      "################################  50  ################################\n",
      "Loss:  0.1924086958169937\n",
      "################################  51  ################################\n",
      "Loss:  0.17711110413074493\n",
      "################################  52  ################################\n",
      "Loss:  0.16265679895877838\n",
      "################################  53  ################################\n",
      "Loss:  0.15077416598796844\n",
      "################################  54  ################################\n",
      "Loss:  0.13979209959506989\n",
      "################################  55  ################################\n",
      "Loss:  0.1294613629579544\n",
      "################################  56  ################################\n",
      "Loss:  0.12016218900680542\n",
      "################################  57  ################################\n",
      "Loss:  0.11159048974514008\n",
      "################################  58  ################################\n",
      "Loss:  0.10372915118932724\n",
      "################################  59  ################################\n",
      "Loss:  0.09680942445993423\n",
      "################################  60  ################################\n",
      "Loss:  0.09032336622476578\n",
      "################################  61  ################################\n",
      "Loss:  0.08472667634487152\n",
      "################################  62  ################################\n",
      "Loss:  0.07920839637517929\n",
      "################################  63  ################################\n",
      "Loss:  0.0738915205001831\n",
      "################################  64  ################################\n",
      "Loss:  0.0689992681145668\n",
      "################################  65  ################################\n",
      "Loss:  0.06439349800348282\n",
      "################################  66  ################################\n",
      "Loss:  0.060278892517089844\n",
      "################################  67  ################################\n",
      "Loss:  0.05642973259091377\n",
      "################################  68  ################################\n",
      "Loss:  0.05315440893173218\n",
      "################################  69  ################################\n",
      "Loss:  0.049995504319667816\n",
      "################################  70  ################################\n",
      "Loss:  0.04700826108455658\n",
      "################################  71  ################################\n",
      "Loss:  0.04422445595264435\n",
      "################################  72  ################################\n",
      "Loss:  0.041638001799583435\n",
      "################################  73  ################################\n",
      "Loss:  0.03927398845553398\n",
      "################################  74  ################################\n",
      "Loss:  0.037052541971206665\n",
      "################################  75  ################################\n",
      "Loss:  0.03497379273176193\n",
      "################################  76  ################################\n",
      "Loss:  0.032987408339977264\n",
      "################################  77  ################################\n",
      "Loss:  0.031049419194459915\n",
      "################################  78  ################################\n",
      "Loss:  0.029216479510068893\n",
      "################################  79  ################################\n",
      "Loss:  0.027441907674074173\n",
      "################################  80  ################################\n",
      "Loss:  0.02587454579770565\n",
      "################################  81  ################################\n",
      "Loss:  0.024358823895454407\n",
      "################################  82  ################################\n",
      "Loss:  0.023016663268208504\n",
      "################################  83  ################################\n",
      "Loss:  0.02174924686551094\n",
      "################################  84  ################################\n",
      "Loss:  0.020563989877700806\n",
      "################################  85  ################################\n",
      "Loss:  0.019470034167170525\n",
      "################################  86  ################################\n",
      "Loss:  0.018398480489850044\n",
      "################################  87  ################################\n",
      "Loss:  0.017453061416745186\n",
      "################################  88  ################################\n",
      "Loss:  0.016479454934597015\n",
      "################################  89  ################################\n",
      "Loss:  0.015682483091950417\n",
      "################################  90  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.01483091339468956\n",
      "################################  91  ################################\n",
      "Loss:  0.01407969556748867\n",
      "################################  92  ################################\n",
      "Loss:  0.013355374336242676\n",
      "################################  93  ################################\n",
      "Loss:  0.012695414945483208\n",
      "################################  94  ################################\n",
      "Loss:  0.012096638791263103\n",
      "################################  95  ################################\n",
      "Loss:  0.011541255749762058\n",
      "################################  96  ################################\n",
      "Loss:  0.011041149497032166\n",
      "################################  97  ################################\n",
      "Loss:  0.010570930317044258\n",
      "################################  98  ################################\n",
      "Loss:  0.010146447457373142\n",
      "################################  99  ################################\n",
      "Loss:  0.009759914129972458\n",
      "################################  100  ################################\n",
      "Loss:  0.009400926530361176\n",
      "################################  101  ################################\n",
      "Loss:  0.009075121954083443\n",
      "################################  102  ################################\n",
      "Loss:  0.008776647970080376\n",
      "################################  103  ################################\n",
      "Loss:  0.008502744138240814\n",
      "################################  104  ################################\n",
      "Loss:  0.008236881345510483\n",
      "################################  105  ################################\n",
      "Loss:  0.0079986322671175\n",
      "################################  106  ################################\n",
      "Loss:  0.007763182744383812\n",
      "################################  107  ################################\n",
      "Loss:  0.007535719312727451\n",
      "################################  108  ################################\n",
      "Loss:  0.00729817571118474\n",
      "################################  109  ################################\n",
      "Loss:  0.007079527713358402\n",
      "################################  110  ################################\n",
      "Loss:  0.006840393412858248\n",
      "################################  111  ################################\n",
      "Loss:  0.006629642564803362\n",
      "################################  112  ################################\n",
      "Loss:  0.0064148250967264175\n",
      "################################  113  ################################\n",
      "Loss:  0.006208441220223904\n",
      "################################  114  ################################\n",
      "Loss:  0.006017610430717468\n",
      "################################  115  ################################\n",
      "Loss:  0.005834117531776428\n",
      "################################  116  ################################\n",
      "Loss:  0.005667270161211491\n",
      "################################  117  ################################\n",
      "Loss:  0.00550444982945919\n",
      "################################  118  ################################\n",
      "Loss:  0.005349335260689259\n",
      "################################  119  ################################\n",
      "Loss:  0.005200596526265144\n",
      "################################  120  ################################\n",
      "Loss:  0.005053268745541573\n",
      "################################  121  ################################\n",
      "Loss:  0.004914187826216221\n",
      "################################  122  ################################\n",
      "Loss:  0.00478128669783473\n",
      "################################  123  ################################\n",
      "Loss:  0.004654675722122192\n",
      "################################  124  ################################\n",
      "Loss:  0.004538055509328842\n",
      "################################  125  ################################\n",
      "Loss:  0.004425378050655127\n",
      "################################  126  ################################\n",
      "Loss:  0.0043212478049099445\n",
      "################################  127  ################################\n",
      "Loss:  0.004224163480103016\n",
      "################################  128  ################################\n",
      "Loss:  0.004132115747779608\n",
      "################################  129  ################################\n",
      "Loss:  0.004048856906592846\n",
      "################################  130  ################################\n",
      "Loss:  0.0039711203426122665\n",
      "################################  131  ################################\n",
      "Loss:  0.0038989349268376827\n",
      "################################  132  ################################\n",
      "Loss:  0.0038312466349452734\n",
      "################################  133  ################################\n",
      "Loss:  0.0037663672119379044\n",
      "################################  134  ################################\n",
      "Loss:  0.003704556729644537\n",
      "################################  135  ################################\n",
      "Loss:  0.0036440351977944374\n",
      "################################  136  ################################\n",
      "Loss:  0.0035853807348757982\n",
      "################################  137  ################################\n",
      "Loss:  0.003529720474034548\n",
      "################################  138  ################################\n",
      "Loss:  0.0034773049410432577\n",
      "################################  139  ################################\n",
      "Loss:  0.0034301946870982647\n",
      "################################  140  ################################\n",
      "Loss:  0.0033854814246296883\n",
      "################################  141  ################################\n",
      "Loss:  0.0033441493287682533\n",
      "################################  142  ################################\n",
      "Loss:  0.00330474809743464\n",
      "################################  143  ################################\n",
      "Loss:  0.0032682998571544886\n",
      "################################  144  ################################\n",
      "Loss:  0.003234341973438859\n",
      "################################  145  ################################\n",
      "Loss:  0.0031994555611163378\n",
      "################################  146  ################################\n",
      "Loss:  0.0031662331894040108\n",
      "################################  147  ################################\n",
      "Loss:  0.0031298561953008175\n",
      "################################  148  ################################\n",
      "Loss:  0.003093151841312647\n",
      "################################  149  ################################\n",
      "Loss:  0.003054266329854727\n",
      "################################  150  ################################\n",
      "Loss:  0.003013721900060773\n",
      "################################  151  ################################\n",
      "Loss:  0.0029743225313723087\n",
      "################################  152  ################################\n",
      "Loss:  0.0029312490951269865\n",
      "################################  153  ################################\n",
      "Loss:  0.0028913847636431456\n",
      "################################  154  ################################\n",
      "Loss:  0.0028491481207311153\n",
      "################################  155  ################################\n",
      "Loss:  0.0028082183562219143\n",
      "################################  156  ################################\n",
      "Loss:  0.0027616764418780804\n",
      "################################  157  ################################\n",
      "Loss:  0.0027132886461913586\n",
      "################################  158  ################################\n",
      "Loss:  0.002660205587744713\n",
      "################################  159  ################################\n",
      "Loss:  0.002614980563521385\n",
      "################################  160  ################################\n",
      "Loss:  0.002567413728684187\n",
      "################################  161  ################################\n",
      "Loss:  0.0025222362019121647\n",
      "################################  162  ################################\n",
      "Loss:  0.0024813925847411156\n",
      "################################  163  ################################\n",
      "Loss:  0.0024431697092950344\n",
      "################################  164  ################################\n",
      "Loss:  0.0024090853985399008\n",
      "################################  165  ################################\n",
      "Loss:  0.002378131728619337\n",
      "################################  166  ################################\n",
      "Loss:  0.002348653506487608\n",
      "################################  167  ################################\n",
      "Loss:  0.002321989508345723\n",
      "################################  168  ################################\n",
      "Loss:  0.0022969909477978945\n",
      "################################  169  ################################\n",
      "Loss:  0.002273521851748228\n",
      "################################  170  ################################\n",
      "Loss:  0.0022516914177685976\n",
      "################################  171  ################################\n",
      "Loss:  0.002230618614703417\n",
      "################################  172  ################################\n",
      "Loss:  0.002211411017924547\n",
      "################################  173  ################################\n",
      "Loss:  0.002192386193200946\n",
      "################################  174  ################################\n",
      "Loss:  0.0021746524143964052\n",
      "################################  175  ################################\n",
      "Loss:  0.002155385445803404\n",
      "################################  176  ################################\n",
      "Loss:  0.002135429996997118\n",
      "################################  177  ################################\n",
      "Loss:  0.0021098260767757893\n",
      "################################  178  ################################\n",
      "Loss:  0.0020840063225477934\n",
      "################################  179  ################################\n",
      "Loss:  0.0020494526252150536\n",
      "################################  180  ################################\n",
      "Loss:  0.0020196179393678904\n",
      "################################  181  ################################\n",
      "Loss:  0.001984636764973402\n",
      "################################  182  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0019565210677683353\n",
      "################################  183  ################################\n",
      "Loss:  0.0019244318827986717\n",
      "################################  184  ################################\n",
      "Loss:  0.001889770501293242\n",
      "################################  185  ################################\n",
      "Loss:  0.0018558581359684467\n",
      "################################  186  ################################\n",
      "Loss:  0.0018209712579846382\n",
      "################################  187  ################################\n",
      "Loss:  0.0017891887109726667\n",
      "################################  188  ################################\n",
      "Loss:  0.001758954836986959\n",
      "################################  189  ################################\n",
      "Loss:  0.0017291014082729816\n",
      "################################  190  ################################\n",
      "Loss:  0.0017002111999318004\n",
      "################################  191  ################################\n",
      "Loss:  0.0016738285776227713\n",
      "################################  192  ################################\n",
      "Loss:  0.0016506591346114874\n",
      "################################  193  ################################\n",
      "Loss:  0.0016296780668199062\n",
      "################################  194  ################################\n",
      "Loss:  0.0016079635825008154\n",
      "################################  195  ################################\n",
      "Loss:  0.0015882542356848717\n",
      "################################  196  ################################\n",
      "Loss:  0.001567628001794219\n",
      "################################  197  ################################\n",
      "Loss:  0.0015478949062526226\n",
      "################################  198  ################################\n",
      "Loss:  0.001527660177089274\n",
      "################################  199  ################################\n",
      "Loss:  0.0015085870400071144\n",
      "################################  200  ################################\n",
      "Loss:  0.0014898843364790082\n",
      "################################  201  ################################\n",
      "Loss:  0.0014709329698234797\n",
      "################################  202  ################################\n",
      "Loss:  0.001452830620110035\n",
      "################################  203  ################################\n",
      "Loss:  0.0014353643637150526\n",
      "################################  204  ################################\n",
      "Loss:  0.0014182273298501968\n",
      "################################  205  ################################\n",
      "Loss:  0.0014016625937074423\n",
      "################################  206  ################################\n",
      "Loss:  0.001385103678330779\n",
      "################################  207  ################################\n",
      "Loss:  0.0013690112391486764\n",
      "################################  208  ################################\n",
      "Loss:  0.0013528300914913416\n",
      "################################  209  ################################\n",
      "Loss:  0.0013372472021728754\n",
      "################################  210  ################################\n",
      "Loss:  0.0013217490632086992\n",
      "################################  211  ################################\n",
      "Loss:  0.001306604710407555\n",
      "################################  212  ################################\n",
      "Loss:  0.0012915822444483638\n",
      "################################  213  ################################\n",
      "Loss:  0.00127689098007977\n",
      "################################  214  ################################\n",
      "Loss:  0.001263048849068582\n",
      "################################  215  ################################\n",
      "Loss:  0.0012491533998399973\n",
      "################################  216  ################################\n",
      "Loss:  0.0012359702959656715\n",
      "################################  217  ################################\n",
      "Loss:  0.0012231889413669705\n",
      "################################  218  ################################\n",
      "Loss:  0.0012109667295590043\n",
      "################################  219  ################################\n",
      "Loss:  0.001199136022478342\n",
      "################################  220  ################################\n",
      "Loss:  0.0011876749340444803\n",
      "################################  221  ################################\n",
      "Loss:  0.001176929916255176\n",
      "################################  222  ################################\n",
      "Loss:  0.0011664985213428736\n",
      "################################  223  ################################\n",
      "Loss:  0.0011567389592528343\n",
      "################################  224  ################################\n",
      "Loss:  0.0011474192142486572\n",
      "################################  225  ################################\n",
      "Loss:  0.001138054532930255\n",
      "################################  226  ################################\n",
      "Loss:  0.0011294252471998334\n",
      "################################  227  ################################\n",
      "Loss:  0.001120916334912181\n",
      "################################  228  ################################\n",
      "Loss:  0.0011128219775855541\n",
      "################################  229  ################################\n",
      "Loss:  0.0011049531167373061\n",
      "################################  230  ################################\n",
      "Loss:  0.0010968496790155768\n",
      "################################  231  ################################\n",
      "Loss:  0.0010890017729252577\n",
      "################################  232  ################################\n",
      "Loss:  0.0010810515377670527\n",
      "################################  233  ################################\n",
      "Loss:  0.001073432620614767\n",
      "################################  234  ################################\n",
      "Loss:  0.0010656961239874363\n",
      "################################  235  ################################\n",
      "Loss:  0.0010581669630482793\n",
      "################################  236  ################################\n",
      "Loss:  0.001050547230988741\n",
      "################################  237  ################################\n",
      "Loss:  0.001042582094669342\n",
      "################################  238  ################################\n",
      "Loss:  0.001034645945765078\n",
      "################################  239  ################################\n",
      "Loss:  0.001026574638672173\n",
      "################################  240  ################################\n",
      "Loss:  0.0010186847066506743\n",
      "################################  241  ################################\n",
      "Loss:  0.0010114291217178106\n",
      "################################  242  ################################\n",
      "Loss:  0.0010041960049420595\n",
      "################################  243  ################################\n",
      "Loss:  0.0009973086416721344\n",
      "################################  244  ################################\n",
      "Loss:  0.0009903829777613282\n",
      "################################  245  ################################\n",
      "Loss:  0.0009835049277171493\n",
      "################################  246  ################################\n",
      "Loss:  0.0009765869472175837\n",
      "################################  247  ################################\n",
      "Loss:  0.0009695711778476834\n",
      "################################  248  ################################\n",
      "Loss:  0.0009625803795643151\n",
      "################################  249  ################################\n",
      "Loss:  0.0009557114681228995\n",
      "################################  250  ################################\n",
      "Loss:  0.0009491845848970115\n",
      "################################  251  ################################\n",
      "Loss:  0.0009424285963177681\n",
      "################################  252  ################################\n",
      "Loss:  0.0009359124815091491\n",
      "################################  253  ################################\n",
      "Loss:  0.0009294810588471591\n",
      "################################  254  ################################\n",
      "Loss:  0.0009229957358911633\n",
      "################################  255  ################################\n",
      "Loss:  0.0009170086705125868\n",
      "################################  256  ################################\n",
      "Loss:  0.0009106779471039772\n",
      "################################  257  ################################\n",
      "Loss:  0.0009052561363205314\n",
      "################################  258  ################################\n",
      "Loss:  0.000897591351531446\n",
      "################################  259  ################################\n",
      "Loss:  0.0008916961960494518\n",
      "################################  260  ################################\n",
      "Loss:  0.0008854786865413189\n",
      "################################  261  ################################\n",
      "Loss:  0.0008773287991061807\n",
      "################################  262  ################################\n",
      "Loss:  0.0008685027132742107\n",
      "################################  263  ################################\n",
      "Loss:  0.00085779232904315\n",
      "################################  264  ################################\n",
      "Loss:  0.0008475188515149057\n",
      "################################  265  ################################\n",
      "Loss:  0.000837401719763875\n",
      "################################  266  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0008271568804048002\n",
      "################################  267  ################################\n",
      "Loss:  0.000817072403151542\n",
      "################################  268  ################################\n",
      "Loss:  0.0008074887446127832\n",
      "################################  269  ################################\n",
      "Loss:  0.0007980220834724605\n",
      "################################  270  ################################\n",
      "Loss:  0.0007890711422078311\n",
      "################################  271  ################################\n",
      "Loss:  0.0007803854532539845\n",
      "################################  272  ################################\n",
      "Loss:  0.000772117346059531\n",
      "################################  273  ################################\n",
      "Loss:  0.0007643455173820257\n",
      "################################  274  ################################\n",
      "Loss:  0.0007576111238449812\n",
      "################################  275  ################################\n",
      "Loss:  0.0007515241159126163\n",
      "################################  276  ################################\n",
      "Loss:  0.0007457772153429687\n",
      "################################  277  ################################\n",
      "Loss:  0.0007404788630083203\n",
      "################################  278  ################################\n",
      "Loss:  0.0007355730631388724\n",
      "################################  279  ################################\n",
      "Loss:  0.0007311857189051807\n",
      "################################  280  ################################\n",
      "Loss:  0.0007272554794326425\n",
      "################################  281  ################################\n",
      "Loss:  0.000723556149750948\n",
      "################################  282  ################################\n",
      "Loss:  0.0007201930275186896\n",
      "################################  283  ################################\n",
      "Loss:  0.000717111979611218\n",
      "################################  284  ################################\n",
      "Loss:  0.0007141361711546779\n",
      "################################  285  ################################\n",
      "Loss:  0.0007114647887647152\n",
      "################################  286  ################################\n",
      "Loss:  0.0007087455596774817\n",
      "################################  287  ################################\n",
      "Loss:  0.0007060730713419616\n",
      "################################  288  ################################\n",
      "Loss:  0.0007034074515104294\n",
      "################################  289  ################################\n",
      "Loss:  0.000700610049534589\n",
      "################################  290  ################################\n",
      "Loss:  0.0006977689918130636\n",
      "################################  291  ################################\n",
      "Loss:  0.0006948953378014266\n",
      "################################  292  ################################\n",
      "Loss:  0.000691999273840338\n",
      "################################  293  ################################\n",
      "Loss:  0.0006889317301101983\n",
      "################################  294  ################################\n",
      "Loss:  0.0006859024288132787\n",
      "################################  295  ################################\n",
      "Loss:  0.0006828735349699855\n",
      "################################  296  ################################\n",
      "Loss:  0.0006799086695536971\n",
      "################################  297  ################################\n",
      "Loss:  0.0006768348393961787\n",
      "################################  298  ################################\n",
      "Loss:  0.0006736266077496111\n",
      "################################  299  ################################\n",
      "Loss:  0.0006703432882204652\n",
      "################################  300  ################################\n",
      "Loss:  0.0006672009476460516\n",
      "################################  301  ################################\n",
      "Loss:  0.000664224149659276\n",
      "################################  302  ################################\n",
      "Loss:  0.0006611357675865293\n",
      "################################  303  ################################\n",
      "Loss:  0.000658179575111717\n",
      "################################  304  ################################\n",
      "Loss:  0.0006551500409841537\n",
      "################################  305  ################################\n",
      "Loss:  0.0006521589821204543\n",
      "################################  306  ################################\n",
      "Loss:  0.0006492137326858938\n",
      "################################  307  ################################\n",
      "Loss:  0.0006462634773924947\n",
      "################################  308  ################################\n",
      "Loss:  0.0006434370880015194\n",
      "################################  309  ################################\n",
      "Loss:  0.0006407398032024503\n",
      "################################  310  ################################\n",
      "Loss:  0.000638043973594904\n",
      "################################  311  ################################\n",
      "Loss:  0.0006353211356326938\n",
      "################################  312  ################################\n",
      "Loss:  0.0006325976573862135\n",
      "################################  313  ################################\n",
      "Loss:  0.0006298946100287139\n",
      "################################  314  ################################\n",
      "Loss:  0.0006270955200307071\n",
      "################################  315  ################################\n",
      "Loss:  0.0006244678515940905\n",
      "################################  316  ################################\n",
      "Loss:  0.0006217221380211413\n",
      "################################  317  ################################\n",
      "Loss:  0.0006191759021021426\n",
      "################################  318  ################################\n",
      "Loss:  0.0006163135403767228\n",
      "################################  319  ################################\n",
      "Loss:  0.0006132128764875233\n",
      "################################  320  ################################\n",
      "Loss:  0.0006098486483097076\n",
      "################################  321  ################################\n",
      "Loss:  0.0006063904729671776\n",
      "################################  322  ################################\n",
      "Loss:  0.0006030151853337884\n",
      "################################  323  ################################\n",
      "Loss:  0.0005997092230245471\n",
      "################################  324  ################################\n",
      "Loss:  0.0005965741584077477\n",
      "################################  325  ################################\n",
      "Loss:  0.0005934435757808387\n",
      "################################  326  ################################\n",
      "Loss:  0.0005905455909669399\n",
      "################################  327  ################################\n",
      "Loss:  0.000587773509323597\n",
      "################################  328  ################################\n",
      "Loss:  0.0005850504385307431\n",
      "################################  329  ################################\n",
      "Loss:  0.0005824555992148817\n",
      "################################  330  ################################\n",
      "Loss:  0.0005800348008051515\n",
      "################################  331  ################################\n",
      "Loss:  0.0005777429905720055\n",
      "################################  332  ################################\n",
      "Loss:  0.0005755861639045179\n",
      "################################  333  ################################\n",
      "Loss:  0.0005736046587117016\n",
      "################################  334  ################################\n",
      "Loss:  0.0005718061584047973\n",
      "################################  335  ################################\n",
      "Loss:  0.0005701386835426092\n",
      "################################  336  ################################\n",
      "Loss:  0.0005685716751031578\n",
      "################################  337  ################################\n",
      "Loss:  0.0005670922109857202\n",
      "################################  338  ################################\n",
      "Loss:  0.000565709313377738\n",
      "################################  339  ################################\n",
      "Loss:  0.0005644107586704195\n",
      "################################  340  ################################\n",
      "Loss:  0.0005631519597955048\n",
      "################################  341  ################################\n",
      "Loss:  0.00056193966884166\n",
      "################################  342  ################################\n",
      "Loss:  0.0005607744678854942\n",
      "################################  343  ################################\n",
      "Loss:  0.0005596333649009466\n",
      "################################  344  ################################\n",
      "Loss:  0.0005585076287388802\n",
      "################################  345  ################################\n",
      "Loss:  0.0005573731032200158\n",
      "################################  346  ################################\n",
      "Loss:  0.0005562164587900043\n",
      "################################  347  ################################\n",
      "Loss:  0.0005550159839913249\n",
      "################################  348  ################################\n",
      "Loss:  0.0005538216792047024\n",
      "################################  349  ################################\n",
      "Loss:  0.0005525557789951563\n",
      "################################  350  ################################\n",
      "Loss:  0.0005512650823220611\n",
      "################################  351  ################################\n",
      "Loss:  0.0005498732789419591\n",
      "################################  352  ################################\n",
      "Loss:  0.0005482605192810297\n",
      "################################  353  ################################\n",
      "Loss:  0.0005465889116749167\n",
      "################################  354  ################################\n",
      "Loss:  0.0005447959993034601\n",
      "################################  355  ################################\n",
      "Loss:  0.00054293213179335\n",
      "################################  356  ################################\n",
      "Loss:  0.0005408938159234822\n",
      "################################  357  ################################\n",
      "Loss:  0.0005387435667216778\n",
      "################################  358  ################################\n",
      "Loss:  0.0005363141535781324\n",
      "################################  359  ################################\n",
      "Loss:  0.0005335846217349172\n",
      "################################  360  ################################\n",
      "Loss:  0.0005309554398991168\n",
      "################################  361  ################################\n",
      "Loss:  0.0005283289356157184\n",
      "################################  362  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0005257374723441899\n",
      "################################  363  ################################\n",
      "Loss:  0.0005230936221778393\n",
      "################################  364  ################################\n",
      "Loss:  0.0005205743946135044\n",
      "################################  365  ################################\n",
      "Loss:  0.0005179831641726196\n",
      "################################  366  ################################\n",
      "Loss:  0.000515442923642695\n",
      "################################  367  ################################\n",
      "Loss:  0.0005128545453771949\n",
      "################################  368  ################################\n",
      "Loss:  0.0005102171562612057\n",
      "################################  369  ################################\n",
      "Loss:  0.0005076696397736669\n",
      "################################  370  ################################\n",
      "Loss:  0.0005052704946137965\n",
      "################################  371  ################################\n",
      "Loss:  0.000502972980029881\n",
      "################################  372  ################################\n",
      "Loss:  0.0005009004380553961\n",
      "################################  373  ################################\n",
      "Loss:  0.0004989811568520963\n",
      "################################  374  ################################\n",
      "Loss:  0.00049718061927706\n",
      "################################  375  ################################\n",
      "Loss:  0.0004954690812155604\n",
      "################################  376  ################################\n",
      "Loss:  0.0004938107449561357\n",
      "################################  377  ################################\n",
      "Loss:  0.0004923574160784483\n",
      "################################  378  ################################\n",
      "Loss:  0.0004909694544039667\n",
      "################################  379  ################################\n",
      "Loss:  0.000489676371216774\n",
      "################################  380  ################################\n",
      "Loss:  0.0004883846850134432\n",
      "################################  381  ################################\n",
      "Loss:  0.00048708380199968815\n",
      "################################  382  ################################\n",
      "Loss:  0.0004858681932091713\n",
      "################################  383  ################################\n",
      "Loss:  0.00048462668200954795\n",
      "################################  384  ################################\n",
      "Loss:  0.0004834688443224877\n",
      "################################  385  ################################\n",
      "Loss:  0.0004822735209017992\n",
      "################################  386  ################################\n",
      "Loss:  0.00048107359907589853\n",
      "################################  387  ################################\n",
      "Loss:  0.00047981730313040316\n",
      "################################  388  ################################\n",
      "Loss:  0.00047847034875303507\n",
      "################################  389  ################################\n",
      "Loss:  0.00047708675265312195\n",
      "################################  390  ################################\n",
      "Loss:  0.00047558231744915247\n",
      "################################  391  ################################\n",
      "Loss:  0.00047402549535036087\n",
      "################################  392  ################################\n",
      "Loss:  0.0004724062455352396\n",
      "################################  393  ################################\n",
      "Loss:  0.0004707542830146849\n",
      "################################  394  ################################\n",
      "Loss:  0.00046913433470763266\n",
      "################################  395  ################################\n",
      "Loss:  0.00046742212725803256\n",
      "################################  396  ################################\n",
      "Loss:  0.0004657249664887786\n",
      "################################  397  ################################\n",
      "Loss:  0.0004638711689040065\n",
      "################################  398  ################################\n",
      "Loss:  0.00046236321213655174\n",
      "################################  399  ################################\n",
      "Loss:  0.00046063275658525527\n",
      "################################  400  ################################\n",
      "Loss:  0.00045852785115130246\n",
      "################################  401  ################################\n",
      "Loss:  0.00045667897211387753\n",
      "################################  402  ################################\n",
      "Loss:  0.00045487063471227884\n",
      "################################  403  ################################\n",
      "Loss:  0.0004530958249233663\n",
      "################################  404  ################################\n",
      "Loss:  0.00045124226016923785\n",
      "################################  405  ################################\n",
      "Loss:  0.00044953852193430066\n",
      "################################  406  ################################\n",
      "Loss:  0.00044789412640966475\n",
      "################################  407  ################################\n",
      "Loss:  0.0004462754004634917\n",
      "################################  408  ################################\n",
      "Loss:  0.00044471907312981784\n",
      "################################  409  ################################\n",
      "Loss:  0.0004431947600096464\n",
      "################################  410  ################################\n",
      "Loss:  0.00044172030175104737\n",
      "################################  411  ################################\n",
      "Loss:  0.0004402908671181649\n",
      "################################  412  ################################\n",
      "Loss:  0.0004389245295897126\n",
      "################################  413  ################################\n",
      "Loss:  0.00043762309360317886\n",
      "################################  414  ################################\n",
      "Loss:  0.0004363904590718448\n",
      "################################  415  ################################\n",
      "Loss:  0.00043520599137991667\n",
      "################################  416  ################################\n",
      "Loss:  0.0004340530140325427\n",
      "################################  417  ################################\n",
      "Loss:  0.0004329177609179169\n",
      "################################  418  ################################\n",
      "Loss:  0.00043173995800316334\n",
      "################################  419  ################################\n",
      "Loss:  0.00043066826765425503\n",
      "################################  420  ################################\n",
      "Loss:  0.00042969861533492804\n",
      "################################  421  ################################\n",
      "Loss:  0.00042865099385380745\n",
      "################################  422  ################################\n",
      "Loss:  0.0004276545369066298\n",
      "################################  423  ################################\n",
      "Loss:  0.0004265609895810485\n",
      "################################  424  ################################\n",
      "Loss:  0.00042534127715043724\n",
      "################################  425  ################################\n",
      "Loss:  0.000424121564719826\n",
      "################################  426  ################################\n",
      "Loss:  0.0004228707985021174\n",
      "################################  427  ################################\n",
      "Loss:  0.00042166514322161674\n",
      "################################  428  ################################\n",
      "Loss:  0.00042043361463584006\n",
      "################################  429  ################################\n",
      "Loss:  0.00041916518239304423\n",
      "################################  430  ################################\n",
      "Loss:  0.00041787209920585155\n",
      "################################  431  ################################\n",
      "Loss:  0.00041650363709777594\n",
      "################################  432  ################################\n",
      "Loss:  0.00041510898154228926\n",
      "################################  433  ################################\n",
      "Loss:  0.00041363612399436533\n",
      "################################  434  ################################\n",
      "Loss:  0.0004121628007851541\n",
      "################################  435  ################################\n",
      "Loss:  0.0004106773412786424\n",
      "################################  436  ################################\n",
      "Loss:  0.00040919255116023123\n",
      "################################  437  ################################\n",
      "Loss:  0.0004077105550095439\n",
      "################################  438  ################################\n",
      "Loss:  0.00040620029903948307\n",
      "################################  439  ################################\n",
      "Loss:  0.0004046655085403472\n",
      "################################  440  ################################\n",
      "Loss:  0.00040311203338205814\n",
      "################################  441  ################################\n",
      "Loss:  0.00040152866858989\n",
      "################################  442  ################################\n",
      "Loss:  0.00040002050809562206\n",
      "################################  443  ################################\n",
      "Loss:  0.0003984849317930639\n",
      "################################  444  ################################\n",
      "Loss:  0.0003969970275647938\n",
      "################################  445  ################################\n",
      "Loss:  0.00039549294160678983\n",
      "################################  446  ################################\n",
      "Loss:  0.00039393306360580027\n",
      "################################  447  ################################\n",
      "Loss:  0.0003923747572116554\n",
      "################################  448  ################################\n",
      "Loss:  0.00039082003058865666\n",
      "################################  449  ################################\n",
      "Loss:  0.0003892519453074783\n",
      "################################  450  ################################\n",
      "Loss:  0.0003876990231219679\n",
      "################################  451  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0003861341974698007\n",
      "################################  452  ################################\n",
      "Loss:  0.00038462987868115306\n",
      "################################  453  ################################\n",
      "Loss:  0.0003831151989288628\n",
      "################################  454  ################################\n",
      "Loss:  0.0003817088436335325\n",
      "################################  455  ################################\n",
      "Loss:  0.00038025015965104103\n",
      "################################  456  ################################\n",
      "Loss:  0.0003788794274441898\n",
      "################################  457  ################################\n",
      "Loss:  0.00037745802546851337\n",
      "################################  458  ################################\n",
      "Loss:  0.00037592335138469934\n",
      "################################  459  ################################\n",
      "Loss:  0.0003744079149328172\n",
      "################################  460  ################################\n",
      "Loss:  0.0003729805466718972\n",
      "################################  461  ################################\n",
      "Loss:  0.0003716190403793007\n",
      "################################  462  ################################\n",
      "Loss:  0.0003702936228364706\n",
      "################################  463  ################################\n",
      "Loss:  0.00036899105180054903\n",
      "################################  464  ################################\n",
      "Loss:  0.0003676575724966824\n",
      "################################  465  ################################\n",
      "Loss:  0.00036640948383137584\n",
      "################################  466  ################################\n",
      "Loss:  0.000365264160791412\n",
      "################################  467  ################################\n",
      "Loss:  0.0003641272196546197\n",
      "################################  468  ################################\n",
      "Loss:  0.00036307639675214887\n",
      "################################  469  ################################\n",
      "Loss:  0.00036206928780302405\n",
      "################################  470  ################################\n",
      "Loss:  0.0003610944258980453\n",
      "################################  471  ################################\n",
      "Loss:  0.00036013114731758833\n",
      "################################  472  ################################\n",
      "Loss:  0.00035919586662203074\n",
      "################################  473  ################################\n",
      "Loss:  0.0003582392237149179\n",
      "################################  474  ################################\n",
      "Loss:  0.0003572633722797036\n",
      "################################  475  ################################\n",
      "Loss:  0.0003563233185559511\n",
      "################################  476  ################################\n",
      "Loss:  0.0003553676069714129\n",
      "################################  477  ################################\n",
      "Loss:  0.0003544749924913049\n",
      "################################  478  ################################\n",
      "Loss:  0.00035355734871700406\n",
      "################################  479  ################################\n",
      "Loss:  0.0003526801010593772\n",
      "################################  480  ################################\n",
      "Loss:  0.0003519141173455864\n",
      "################################  481  ################################\n",
      "Loss:  0.00035109621239826083\n",
      "################################  482  ################################\n",
      "Loss:  0.000350393820554018\n",
      "################################  483  ################################\n",
      "Loss:  0.00034956634044647217\n",
      "################################  484  ################################\n",
      "Loss:  0.00034865469206124544\n",
      "################################  485  ################################\n",
      "Loss:  0.00034761964343488216\n",
      "################################  486  ################################\n",
      "Loss:  0.0003466117777861655\n",
      "################################  487  ################################\n",
      "Loss:  0.00034561322536319494\n",
      "################################  488  ################################\n",
      "Loss:  0.0003445153997745365\n",
      "################################  489  ################################\n",
      "Loss:  0.0003433980164118111\n",
      "################################  490  ################################\n",
      "Loss:  0.0003422357840463519\n",
      "################################  491  ################################\n",
      "Loss:  0.00034108723048120737\n",
      "################################  492  ################################\n",
      "Loss:  0.00033996120328083634\n",
      "################################  493  ################################\n",
      "Loss:  0.0003387792967259884\n",
      "################################  494  ################################\n",
      "Loss:  0.00033760161022655666\n",
      "################################  495  ################################\n",
      "Loss:  0.00033642578637227416\n",
      "################################  496  ################################\n",
      "Loss:  0.0003352451603859663\n",
      "################################  497  ################################\n",
      "Loss:  0.00033413677010685205\n",
      "################################  498  ################################\n",
      "Loss:  0.0003328978782519698\n",
      "################################  499  ################################\n",
      "Loss:  0.00033191763213835657\n",
      "################################  500  ################################\n",
      "Loss:  0.00033088342752307653\n",
      "################################  501  ################################\n",
      "Loss:  0.0003298952942714095\n",
      "################################  502  ################################\n",
      "Loss:  0.0003289647283963859\n",
      "################################  503  ################################\n",
      "Loss:  0.00032788788666948676\n",
      "################################  504  ################################\n",
      "Loss:  0.00032691447995603085\n",
      "################################  505  ################################\n",
      "Loss:  0.00032576272496953607\n",
      "################################  506  ################################\n",
      "Loss:  0.00032478373032063246\n",
      "################################  507  ################################\n",
      "Loss:  0.00032371285487897694\n",
      "################################  508  ################################\n",
      "Loss:  0.00032254616962745786\n",
      "################################  509  ################################\n",
      "Loss:  0.00032126071164384484\n",
      "################################  510  ################################\n",
      "Loss:  0.0003199171624146402\n",
      "################################  511  ################################\n",
      "Loss:  0.00031867611687630415\n",
      "################################  512  ################################\n",
      "Loss:  0.00031735224183648825\n",
      "################################  513  ################################\n",
      "Loss:  0.0003160702472086996\n",
      "################################  514  ################################\n",
      "Loss:  0.0003147657262161374\n",
      "################################  515  ################################\n",
      "Loss:  0.0003134448197670281\n",
      "################################  516  ################################\n",
      "Loss:  0.00031219300581142306\n",
      "################################  517  ################################\n",
      "Loss:  0.00031095134909264743\n",
      "################################  518  ################################\n",
      "Loss:  0.0003097826847806573\n",
      "################################  519  ################################\n",
      "Loss:  0.00030866701854392886\n",
      "################################  520  ################################\n",
      "Loss:  0.0003075918066315353\n",
      "################################  521  ################################\n",
      "Loss:  0.0003065445343963802\n",
      "################################  522  ################################\n",
      "Loss:  0.0003054712724406272\n",
      "################################  523  ################################\n",
      "Loss:  0.00030457868706434965\n",
      "################################  524  ################################\n",
      "Loss:  0.000303757784422487\n",
      "################################  525  ################################\n",
      "Loss:  0.0003029608924407512\n",
      "################################  526  ################################\n",
      "Loss:  0.0003021155425813049\n",
      "################################  527  ################################\n",
      "Loss:  0.0003013233654201031\n",
      "################################  528  ################################\n",
      "Loss:  0.00030042900471016765\n",
      "################################  529  ################################\n",
      "Loss:  0.00029956724029034376\n",
      "################################  530  ################################\n",
      "Loss:  0.00029872823506593704\n",
      "################################  531  ################################\n",
      "Loss:  0.00029765948420390487\n",
      "################################  532  ################################\n",
      "Loss:  0.0002965971943922341\n",
      "################################  533  ################################\n",
      "Loss:  0.00029529232415370643\n",
      "################################  534  ################################\n",
      "Loss:  0.00029398908372968435\n",
      "################################  535  ################################\n",
      "Loss:  0.00029242338496260345\n",
      "################################  536  ################################\n",
      "Loss:  0.00029127299785614014\n",
      "################################  537  ################################\n",
      "Loss:  0.00028997877961955965\n",
      "################################  538  ################################\n",
      "Loss:  0.00028862192993983626\n",
      "################################  539  ################################\n",
      "Loss:  0.00028720474801957607\n",
      "################################  540  ################################\n",
      "Loss:  0.00028584065148606896\n",
      "################################  541  ################################\n",
      "Loss:  0.00028455621213652194\n",
      "################################  542  ################################\n",
      "Loss:  0.0002832437166944146\n",
      "################################  543  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0002820705994963646\n",
      "################################  544  ################################\n",
      "Loss:  0.0002809080178849399\n",
      "################################  545  ################################\n",
      "Loss:  0.0002798840869218111\n",
      "################################  546  ################################\n",
      "Loss:  0.00027885069721378386\n",
      "################################  547  ################################\n",
      "Loss:  0.00027786559076048434\n",
      "################################  548  ################################\n",
      "Loss:  0.00027684037922881544\n",
      "################################  549  ################################\n",
      "Loss:  0.0002756597241386771\n",
      "################################  550  ################################\n",
      "Loss:  0.0002743912336882204\n",
      "################################  551  ################################\n",
      "Loss:  0.00027325021801516414\n",
      "################################  552  ################################\n",
      "Loss:  0.0002721597265917808\n",
      "################################  553  ################################\n",
      "Loss:  0.00027097767451778054\n",
      "################################  554  ################################\n",
      "Loss:  0.0002698899479582906\n",
      "################################  555  ################################\n",
      "Loss:  0.00026878202334046364\n",
      "################################  556  ################################\n",
      "Loss:  0.0002676104777492583\n",
      "################################  557  ################################\n",
      "Loss:  0.0002664228668436408\n",
      "################################  558  ################################\n",
      "Loss:  0.0002652695693541318\n",
      "################################  559  ################################\n",
      "Loss:  0.0002641204046085477\n",
      "################################  560  ################################\n",
      "Loss:  0.00026302249170839787\n",
      "################################  561  ################################\n",
      "Loss:  0.00026195700047537684\n",
      "################################  562  ################################\n",
      "Loss:  0.00026094765053130686\n",
      "################################  563  ################################\n",
      "Loss:  0.00025992654263973236\n",
      "################################  564  ################################\n",
      "Loss:  0.00025894527789205313\n",
      "################################  565  ################################\n",
      "Loss:  0.00025794527027755976\n",
      "################################  566  ################################\n",
      "Loss:  0.00025701936101540923\n",
      "################################  567  ################################\n",
      "Loss:  0.00025613000616431236\n",
      "################################  568  ################################\n",
      "Loss:  0.00025520173949189484\n",
      "################################  569  ################################\n",
      "Loss:  0.00025428604567423463\n",
      "################################  570  ################################\n",
      "Loss:  0.0002533680817577988\n",
      "################################  571  ################################\n",
      "Loss:  0.0002524582378100604\n",
      "################################  572  ################################\n",
      "Loss:  0.00025163445388898253\n",
      "################################  573  ################################\n",
      "Loss:  0.0002508219622541219\n",
      "################################  574  ################################\n",
      "Loss:  0.0002500987611711025\n",
      "################################  575  ################################\n",
      "Loss:  0.00024936418049037457\n",
      "################################  576  ################################\n",
      "Loss:  0.00024865794694051147\n",
      "################################  577  ################################\n",
      "Loss:  0.0002479012473486364\n",
      "################################  578  ################################\n",
      "Loss:  0.0002470849722158164\n",
      "################################  579  ################################\n",
      "Loss:  0.00024627026868984103\n",
      "################################  580  ################################\n",
      "Loss:  0.00024541871971450746\n",
      "################################  581  ################################\n",
      "Loss:  0.0002445067511871457\n",
      "################################  582  ################################\n",
      "Loss:  0.0002437379735056311\n",
      "################################  583  ################################\n",
      "Loss:  0.0002429921878501773\n",
      "################################  584  ################################\n",
      "Loss:  0.00024223863147199154\n",
      "################################  585  ################################\n",
      "Loss:  0.00024155806750059128\n",
      "################################  586  ################################\n",
      "Loss:  0.00024086842313408852\n",
      "################################  587  ################################\n",
      "Loss:  0.00024017487885430455\n",
      "################################  588  ################################\n",
      "Loss:  0.00023951174807734787\n",
      "################################  589  ################################\n",
      "Loss:  0.00023879343643784523\n",
      "################################  590  ################################\n",
      "Loss:  0.00023815389431547374\n",
      "################################  591  ################################\n",
      "Loss:  0.00023745012003928423\n",
      "################################  592  ################################\n",
      "Loss:  0.00023684176267124712\n",
      "################################  593  ################################\n",
      "Loss:  0.00023612062796019018\n",
      "################################  594  ################################\n",
      "Loss:  0.00023537647211924195\n",
      "################################  595  ################################\n",
      "Loss:  0.00023450175649486482\n",
      "################################  596  ################################\n",
      "Loss:  0.00023372314171865582\n",
      "################################  597  ################################\n",
      "Loss:  0.00023286425857804716\n",
      "################################  598  ################################\n",
      "Loss:  0.00023196227266453207\n",
      "################################  599  ################################\n",
      "Loss:  0.0002310873824171722\n",
      "################################  600  ################################\n",
      "Loss:  0.00023018254432827234\n",
      "################################  601  ################################\n",
      "Loss:  0.00022938172332942486\n",
      "################################  602  ################################\n",
      "Loss:  0.000228579476242885\n",
      "################################  603  ################################\n",
      "Loss:  0.00022777303820475936\n",
      "################################  604  ################################\n",
      "Loss:  0.00022698569227941334\n",
      "################################  605  ################################\n",
      "Loss:  0.00022627561702392995\n",
      "################################  606  ################################\n",
      "Loss:  0.00022562519006896764\n",
      "################################  607  ################################\n",
      "Loss:  0.0002249420213047415\n",
      "################################  608  ################################\n",
      "Loss:  0.00022428258671425283\n",
      "################################  609  ################################\n",
      "Loss:  0.00022365157201420516\n",
      "################################  610  ################################\n",
      "Loss:  0.00022301750141195953\n",
      "################################  611  ################################\n",
      "Loss:  0.0002223629126092419\n",
      "################################  612  ################################\n",
      "Loss:  0.00022172056196723133\n",
      "################################  613  ################################\n",
      "Loss:  0.0002210957172792405\n",
      "################################  614  ################################\n",
      "Loss:  0.00022047696984373033\n",
      "################################  615  ################################\n",
      "Loss:  0.00021987958461977541\n",
      "################################  616  ################################\n",
      "Loss:  0.0002192907122662291\n",
      "################################  617  ################################\n",
      "Loss:  0.0002187367936130613\n",
      "################################  618  ################################\n",
      "Loss:  0.0002181666495744139\n",
      "################################  619  ################################\n",
      "Loss:  0.00021761454991064966\n",
      "################################  620  ################################\n",
      "Loss:  0.00021705091057810932\n",
      "################################  621  ################################\n",
      "Loss:  0.0002164747129427269\n",
      "################################  622  ################################\n",
      "Loss:  0.00021590289543382823\n",
      "################################  623  ################################\n",
      "Loss:  0.00021531342645175755\n",
      "################################  624  ################################\n",
      "Loss:  0.00021471167565323412\n",
      "################################  625  ################################\n",
      "Loss:  0.0002140883734682575\n",
      "################################  626  ################################\n",
      "Loss:  0.00021338528313208371\n",
      "################################  627  ################################\n",
      "Loss:  0.00021271673904266208\n",
      "################################  628  ################################\n",
      "Loss:  0.00021212987485341728\n",
      "################################  629  ################################\n",
      "Loss:  0.0002115175302606076\n",
      "################################  630  ################################\n",
      "Loss:  0.00021088140783831477\n",
      "################################  631  ################################\n",
      "Loss:  0.00021020520944148302\n",
      "################################  632  ################################\n",
      "Loss:  0.00020954225328750908\n",
      "################################  633  ################################\n",
      "Loss:  0.00020888217841275036\n",
      "################################  634  ################################\n",
      "Loss:  0.00020822507212869823\n",
      "################################  635  ################################\n",
      "Loss:  0.00020757567835971713\n",
      "################################  636  ################################\n",
      "Loss:  0.00020692475663963705\n",
      "################################  637  ################################\n",
      "Loss:  0.00020633457461372018\n",
      "################################  638  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.00020578238763846457\n",
      "################################  639  ################################\n",
      "Loss:  0.00020523893181234598\n",
      "################################  640  ################################\n",
      "Loss:  0.00020469698938541114\n",
      "################################  641  ################################\n",
      "Loss:  0.00020415851031430066\n",
      "################################  642  ################################\n",
      "Loss:  0.00020365053205750883\n",
      "################################  643  ################################\n",
      "Loss:  0.0002031274198088795\n",
      "################################  644  ################################\n",
      "Loss:  0.0002025803696596995\n",
      "################################  645  ################################\n",
      "Loss:  0.00020200430299155414\n",
      "################################  646  ################################\n",
      "Loss:  0.00020145345479249954\n",
      "################################  647  ################################\n",
      "Loss:  0.00020095046784263104\n",
      "################################  648  ################################\n",
      "Loss:  0.00020039358059875667\n",
      "################################  649  ################################\n",
      "Loss:  0.0001998811203520745\n",
      "################################  650  ################################\n",
      "Loss:  0.0001993570476770401\n",
      "################################  651  ################################\n",
      "Loss:  0.00019873876590281725\n",
      "################################  652  ################################\n",
      "Loss:  0.0001981529640033841\n",
      "################################  653  ################################\n",
      "Loss:  0.0001974532351596281\n",
      "################################  654  ################################\n",
      "Loss:  0.00019662025442812592\n",
      "################################  655  ################################\n",
      "Loss:  0.00019567954586818814\n",
      "################################  656  ################################\n",
      "Loss:  0.00019491318380460143\n",
      "################################  657  ################################\n",
      "Loss:  0.00019389920635148883\n",
      "################################  658  ################################\n",
      "Loss:  0.00019289078772999346\n",
      "################################  659  ################################\n",
      "Loss:  0.00019176880596205592\n",
      "################################  660  ################################\n",
      "Loss:  0.0001906060497276485\n",
      "################################  661  ################################\n",
      "Loss:  0.00018943866598419845\n",
      "################################  662  ################################\n",
      "Loss:  0.0001882764045149088\n",
      "################################  663  ################################\n",
      "Loss:  0.0001871706044767052\n",
      "################################  664  ################################\n",
      "Loss:  0.00018609385006129742\n",
      "################################  665  ################################\n",
      "Loss:  0.0001851535344030708\n",
      "################################  666  ################################\n",
      "Loss:  0.00018429019837640226\n",
      "################################  667  ################################\n",
      "Loss:  0.00018352916231378913\n",
      "################################  668  ################################\n",
      "Loss:  0.0001828462554840371\n",
      "################################  669  ################################\n",
      "Loss:  0.00018219095363747329\n",
      "################################  670  ################################\n",
      "Loss:  0.0001816229341784492\n",
      "################################  671  ################################\n",
      "Loss:  0.0001810948015190661\n",
      "################################  672  ################################\n",
      "Loss:  0.00018060342699754983\n",
      "################################  673  ################################\n",
      "Loss:  0.0001801483886083588\n",
      "################################  674  ################################\n",
      "Loss:  0.00017969374312087893\n",
      "################################  675  ################################\n",
      "Loss:  0.00017927243607118726\n",
      "################################  676  ################################\n",
      "Loss:  0.0001788674562703818\n",
      "################################  677  ################################\n",
      "Loss:  0.00017849775031208992\n",
      "################################  678  ################################\n",
      "Loss:  0.00017813951126299798\n",
      "################################  679  ################################\n",
      "Loss:  0.00017774547450244427\n",
      "################################  680  ################################\n",
      "Loss:  0.00017736887093633413\n",
      "################################  681  ################################\n",
      "Loss:  0.00017699802992865443\n",
      "################################  682  ################################\n",
      "Loss:  0.0001766193745424971\n",
      "################################  683  ################################\n",
      "Loss:  0.00017623172607272863\n",
      "################################  684  ################################\n",
      "Loss:  0.00017583416774868965\n",
      "################################  685  ################################\n",
      "Loss:  0.00017542846035212278\n",
      "################################  686  ################################\n",
      "Loss:  0.00017498018860351294\n",
      "################################  687  ################################\n",
      "Loss:  0.0001745387853588909\n",
      "################################  688  ################################\n",
      "Loss:  0.00017403686069883406\n",
      "################################  689  ################################\n",
      "Loss:  0.00017356286116410047\n",
      "################################  690  ################################\n",
      "Loss:  0.00017291605763603002\n",
      "################################  691  ################################\n",
      "Loss:  0.00017244901391677558\n",
      "################################  692  ################################\n",
      "Loss:  0.000171900013810955\n",
      "################################  693  ################################\n",
      "Loss:  0.00017120232223533094\n",
      "################################  694  ################################\n",
      "Loss:  0.00017037734505720437\n",
      "################################  695  ################################\n",
      "Loss:  0.00016942589718382806\n",
      "################################  696  ################################\n",
      "Loss:  0.00016870448598638177\n",
      "################################  697  ################################\n",
      "Loss:  0.0001679285487625748\n",
      "################################  698  ################################\n",
      "Loss:  0.00016703209257684648\n",
      "################################  699  ################################\n",
      "Loss:  0.0001663014554651454\n",
      "################################  700  ################################\n",
      "Loss:  0.00016560949734412134\n",
      "################################  701  ################################\n",
      "Loss:  0.00016491221322212368\n",
      "################################  702  ################################\n",
      "Loss:  0.00016422200133092701\n",
      "################################  703  ################################\n",
      "Loss:  0.00016360606241505593\n",
      "################################  704  ################################\n",
      "Loss:  0.0001630300102988258\n",
      "################################  705  ################################\n",
      "Loss:  0.00016251238412223756\n",
      "################################  706  ################################\n",
      "Loss:  0.00016206469445023686\n",
      "################################  707  ################################\n",
      "Loss:  0.0001616714580450207\n",
      "################################  708  ################################\n",
      "Loss:  0.00016126356786116958\n",
      "################################  709  ################################\n",
      "Loss:  0.00016091532597783953\n",
      "################################  710  ################################\n",
      "Loss:  0.00016053592844400555\n",
      "################################  711  ################################\n",
      "Loss:  0.00016012249398045242\n",
      "################################  712  ################################\n",
      "Loss:  0.00015970651293173432\n",
      "################################  713  ################################\n",
      "Loss:  0.0001592533226357773\n",
      "################################  714  ################################\n",
      "Loss:  0.0001588346785865724\n",
      "################################  715  ################################\n",
      "Loss:  0.00015841670392546803\n",
      "################################  716  ################################\n",
      "Loss:  0.0001579868549015373\n",
      "################################  717  ################################\n",
      "Loss:  0.00015755780623294413\n",
      "################################  718  ################################\n",
      "Loss:  0.000157108879648149\n",
      "################################  719  ################################\n",
      "Loss:  0.00015667056140955538\n",
      "################################  720  ################################\n",
      "Loss:  0.00015624635852873325\n",
      "################################  721  ################################\n",
      "Loss:  0.00015583653294015676\n",
      "################################  722  ################################\n",
      "Loss:  0.00015543733024969697\n",
      "################################  723  ################################\n",
      "Loss:  0.0001550365414004773\n",
      "################################  724  ################################\n",
      "Loss:  0.0001546438579680398\n",
      "################################  725  ################################\n",
      "Loss:  0.0001542474201414734\n",
      "################################  726  ################################\n",
      "Loss:  0.00015384660218842328\n",
      "################################  727  ################################\n",
      "Loss:  0.00015343207633122802\n",
      "################################  728  ################################\n",
      "Loss:  0.00015302481187973171\n",
      "################################  729  ################################\n",
      "Loss:  0.00015261347289197147\n",
      "################################  730  ################################\n",
      "Loss:  0.00015218963380903006\n",
      "################################  731  ################################\n",
      "Loss:  0.00015175380394794047\n",
      "################################  732  ################################\n",
      "Loss:  0.00015127220831345767\n",
      "################################  733  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.00015083790640346706\n",
      "################################  734  ################################\n",
      "Loss:  0.00015039295249152929\n",
      "################################  735  ################################\n",
      "Loss:  0.00014993605145718902\n",
      "################################  736  ################################\n",
      "Loss:  0.00014947712770663202\n",
      "################################  737  ################################\n",
      "Loss:  0.00014901257236488163\n",
      "################################  738  ################################\n",
      "Loss:  0.00014854836626909673\n",
      "################################  739  ################################\n",
      "Loss:  0.0001480831124354154\n",
      "################################  740  ################################\n",
      "Loss:  0.00014761334750801325\n",
      "################################  741  ################################\n",
      "Loss:  0.00014713633572682738\n",
      "################################  742  ################################\n",
      "Loss:  0.00014667405048385262\n",
      "################################  743  ################################\n",
      "Loss:  0.0001462080399505794\n",
      "################################  744  ################################\n",
      "Loss:  0.00014575282693840563\n",
      "################################  745  ################################\n",
      "Loss:  0.00014529915642924607\n",
      "################################  746  ################################\n",
      "Loss:  0.00014481083780992776\n",
      "################################  747  ################################\n",
      "Loss:  0.00014435616321861744\n",
      "################################  748  ################################\n",
      "Loss:  0.0001438334584236145\n",
      "################################  749  ################################\n",
      "Loss:  0.00014336769527290016\n",
      "################################  750  ################################\n",
      "Loss:  0.00014271705003920943\n",
      "################################  751  ################################\n",
      "Loss:  0.0001421577180735767\n",
      "################################  752  ################################\n",
      "Loss:  0.00014139263657853007\n",
      "################################  753  ################################\n",
      "Loss:  0.00014066063158679754\n",
      "################################  754  ################################\n",
      "Loss:  0.00013981704250909388\n",
      "################################  755  ################################\n",
      "Loss:  0.00013894567382521927\n",
      "################################  756  ################################\n",
      "Loss:  0.00013797727297060192\n",
      "################################  757  ################################\n",
      "Loss:  0.00013726582983508706\n",
      "################################  758  ################################\n",
      "Loss:  0.00013656240480486304\n",
      "################################  759  ################################\n",
      "Loss:  0.0001358621520921588\n",
      "################################  760  ################################\n",
      "Loss:  0.00013514034799300134\n",
      "################################  761  ################################\n",
      "Loss:  0.0001345787604805082\n",
      "################################  762  ################################\n",
      "Loss:  0.0001340203598374501\n",
      "################################  763  ################################\n",
      "Loss:  0.0001334713597316295\n",
      "################################  764  ################################\n",
      "Loss:  0.00013301856233738363\n",
      "################################  765  ################################\n",
      "Loss:  0.000132626824779436\n",
      "################################  766  ################################\n",
      "Loss:  0.00013226130977272987\n",
      "################################  767  ################################\n",
      "Loss:  0.0001318884314969182\n",
      "################################  768  ################################\n",
      "Loss:  0.0001315444242209196\n",
      "################################  769  ################################\n",
      "Loss:  0.0001312064123339951\n",
      "################################  770  ################################\n",
      "Loss:  0.00013087177649140358\n",
      "################################  771  ################################\n",
      "Loss:  0.00013057116302661598\n",
      "################################  772  ################################\n",
      "Loss:  0.00013027583190705627\n",
      "################################  773  ################################\n",
      "Loss:  0.00013000666513107717\n",
      "################################  774  ################################\n",
      "Loss:  0.00012972584227100015\n",
      "################################  775  ################################\n",
      "Loss:  0.0001294683024752885\n",
      "################################  776  ################################\n",
      "Loss:  0.0001291618391405791\n",
      "################################  777  ################################\n",
      "Loss:  0.000128830288304016\n",
      "################################  778  ################################\n",
      "Loss:  0.00012845404853578657\n",
      "################################  779  ################################\n",
      "Loss:  0.00012806446466129273\n",
      "################################  780  ################################\n",
      "Loss:  0.0001276657567359507\n",
      "################################  781  ################################\n",
      "Loss:  0.00012718008656520396\n",
      "################################  782  ################################\n",
      "Loss:  0.0001267143088625744\n",
      "################################  783  ################################\n",
      "Loss:  0.00012614286970347166\n",
      "################################  784  ################################\n",
      "Loss:  0.00012566358782351017\n",
      "################################  785  ################################\n",
      "Loss:  0.0001251197245437652\n",
      "################################  786  ################################\n",
      "Loss:  0.00012449067435227334\n",
      "################################  787  ################################\n",
      "Loss:  0.00012379229883663356\n",
      "################################  788  ################################\n",
      "Loss:  0.00012310995953157544\n",
      "################################  789  ################################\n",
      "Loss:  0.0001223911385750398\n",
      "################################  790  ################################\n",
      "Loss:  0.00012171092384960502\n",
      "################################  791  ################################\n",
      "Loss:  0.00012102434266125783\n",
      "################################  792  ################################\n",
      "Loss:  0.00012037652777507901\n",
      "################################  793  ################################\n",
      "Loss:  0.00011978491966146976\n",
      "################################  794  ################################\n",
      "Loss:  0.00011911849287571386\n",
      "################################  795  ################################\n",
      "Loss:  0.00011856830678880215\n",
      "################################  796  ################################\n",
      "Loss:  0.00011800677020801231\n",
      "################################  797  ################################\n",
      "Loss:  0.00011752513819374144\n",
      "################################  798  ################################\n",
      "Loss:  0.00011703468044288456\n",
      "################################  799  ################################\n",
      "Loss:  0.00011666463251458481\n",
      "################################  800  ################################\n",
      "Loss:  0.00011627802450675517\n",
      "################################  801  ################################\n",
      "Loss:  0.00011586175241973251\n",
      "################################  802  ################################\n",
      "Loss:  0.00011540044215507805\n",
      "################################  803  ################################\n",
      "Loss:  0.00011496363731566817\n",
      "################################  804  ################################\n",
      "Loss:  0.00011455857747932896\n",
      "################################  805  ################################\n",
      "Loss:  0.00011411764717195183\n",
      "################################  806  ################################\n",
      "Loss:  0.00011372147127985954\n",
      "################################  807  ################################\n",
      "Loss:  0.00011330075358273461\n",
      "################################  808  ################################\n",
      "Loss:  0.00011298854951746762\n",
      "################################  809  ################################\n",
      "Loss:  0.00011261743202339858\n",
      "################################  810  ################################\n",
      "Loss:  0.00011225484195165336\n",
      "################################  811  ################################\n",
      "Loss:  0.00011187826748937368\n",
      "################################  812  ################################\n",
      "Loss:  0.00011150696082040668\n",
      "################################  813  ################################\n",
      "Loss:  0.00011112831998616457\n",
      "################################  814  ################################\n",
      "Loss:  0.0001107349235098809\n",
      "################################  815  ################################\n",
      "Loss:  0.00011035554052796215\n",
      "################################  816  ################################\n",
      "Loss:  0.00010996137280017138\n",
      "################################  817  ################################\n",
      "Loss:  0.00010961549560306594\n",
      "################################  818  ################################\n",
      "Loss:  0.00010929232666967437\n",
      "################################  819  ################################\n",
      "Loss:  0.00010895775631070137\n",
      "################################  820  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.00010865223885048181\n",
      "################################  821  ################################\n",
      "Loss:  0.00010835380817297846\n",
      "################################  822  ################################\n",
      "Loss:  0.00010808498336700723\n",
      "################################  823  ################################\n",
      "Loss:  0.00010782858589664102\n",
      "################################  824  ################################\n",
      "Loss:  0.00010757487325463444\n",
      "################################  825  ################################\n",
      "Loss:  0.00010733586532296613\n",
      "################################  826  ################################\n",
      "Loss:  0.00010710334754548967\n",
      "################################  827  ################################\n",
      "Loss:  0.00010688035399653018\n",
      "################################  828  ################################\n",
      "Loss:  0.00010666767047950998\n",
      "################################  829  ################################\n",
      "Loss:  0.00010646114242263138\n",
      "################################  830  ################################\n",
      "Loss:  0.00010625892173266038\n",
      "################################  831  ################################\n",
      "Loss:  0.00010606089199427515\n",
      "################################  832  ################################\n",
      "Loss:  0.0001058626949088648\n",
      "################################  833  ################################\n",
      "Loss:  0.00010566966375336051\n",
      "################################  834  ################################\n",
      "Loss:  0.00010547052079346031\n",
      "################################  835  ################################\n",
      "Loss:  0.00010527134872972965\n",
      "################################  836  ################################\n",
      "Loss:  0.00010506331454962492\n",
      "################################  837  ################################\n",
      "Loss:  0.00010484767699381337\n",
      "################################  838  ################################\n",
      "Loss:  0.00010462771024322137\n",
      "################################  839  ################################\n",
      "Loss:  0.00010439657489769161\n",
      "################################  840  ################################\n",
      "Loss:  0.00010416396253276616\n",
      "################################  841  ################################\n",
      "Loss:  0.00010394678974989802\n",
      "################################  842  ################################\n",
      "Loss:  0.00010372951510362327\n",
      "################################  843  ################################\n",
      "Loss:  0.00010352267418056726\n",
      "################################  844  ################################\n",
      "Loss:  0.00010331495286663994\n",
      "################################  845  ################################\n",
      "Loss:  0.00010310298239346594\n",
      "################################  846  ################################\n",
      "Loss:  0.0001028978222166188\n",
      "################################  847  ################################\n",
      "Loss:  0.00010268548794556409\n",
      "################################  848  ################################\n",
      "Loss:  0.00010250017658108845\n",
      "################################  849  ################################\n",
      "Loss:  0.00010230820043943822\n",
      "################################  850  ################################\n",
      "Loss:  0.00010214755457127467\n",
      "################################  851  ################################\n",
      "Loss:  0.00010199144162470475\n",
      "################################  852  ################################\n",
      "Loss:  0.00010181585093960166\n",
      "################################  853  ################################\n",
      "Loss:  0.00010161659884033725\n",
      "################################  854  ################################\n",
      "Loss:  0.00010142728569917381\n",
      "################################  855  ################################\n",
      "Loss:  0.00010126976849278435\n",
      "################################  856  ################################\n",
      "Loss:  0.0001010754203889519\n",
      "################################  857  ################################\n",
      "Loss:  0.00010087341797770932\n",
      "################################  858  ################################\n",
      "Loss:  0.00010067092080134898\n",
      "################################  859  ################################\n",
      "Loss:  0.00010043483052868396\n",
      "################################  860  ################################\n",
      "Loss:  0.00010016730084316805\n",
      "################################  861  ################################\n",
      "Loss:  9.988810052163899e-05\n",
      "################################  862  ################################\n",
      "Loss:  9.957767906598747e-05\n",
      "################################  863  ################################\n",
      "Loss:  9.929604129865766e-05\n",
      "################################  864  ################################\n",
      "Loss:  9.899988071992993e-05\n",
      "################################  865  ################################\n",
      "Loss:  9.870493522612378e-05\n",
      "################################  866  ################################\n",
      "Loss:  9.838597907219082e-05\n",
      "################################  867  ################################\n",
      "Loss:  9.805220179259777e-05\n",
      "################################  868  ################################\n",
      "Loss:  9.771278564585373e-05\n",
      "################################  869  ################################\n",
      "Loss:  9.737125947140157e-05\n",
      "################################  870  ################################\n",
      "Loss:  9.704518743092194e-05\n",
      "################################  871  ################################\n",
      "Loss:  9.669216524343938e-05\n",
      "################################  872  ################################\n",
      "Loss:  9.634880552766845e-05\n",
      "################################  873  ################################\n",
      "Loss:  9.601303463568911e-05\n",
      "################################  874  ################################\n",
      "Loss:  9.565911022946239e-05\n",
      "################################  875  ################################\n",
      "Loss:  9.534001583233476e-05\n",
      "################################  876  ################################\n",
      "Loss:  9.503280307399109e-05\n",
      "################################  877  ################################\n",
      "Loss:  9.474229591432959e-05\n",
      "################################  878  ################################\n",
      "Loss:  9.447737829759717e-05\n",
      "################################  879  ################################\n",
      "Loss:  9.419159323442727e-05\n",
      "################################  880  ################################\n",
      "Loss:  9.395327651873231e-05\n",
      "################################  881  ################################\n",
      "Loss:  9.365809091832489e-05\n",
      "################################  882  ################################\n",
      "Loss:  9.338634845335037e-05\n",
      "################################  883  ################################\n",
      "Loss:  9.308289736509323e-05\n",
      "################################  884  ################################\n",
      "Loss:  9.276224591303617e-05\n",
      "################################  885  ################################\n",
      "Loss:  9.248951391782612e-05\n",
      "################################  886  ################################\n",
      "Loss:  9.221972868544981e-05\n",
      "################################  887  ################################\n",
      "Loss:  9.194044832838699e-05\n",
      "################################  888  ################################\n",
      "Loss:  9.165404480881989e-05\n",
      "################################  889  ################################\n",
      "Loss:  9.135739674093202e-05\n",
      "################################  890  ################################\n",
      "Loss:  9.10604721866548e-05\n",
      "################################  891  ################################\n",
      "Loss:  9.079260780708864e-05\n",
      "################################  892  ################################\n",
      "Loss:  9.05356791918166e-05\n",
      "################################  893  ################################\n",
      "Loss:  9.029038483276963e-05\n",
      "################################  894  ################################\n",
      "Loss:  9.005919855553657e-05\n",
      "################################  895  ################################\n",
      "Loss:  8.983982115751132e-05\n",
      "################################  896  ################################\n",
      "Loss:  8.964469452621415e-05\n",
      "################################  897  ################################\n",
      "Loss:  8.945092849899083e-05\n",
      "################################  898  ################################\n",
      "Loss:  8.927055750973523e-05\n",
      "################################  899  ################################\n",
      "Loss:  8.910085307434201e-05\n",
      "################################  900  ################################\n",
      "Loss:  8.891736069926992e-05\n",
      "################################  901  ################################\n",
      "Loss:  8.877927757566795e-05\n",
      "################################  902  ################################\n",
      "Loss:  8.86389534571208e-05\n",
      "################################  903  ################################\n",
      "Loss:  8.850880840327591e-05\n",
      "################################  904  ################################\n",
      "Loss:  8.839192742016166e-05\n",
      "################################  905  ################################\n",
      "Loss:  8.818092464935035e-05\n",
      "################################  906  ################################\n",
      "Loss:  8.802708907751366e-05\n",
      "################################  907  ################################\n",
      "Loss:  8.7796724983491e-05\n",
      "################################  908  ################################\n",
      "Loss:  8.751764835324138e-05\n",
      "################################  909  ################################\n",
      "Loss:  8.717608579900116e-05\n",
      "################################  910  ################################\n",
      "Loss:  8.691418042872101e-05\n",
      "################################  911  ################################\n",
      "Loss:  8.662696927785873e-05\n",
      "################################  912  ################################\n",
      "Loss:  8.628630894236267e-05\n",
      "################################  913  ################################\n",
      "Loss:  8.597799751441926e-05\n",
      "################################  914  ################################\n",
      "Loss:  8.568168414058164e-05\n",
      "################################  915  ################################\n",
      "Loss:  8.535446249879897e-05\n",
      "################################  916  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  8.507068559993058e-05\n",
      "################################  917  ################################\n",
      "Loss:  8.481297118123621e-05\n",
      "################################  918  ################################\n",
      "Loss:  8.456136856693774e-05\n",
      "################################  919  ################################\n",
      "Loss:  8.432436879957095e-05\n",
      "################################  920  ################################\n",
      "Loss:  8.411436283495277e-05\n",
      "################################  921  ################################\n",
      "Loss:  8.390344009967521e-05\n",
      "################################  922  ################################\n",
      "Loss:  8.37172192404978e-05\n",
      "################################  923  ################################\n",
      "Loss:  8.35213577374816e-05\n",
      "################################  924  ################################\n",
      "Loss:  8.335110760526732e-05\n",
      "################################  925  ################################\n",
      "Loss:  8.317948959302157e-05\n",
      "################################  926  ################################\n",
      "Loss:  8.299105684272945e-05\n",
      "################################  927  ################################\n",
      "Loss:  8.281532063847408e-05\n",
      "################################  928  ################################\n",
      "Loss:  8.264863572549075e-05\n",
      "################################  929  ################################\n",
      "Loss:  8.248636004282162e-05\n",
      "################################  930  ################################\n",
      "Loss:  8.233212429331616e-05\n",
      "################################  931  ################################\n",
      "Loss:  8.21784560685046e-05\n",
      "################################  932  ################################\n",
      "Loss:  8.202921890188009e-05\n",
      "################################  933  ################################\n",
      "Loss:  8.188203355530277e-05\n",
      "################################  934  ################################\n",
      "Loss:  8.173539390554652e-05\n",
      "################################  935  ################################\n",
      "Loss:  8.15916937426664e-05\n",
      "################################  936  ################################\n",
      "Loss:  8.144557068590075e-05\n",
      "################################  937  ################################\n",
      "Loss:  8.130285277729854e-05\n",
      "################################  938  ################################\n",
      "Loss:  8.115747186820954e-05\n",
      "################################  939  ################################\n",
      "Loss:  8.101404819171876e-05\n",
      "################################  940  ################################\n",
      "Loss:  8.086890011327341e-05\n",
      "################################  941  ################################\n",
      "Loss:  8.072835771599784e-05\n",
      "################################  942  ################################\n",
      "Loss:  8.058820094447583e-05\n",
      "################################  943  ################################\n",
      "Loss:  8.044661080930382e-05\n",
      "################################  944  ################################\n",
      "Loss:  8.031183097045869e-05\n",
      "################################  945  ################################\n",
      "Loss:  8.017822983674705e-05\n",
      "################################  946  ################################\n",
      "Loss:  8.00517009338364e-05\n",
      "################################  947  ################################\n",
      "Loss:  7.992773316800594e-05\n",
      "################################  948  ################################\n",
      "Loss:  7.980453665368259e-05\n",
      "################################  949  ################################\n",
      "Loss:  7.96848107711412e-05\n",
      "################################  950  ################################\n",
      "Loss:  7.95605665189214e-05\n",
      "################################  951  ################################\n",
      "Loss:  7.944661047076806e-05\n",
      "################################  952  ################################\n",
      "Loss:  7.933517917990685e-05\n",
      "################################  953  ################################\n",
      "Loss:  7.92266582720913e-05\n",
      "################################  954  ################################\n",
      "Loss:  7.911491411505267e-05\n",
      "################################  955  ################################\n",
      "Loss:  7.899269985500723e-05\n",
      "################################  956  ################################\n",
      "Loss:  7.886502135079354e-05\n",
      "################################  957  ################################\n",
      "Loss:  7.874405127950013e-05\n",
      "################################  958  ################################\n",
      "Loss:  7.861920312279835e-05\n",
      "################################  959  ################################\n",
      "Loss:  7.84992371336557e-05\n",
      "################################  960  ################################\n",
      "Loss:  7.838141027605161e-05\n",
      "################################  961  ################################\n",
      "Loss:  7.824692147551104e-05\n",
      "################################  962  ################################\n",
      "Loss:  7.811540854163468e-05\n",
      "################################  963  ################################\n",
      "Loss:  7.795826240908355e-05\n",
      "################################  964  ################################\n",
      "Loss:  7.78206522227265e-05\n",
      "################################  965  ################################\n",
      "Loss:  7.763860776321962e-05\n",
      "################################  966  ################################\n",
      "Loss:  7.748473581159487e-05\n",
      "################################  967  ################################\n",
      "Loss:  7.730856304988265e-05\n",
      "################################  968  ################################\n",
      "Loss:  7.71083141444251e-05\n",
      "################################  969  ################################\n",
      "Loss:  7.691234350204468e-05\n",
      "################################  970  ################################\n",
      "Loss:  7.671173079870641e-05\n",
      "################################  971  ################################\n",
      "Loss:  7.65179647714831e-05\n",
      "################################  972  ################################\n",
      "Loss:  7.632743654539809e-05\n",
      "################################  973  ################################\n",
      "Loss:  7.613911293447018e-05\n",
      "################################  974  ################################\n",
      "Loss:  7.596267096232623e-05\n",
      "################################  975  ################################\n",
      "Loss:  7.578702206956223e-05\n",
      "################################  976  ################################\n",
      "Loss:  7.56300360080786e-05\n",
      "################################  977  ################################\n",
      "Loss:  7.547884888481349e-05\n",
      "################################  978  ################################\n",
      "Loss:  7.533283496741205e-05\n",
      "################################  979  ################################\n",
      "Loss:  7.51911211409606e-05\n",
      "################################  980  ################################\n",
      "Loss:  7.504860695917159e-05\n",
      "################################  981  ################################\n",
      "Loss:  7.491295400541276e-05\n",
      "################################  982  ################################\n",
      "Loss:  7.477107283193618e-05\n",
      "################################  983  ################################\n",
      "Loss:  7.464343070751056e-05\n",
      "################################  984  ################################\n",
      "Loss:  7.452406862284988e-05\n",
      "################################  985  ################################\n",
      "Loss:  7.440281478920951e-05\n",
      "################################  986  ################################\n",
      "Loss:  7.426925731124356e-05\n",
      "################################  987  ################################\n",
      "Loss:  7.415546861011535e-05\n",
      "################################  988  ################################\n",
      "Loss:  7.404836651403457e-05\n",
      "################################  989  ################################\n",
      "Loss:  7.390652172034606e-05\n",
      "################################  990  ################################\n",
      "Loss:  7.37585942260921e-05\n",
      "################################  991  ################################\n",
      "Loss:  7.355120760621503e-05\n",
      "################################  992  ################################\n",
      "Loss:  7.335736881941557e-05\n",
      "################################  993  ################################\n",
      "Loss:  7.310857472475618e-05\n",
      "################################  994  ################################\n",
      "Loss:  7.282366277649999e-05\n",
      "################################  995  ################################\n",
      "Loss:  7.250985072460026e-05\n",
      "################################  996  ################################\n",
      "Loss:  7.22135737305507e-05\n",
      "################################  997  ################################\n",
      "Loss:  7.19123927410692e-05\n",
      "################################  998  ################################\n",
      "Loss:  7.160840323194861e-05\n",
      "################################  999  ################################\n",
      "Loss:  7.130960148060694e-05\n",
      "################################  1000  ################################\n",
      "Loss:  7.103105599526316e-05\n",
      "################################  1001  ################################\n",
      "Loss:  7.076351903378963e-05\n",
      "################################  1002  ################################\n",
      "Loss:  7.050824933685362e-05\n",
      "################################  1003  ################################\n",
      "Loss:  7.027859101071954e-05\n",
      "################################  1004  ################################\n",
      "Loss:  7.008158718235791e-05\n",
      "################################  1005  ################################\n",
      "Loss:  6.990914698690176e-05\n",
      "################################  1006  ################################\n",
      "Loss:  6.9752088165842e-05\n",
      "################################  1007  ################################\n",
      "Loss:  6.960312748560682e-05\n",
      "################################  1008  ################################\n",
      "Loss:  6.944780761841685e-05\n",
      "################################  1009  ################################\n",
      "Loss:  6.932822725502774e-05\n",
      "################################  1010  ################################\n",
      "Loss:  6.92260218784213e-05\n",
      "################################  1011  ################################\n",
      "Loss:  6.912129902048036e-05\n",
      "################################  1012  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  6.90255910740234e-05\n",
      "################################  1013  ################################\n",
      "Loss:  6.893817044328898e-05\n",
      "################################  1014  ################################\n",
      "Loss:  6.882822344778106e-05\n",
      "################################  1015  ################################\n",
      "Loss:  6.873933307360858e-05\n",
      "################################  1016  ################################\n",
      "Loss:  6.858147389721125e-05\n",
      "################################  1017  ################################\n",
      "Loss:  6.840098649263382e-05\n",
      "################################  1018  ################################\n",
      "Loss:  6.814131484134123e-05\n",
      "################################  1019  ################################\n",
      "Loss:  6.786709855077788e-05\n",
      "################################  1020  ################################\n",
      "Loss:  6.752533954568207e-05\n",
      "################################  1021  ################################\n",
      "Loss:  6.724208651576191e-05\n",
      "################################  1022  ################################\n",
      "Loss:  6.690337613690645e-05\n",
      "################################  1023  ################################\n",
      "Loss:  6.659113569185138e-05\n",
      "################################  1024  ################################\n",
      "Loss:  6.628775736317039e-05\n",
      "################################  1025  ################################\n",
      "Loss:  6.594833394046873e-05\n",
      "################################  1026  ################################\n",
      "Loss:  6.568259414052591e-05\n",
      "################################  1027  ################################\n",
      "Loss:  6.54135801596567e-05\n",
      "################################  1028  ################################\n",
      "Loss:  6.517912697745487e-05\n",
      "################################  1029  ################################\n",
      "Loss:  6.495298293884844e-05\n",
      "################################  1030  ################################\n",
      "Loss:  6.471694359788671e-05\n",
      "################################  1031  ################################\n",
      "Loss:  6.450585351558402e-05\n",
      "################################  1032  ################################\n",
      "Loss:  6.432879308704287e-05\n",
      "################################  1033  ################################\n",
      "Loss:  6.414692325051874e-05\n",
      "################################  1034  ################################\n",
      "Loss:  6.399824633263052e-05\n",
      "################################  1035  ################################\n",
      "Loss:  6.38035562587902e-05\n",
      "################################  1036  ################################\n",
      "Loss:  6.363936699926853e-05\n",
      "################################  1037  ################################\n",
      "Loss:  6.346137524815276e-05\n",
      "################################  1038  ################################\n",
      "Loss:  6.326359289232641e-05\n",
      "################################  1039  ################################\n",
      "Loss:  6.305658462224528e-05\n",
      "################################  1040  ################################\n",
      "Loss:  6.286363350227475e-05\n",
      "################################  1041  ################################\n",
      "Loss:  6.266233685892075e-05\n",
      "################################  1042  ################################\n",
      "Loss:  6.247882993193343e-05\n",
      "################################  1043  ################################\n",
      "Loss:  6.228678103070706e-05\n",
      "################################  1044  ################################\n",
      "Loss:  6.207074329722673e-05\n",
      "################################  1045  ################################\n",
      "Loss:  6.186150858411565e-05\n",
      "################################  1046  ################################\n",
      "Loss:  6.165209924802184e-05\n",
      "################################  1047  ################################\n",
      "Loss:  6.14571399637498e-05\n",
      "################################  1048  ################################\n",
      "Loss:  6.126205698819831e-05\n",
      "################################  1049  ################################\n",
      "Loss:  6.106582441134378e-05\n",
      "################################  1050  ################################\n",
      "Loss:  6.0876696807099506e-05\n",
      "################################  1051  ################################\n",
      "Loss:  6.067811045795679e-05\n",
      "################################  1052  ################################\n",
      "Loss:  6.049525836715475e-05\n",
      "################################  1053  ################################\n",
      "Loss:  6.031732482369989e-05\n",
      "################################  1054  ################################\n",
      "Loss:  6.0152302467031404e-05\n",
      "################################  1055  ################################\n",
      "Loss:  5.998905544402078e-05\n",
      "################################  1056  ################################\n",
      "Loss:  5.983428854960948e-05\n",
      "################################  1057  ################################\n",
      "Loss:  5.969007179373875e-05\n",
      "################################  1058  ################################\n",
      "Loss:  5.954216976533644e-05\n",
      "################################  1059  ################################\n",
      "Loss:  5.9415295254439116e-05\n",
      "################################  1060  ################################\n",
      "Loss:  5.9293019148753956e-05\n",
      "################################  1061  ################################\n",
      "Loss:  5.91765419812873e-05\n",
      "################################  1062  ################################\n",
      "Loss:  5.90658892178908e-05\n",
      "################################  1063  ################################\n",
      "Loss:  5.895741924177855e-05\n",
      "################################  1064  ################################\n",
      "Loss:  5.88492039241828e-05\n",
      "################################  1065  ################################\n",
      "Loss:  5.8741497923620045e-05\n",
      "################################  1066  ################################\n",
      "Loss:  5.865050479769707e-05\n",
      "################################  1067  ################################\n",
      "Loss:  5.855362906004302e-05\n",
      "################################  1068  ################################\n",
      "Loss:  5.848293949384242e-05\n",
      "################################  1069  ################################\n",
      "Loss:  5.840703670401126e-05\n",
      "################################  1070  ################################\n",
      "Loss:  5.832401438965462e-05\n",
      "################################  1071  ################################\n",
      "Loss:  5.8241748774889857e-05\n",
      "################################  1072  ################################\n",
      "Loss:  5.815314216306433e-05\n",
      "################################  1073  ################################\n",
      "Loss:  5.807072375318967e-05\n",
      "################################  1074  ################################\n",
      "Loss:  5.799253995064646e-05\n",
      "################################  1075  ################################\n",
      "Loss:  5.791547300759703e-05\n",
      "################################  1076  ################################\n",
      "Loss:  5.783919186796993e-05\n",
      "################################  1077  ################################\n",
      "Loss:  5.7770048442762345e-05\n",
      "################################  1078  ################################\n",
      "Loss:  5.770381903857924e-05\n",
      "################################  1079  ################################\n",
      "Loss:  5.76369566260837e-05\n",
      "################################  1080  ################################\n",
      "Loss:  5.75740632484667e-05\n",
      "################################  1081  ################################\n",
      "Loss:  5.751034041168168e-05\n",
      "################################  1082  ################################\n",
      "Loss:  5.7449604355497286e-05\n",
      "################################  1083  ################################\n",
      "Loss:  5.738966865465045e-05\n",
      "################################  1084  ################################\n",
      "Loss:  5.7329169067088515e-05\n",
      "################################  1085  ################################\n",
      "Loss:  5.727087409468368e-05\n",
      "################################  1086  ################################\n",
      "Loss:  5.720894114347175e-05\n",
      "################################  1087  ################################\n",
      "Loss:  5.71543860132806e-05\n",
      "################################  1088  ################################\n",
      "Loss:  5.7095963711617514e-05\n",
      "################################  1089  ################################\n",
      "Loss:  5.704375871573575e-05\n",
      "################################  1090  ################################\n",
      "Loss:  5.6987199059221894e-05\n",
      "################################  1091  ################################\n",
      "Loss:  5.69257463212125e-05\n",
      "################################  1092  ################################\n",
      "Loss:  5.6862390920287e-05\n",
      "################################  1093  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.679182140738703e-05\n",
      "################################  1094  ################################\n",
      "Loss:  5.671848339261487e-05\n",
      "################################  1095  ################################\n",
      "Loss:  5.66372909815982e-05\n",
      "################################  1096  ################################\n",
      "Loss:  5.6553846661699936e-05\n",
      "################################  1097  ################################\n",
      "Loss:  5.6467491958756e-05\n",
      "################################  1098  ################################\n",
      "Loss:  5.6378921726718545e-05\n",
      "################################  1099  ################################\n",
      "Loss:  5.629180668620393e-05\n",
      "################################  1100  ################################\n",
      "Loss:  5.6202945415861905e-05\n",
      "################################  1101  ################################\n",
      "Loss:  5.611540109384805e-05\n",
      "################################  1102  ################################\n",
      "Loss:  5.6025590311037377e-05\n",
      "################################  1103  ################################\n",
      "Loss:  5.593925016000867e-05\n",
      "################################  1104  ################################\n",
      "Loss:  5.585241160588339e-05\n",
      "################################  1105  ################################\n",
      "Loss:  5.5767493904568255e-05\n",
      "################################  1106  ################################\n",
      "Loss:  5.568417691392824e-05\n",
      "################################  1107  ################################\n",
      "Loss:  5.5597582104383036e-05\n",
      "################################  1108  ################################\n",
      "Loss:  5.552044603973627e-05\n",
      "################################  1109  ################################\n",
      "Loss:  5.544532177736983e-05\n",
      "################################  1110  ################################\n",
      "Loss:  5.537007746170275e-05\n",
      "################################  1111  ################################\n",
      "Loss:  5.529243208002299e-05\n",
      "################################  1112  ################################\n",
      "Loss:  5.5229756981134415e-05\n",
      "################################  1113  ################################\n",
      "Loss:  5.5172051361296326e-05\n",
      "################################  1114  ################################\n",
      "Loss:  5.511292692972347e-05\n",
      "################################  1115  ################################\n",
      "Loss:  5.505566514329985e-05\n",
      "################################  1116  ################################\n",
      "Loss:  5.500063707586378e-05\n",
      "################################  1117  ################################\n",
      "Loss:  5.494853394338861e-05\n",
      "################################  1118  ################################\n",
      "Loss:  5.4900781833566725e-05\n",
      "################################  1119  ################################\n",
      "Loss:  5.4849893786013126e-05\n",
      "################################  1120  ################################\n",
      "Loss:  5.4800097132101655e-05\n",
      "################################  1121  ################################\n",
      "Loss:  5.4743708460591733e-05\n",
      "################################  1122  ################################\n",
      "Loss:  5.4678836022503674e-05\n",
      "################################  1123  ################################\n",
      "Loss:  5.461249384097755e-05\n",
      "################################  1124  ################################\n",
      "Loss:  5.4545700550079346e-05\n",
      "################################  1125  ################################\n",
      "Loss:  5.4479802201967686e-05\n",
      "################################  1126  ################################\n",
      "Loss:  5.440897075459361e-05\n",
      "################################  1127  ################################\n",
      "Loss:  5.4340067435987294e-05\n",
      "################################  1128  ################################\n",
      "Loss:  5.4266623919829726e-05\n",
      "################################  1129  ################################\n",
      "Loss:  5.420534944278188e-05\n",
      "################################  1130  ################################\n",
      "Loss:  5.412665268522687e-05\n",
      "################################  1131  ################################\n",
      "Loss:  5.4046453442424536e-05\n",
      "################################  1132  ################################\n",
      "Loss:  5.3961492085363716e-05\n",
      "################################  1133  ################################\n",
      "Loss:  5.387610144680366e-05\n",
      "################################  1134  ################################\n",
      "Loss:  5.378987407311797e-05\n",
      "################################  1135  ################################\n",
      "Loss:  5.370333747123368e-05\n",
      "################################  1136  ################################\n",
      "Loss:  5.3625226428266615e-05\n",
      "################################  1137  ################################\n",
      "Loss:  5.354415043257177e-05\n",
      "################################  1138  ################################\n",
      "Loss:  5.346947000361979e-05\n",
      "################################  1139  ################################\n",
      "Loss:  5.3394473070511594e-05\n",
      "################################  1140  ################################\n",
      "Loss:  5.332032742444426e-05\n",
      "################################  1141  ################################\n",
      "Loss:  5.3248317271936685e-05\n",
      "################################  1142  ################################\n",
      "Loss:  5.3175815992290154e-05\n",
      "################################  1143  ################################\n",
      "Loss:  5.311103814165108e-05\n",
      "################################  1144  ################################\n",
      "Loss:  5.3050775022711605e-05\n",
      "################################  1145  ################################\n",
      "Loss:  5.299514305079356e-05\n",
      "################################  1146  ################################\n",
      "Loss:  5.294364018482156e-05\n",
      "################################  1147  ################################\n",
      "Loss:  5.289567343425006e-05\n",
      "################################  1148  ################################\n",
      "Loss:  5.2849456551484764e-05\n",
      "################################  1149  ################################\n",
      "Loss:  5.280716868583113e-05\n",
      "################################  1150  ################################\n",
      "Loss:  5.276612864690833e-05\n",
      "################################  1151  ################################\n",
      "Loss:  5.2727642469108105e-05\n",
      "################################  1152  ################################\n",
      "Loss:  5.268810855341144e-05\n",
      "################################  1153  ################################\n",
      "Loss:  5.264907667879015e-05\n",
      "################################  1154  ################################\n",
      "Loss:  5.260936450213194e-05\n",
      "################################  1155  ################################\n",
      "Loss:  5.256175791146234e-05\n",
      "################################  1156  ################################\n",
      "Loss:  5.2512499678414315e-05\n",
      "################################  1157  ################################\n",
      "Loss:  5.2462390158325434e-05\n",
      "################################  1158  ################################\n",
      "Loss:  5.241391409072094e-05\n",
      "################################  1159  ################################\n",
      "Loss:  5.235604112385772e-05\n",
      "################################  1160  ################################\n",
      "Loss:  5.230997339822352e-05\n",
      "################################  1161  ################################\n",
      "Loss:  5.224133201409131e-05\n",
      "################################  1162  ################################\n",
      "Loss:  5.2177383622620255e-05\n",
      "################################  1163  ################################\n",
      "Loss:  5.208479706197977e-05\n",
      "################################  1164  ################################\n",
      "Loss:  5.198427970753983e-05\n",
      "################################  1165  ################################\n",
      "Loss:  5.18701781402342e-05\n",
      "################################  1166  ################################\n",
      "Loss:  5.174257603357546e-05\n",
      "################################  1167  ################################\n",
      "Loss:  5.161079752724618e-05\n",
      "################################  1168  ################################\n",
      "Loss:  5.1469600293785334e-05\n",
      "################################  1169  ################################\n",
      "Loss:  5.136325489729643e-05\n",
      "################################  1170  ################################\n",
      "Loss:  5.125671668793075e-05\n",
      "################################  1171  ################################\n",
      "Loss:  5.114037048770115e-05\n",
      "################################  1172  ################################\n",
      "Loss:  5.103632065583952e-05\n",
      "################################  1173  ################################\n",
      "Loss:  5.094608786748722e-05\n",
      "################################  1174  ################################\n",
      "Loss:  5.085034354124218e-05\n",
      "################################  1175  ################################\n",
      "Loss:  5.0766793719958514e-05\n",
      "################################  1176  ################################\n",
      "Loss:  5.068215250503272e-05\n",
      "################################  1177  ################################\n",
      "Loss:  5.0585738790687174e-05\n",
      "################################  1178  ################################\n",
      "Loss:  5.048860839451663e-05\n",
      "################################  1179  ################################\n",
      "Loss:  5.040354881202802e-05\n",
      "################################  1180  ################################\n",
      "Loss:  5.031565160606988e-05\n",
      "################################  1181  ################################\n",
      "Loss:  5.024069469072856e-05\n",
      "################################  1182  ################################\n",
      "Loss:  5.016603245167062e-05\n",
      "################################  1183  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.00848691444844e-05\n",
      "################################  1184  ################################\n",
      "Loss:  5.000861347070895e-05\n",
      "################################  1185  ################################\n",
      "Loss:  4.9930913519347087e-05\n",
      "################################  1186  ################################\n",
      "Loss:  4.985466512152925e-05\n",
      "################################  1187  ################################\n",
      "Loss:  4.97779983561486e-05\n",
      "################################  1188  ################################\n",
      "Loss:  4.969767906004563e-05\n",
      "################################  1189  ################################\n",
      "Loss:  4.962035018252209e-05\n",
      "################################  1190  ################################\n",
      "Loss:  4.9543912609806284e-05\n",
      "################################  1191  ################################\n",
      "Loss:  4.946726403431967e-05\n",
      "################################  1192  ################################\n",
      "Loss:  4.939391146763228e-05\n",
      "################################  1193  ################################\n",
      "Loss:  4.931891453452408e-05\n",
      "################################  1194  ################################\n",
      "Loss:  4.924530003336258e-05\n",
      "################################  1195  ################################\n",
      "Loss:  4.916999023407698e-05\n",
      "################################  1196  ################################\n",
      "Loss:  4.9089459935203195e-05\n",
      "################################  1197  ################################\n",
      "Loss:  4.90097954752855e-05\n",
      "################################  1198  ################################\n",
      "Loss:  4.8926267481874675e-05\n",
      "################################  1199  ################################\n",
      "Loss:  4.884719237452373e-05\n",
      "################################  1200  ################################\n",
      "Loss:  4.8764737584860995e-05\n",
      "################################  1201  ################################\n",
      "Loss:  4.869193799095228e-05\n",
      "################################  1202  ################################\n",
      "Loss:  4.8616493586450815e-05\n",
      "################################  1203  ################################\n",
      "Loss:  4.8529931518714875e-05\n",
      "################################  1204  ################################\n",
      "Loss:  4.845223156735301e-05\n",
      "################################  1205  ################################\n",
      "Loss:  4.8368463467340916e-05\n",
      "################################  1206  ################################\n",
      "Loss:  4.8282905481755733e-05\n",
      "################################  1207  ################################\n",
      "Loss:  4.819503374164924e-05\n",
      "################################  1208  ################################\n",
      "Loss:  4.810555401491001e-05\n",
      "################################  1209  ################################\n",
      "Loss:  4.801690738531761e-05\n",
      "################################  1210  ################################\n",
      "Loss:  4.79242553410586e-05\n",
      "################################  1211  ################################\n",
      "Loss:  4.78382789879106e-05\n",
      "################################  1212  ################################\n",
      "Loss:  4.7750545490998775e-05\n",
      "################################  1213  ################################\n",
      "Loss:  4.766377242049202e-05\n",
      "################################  1214  ################################\n",
      "Loss:  4.757836723001674e-05\n",
      "################################  1215  ################################\n",
      "Loss:  4.749150321003981e-05\n",
      "################################  1216  ################################\n",
      "Loss:  4.740995791507885e-05\n",
      "################################  1217  ################################\n",
      "Loss:  4.7326891944976524e-05\n",
      "################################  1218  ################################\n",
      "Loss:  4.7242141590686515e-05\n",
      "################################  1219  ################################\n",
      "Loss:  4.715931572718546e-05\n",
      "################################  1220  ################################\n",
      "Loss:  4.706800609710626e-05\n",
      "################################  1221  ################################\n",
      "Loss:  4.698858901974745e-05\n",
      "################################  1222  ################################\n",
      "Loss:  4.6901470341254026e-05\n",
      "################################  1223  ################################\n",
      "Loss:  4.680048004956916e-05\n",
      "################################  1224  ################################\n",
      "Loss:  4.6699424274265766e-05\n",
      "################################  1225  ################################\n",
      "Loss:  4.6592809667345136e-05\n",
      "################################  1226  ################################\n",
      "Loss:  4.648810136131942e-05\n",
      "################################  1227  ################################\n",
      "Loss:  4.638016253011301e-05\n",
      "################################  1228  ################################\n",
      "Loss:  4.627909220289439e-05\n",
      "################################  1229  ################################\n",
      "Loss:  4.617832746589556e-05\n",
      "################################  1230  ################################\n",
      "Loss:  4.606577931554057e-05\n",
      "################################  1231  ################################\n",
      "Loss:  4.5962857257109135e-05\n",
      "################################  1232  ################################\n",
      "Loss:  4.5862085244152695e-05\n",
      "################################  1233  ################################\n",
      "Loss:  4.576490755425766e-05\n",
      "################################  1234  ################################\n",
      "Loss:  4.56660236523021e-05\n",
      "################################  1235  ################################\n",
      "Loss:  4.557299689622596e-05\n",
      "################################  1236  ################################\n",
      "Loss:  4.547921707853675e-05\n",
      "################################  1237  ################################\n",
      "Loss:  4.540031295618974e-05\n",
      "################################  1238  ################################\n",
      "Loss:  4.5318905904423445e-05\n",
      "################################  1239  ################################\n",
      "Loss:  4.5244876673677936e-05\n",
      "################################  1240  ################################\n",
      "Loss:  4.5153963583288714e-05\n",
      "################################  1241  ################################\n",
      "Loss:  4.5075517846271396e-05\n",
      "################################  1242  ################################\n",
      "Loss:  4.497823101701215e-05\n",
      "################################  1243  ################################\n",
      "Loss:  4.486973921302706e-05\n",
      "################################  1244  ################################\n",
      "Loss:  4.475328023545444e-05\n",
      "################################  1245  ################################\n",
      "Loss:  4.46373232989572e-05\n",
      "################################  1246  ################################\n",
      "Loss:  4.452431676327251e-05\n",
      "################################  1247  ################################\n",
      "Loss:  4.441163036972284e-05\n",
      "################################  1248  ################################\n",
      "Loss:  4.430640547070652e-05\n",
      "################################  1249  ################################\n",
      "Loss:  4.4202279241289943e-05\n",
      "################################  1250  ################################\n",
      "Loss:  4.410826659295708e-05\n",
      "################################  1251  ################################\n",
      "Loss:  4.4017306208843365e-05\n",
      "################################  1252  ################################\n",
      "Loss:  4.3935862777289e-05\n",
      "################################  1253  ################################\n",
      "Loss:  4.385668580653146e-05\n",
      "################################  1254  ################################\n",
      "Loss:  4.377884397399612e-05\n",
      "################################  1255  ################################\n",
      "Loss:  4.37056842201855e-05\n",
      "################################  1256  ################################\n",
      "Loss:  4.363041080068797e-05\n",
      "################################  1257  ################################\n",
      "Loss:  4.356323915999383e-05\n",
      "################################  1258  ################################\n",
      "Loss:  4.3497664591996e-05\n",
      "################################  1259  ################################\n",
      "Loss:  4.343788168625906e-05\n",
      "################################  1260  ################################\n",
      "Loss:  4.338209691923112e-05\n",
      "################################  1261  ################################\n",
      "Loss:  4.332009484642185e-05\n",
      "################################  1262  ################################\n",
      "Loss:  4.327025089878589e-05\n",
      "################################  1263  ################################\n",
      "Loss:  4.3219606595812365e-05\n",
      "################################  1264  ################################\n",
      "Loss:  4.317738057579845e-05\n",
      "################################  1265  ################################\n",
      "Loss:  4.31374937761575e-05\n",
      "################################  1266  ################################\n",
      "Loss:  4.309243377065286e-05\n",
      "################################  1267  ################################\n",
      "Loss:  4.304980393499136e-05\n",
      "################################  1268  ################################\n",
      "Loss:  4.300520959077403e-05\n",
      "################################  1269  ################################\n",
      "Loss:  4.295697726774961e-05\n",
      "################################  1270  ################################\n",
      "Loss:  4.2908821342280135e-05\n",
      "################################  1271  ################################\n",
      "Loss:  4.2865263822022825e-05\n",
      "################################  1272  ################################\n",
      "Loss:  4.282293957658112e-05\n",
      "################################  1273  ################################\n",
      "Loss:  4.277881816960871e-05\n",
      "################################  1274  ################################\n",
      "Loss:  4.273430386092514e-05\n",
      "################################  1275  ################################\n",
      "Loss:  4.268889097147621e-05\n",
      "################################  1276  ################################\n",
      "Loss:  4.263740993337706e-05\n",
      "################################  1277  ################################\n",
      "Loss:  4.259259003447369e-05\n",
      "################################  1278  ################################\n",
      "Loss:  4.2546897020656615e-05\n",
      "################################  1279  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  4.2499326809775084e-05\n",
      "################################  1280  ################################\n",
      "Loss:  4.2450679757166654e-05\n",
      "################################  1281  ################################\n",
      "Loss:  4.2401741666253656e-05\n",
      "################################  1282  ################################\n",
      "Loss:  4.2355022742412984e-05\n",
      "################################  1283  ################################\n",
      "Loss:  4.2307638068450615e-05\n",
      "################################  1284  ################################\n",
      "Loss:  4.22597186116036e-05\n",
      "################################  1285  ################################\n",
      "Loss:  4.221247945679352e-05\n",
      "################################  1286  ################################\n",
      "Loss:  4.2163985199294984e-05\n",
      "################################  1287  ################################\n",
      "Loss:  4.211722261970863e-05\n",
      "################################  1288  ################################\n",
      "Loss:  4.2069554183399305e-05\n",
      "################################  1289  ################################\n",
      "Loss:  4.2023370042443275e-05\n",
      "################################  1290  ################################\n",
      "Loss:  4.1977298678830266e-05\n",
      "################################  1291  ################################\n",
      "Loss:  4.1927174606826156e-05\n",
      "################################  1292  ################################\n",
      "Loss:  4.1879728087224066e-05\n",
      "################################  1293  ################################\n",
      "Loss:  4.183409328106791e-05\n",
      "################################  1294  ################################\n",
      "Loss:  4.178814560873434e-05\n",
      "################################  1295  ################################\n",
      "Loss:  4.174662899458781e-05\n",
      "################################  1296  ################################\n",
      "Loss:  4.170538886683062e-05\n",
      "################################  1297  ################################\n",
      "Loss:  4.166404323768802e-05\n",
      "################################  1298  ################################\n",
      "Loss:  4.162448749411851e-05\n",
      "################################  1299  ################################\n",
      "Loss:  4.157843432039954e-05\n",
      "################################  1300  ################################\n",
      "Loss:  4.1539158701198176e-05\n",
      "################################  1301  ################################\n",
      "Loss:  4.14845890190918e-05\n",
      "################################  1302  ################################\n",
      "Loss:  4.142951365793124e-05\n",
      "################################  1303  ################################\n",
      "Loss:  4.135279232286848e-05\n",
      "################################  1304  ################################\n",
      "Loss:  4.130016532144509e-05\n",
      "################################  1305  ################################\n",
      "Loss:  4.1240167774958536e-05\n",
      "################################  1306  ################################\n",
      "Loss:  4.116775016882457e-05\n",
      "################################  1307  ################################\n",
      "Loss:  4.1104634874500334e-05\n",
      "################################  1308  ################################\n",
      "Loss:  4.1038561903405935e-05\n",
      "################################  1309  ################################\n",
      "Loss:  4.098156205145642e-05\n",
      "################################  1310  ################################\n",
      "Loss:  4.0919039747677743e-05\n",
      "################################  1311  ################################\n",
      "Loss:  4.0851340600056574e-05\n",
      "################################  1312  ################################\n",
      "Loss:  4.07901025027968e-05\n",
      "################################  1313  ################################\n",
      "Loss:  4.072481897310354e-05\n",
      "################################  1314  ################################\n",
      "Loss:  4.0665792766958475e-05\n",
      "################################  1315  ################################\n",
      "Loss:  4.060759601998143e-05\n",
      "################################  1316  ################################\n",
      "Loss:  4.0545706724515185e-05\n",
      "################################  1317  ################################\n",
      "Loss:  4.0488528611604124e-05\n",
      "################################  1318  ################################\n",
      "Loss:  4.04374732170254e-05\n",
      "################################  1319  ################################\n",
      "Loss:  4.038529732497409e-05\n",
      "################################  1320  ################################\n",
      "Loss:  4.034021549159661e-05\n",
      "################################  1321  ################################\n",
      "Loss:  4.029725823784247e-05\n",
      "################################  1322  ################################\n",
      "Loss:  4.02543373638764e-05\n",
      "################################  1323  ################################\n",
      "Loss:  4.021422500954941e-05\n",
      "################################  1324  ################################\n",
      "Loss:  4.017136961920187e-05\n",
      "################################  1325  ################################\n",
      "Loss:  4.013540456071496e-05\n",
      "################################  1326  ################################\n",
      "Loss:  4.010176417068578e-05\n",
      "################################  1327  ################################\n",
      "Loss:  4.006774543086067e-05\n",
      "################################  1328  ################################\n",
      "Loss:  4.0034974517766386e-05\n",
      "################################  1329  ################################\n",
      "Loss:  4.000239277957007e-05\n",
      "################################  1330  ################################\n",
      "Loss:  3.997159728896804e-05\n",
      "################################  1331  ################################\n",
      "Loss:  3.993985228589736e-05\n",
      "################################  1332  ################################\n",
      "Loss:  3.990890036220662e-05\n",
      "################################  1333  ################################\n",
      "Loss:  3.987683157902211e-05\n",
      "################################  1334  ################################\n",
      "Loss:  3.984588693128899e-05\n",
      "################################  1335  ################################\n",
      "Loss:  3.9816070056986064e-05\n",
      "################################  1336  ################################\n",
      "Loss:  3.978611857746728e-05\n",
      "################################  1337  ################################\n",
      "Loss:  3.975724393967539e-05\n",
      "################################  1338  ################################\n",
      "Loss:  3.972602280555293e-05\n",
      "################################  1339  ################################\n",
      "Loss:  3.969395766034722e-05\n",
      "################################  1340  ################################\n",
      "Loss:  3.9664060750510544e-05\n",
      "################################  1341  ################################\n",
      "Loss:  3.963328344980255e-05\n",
      "################################  1342  ################################\n",
      "Loss:  3.9604103221790865e-05\n",
      "################################  1343  ################################\n",
      "Loss:  3.9574508264195174e-05\n",
      "################################  1344  ################################\n",
      "Loss:  3.953967825509608e-05\n",
      "################################  1345  ################################\n",
      "Loss:  3.950708924094215e-05\n",
      "################################  1346  ################################\n",
      "Loss:  3.9470807678299025e-05\n",
      "################################  1347  ################################\n",
      "Loss:  3.9438484236598015e-05\n",
      "################################  1348  ################################\n",
      "Loss:  3.939618181902915e-05\n",
      "################################  1349  ################################\n",
      "Loss:  3.935864515369758e-05\n",
      "################################  1350  ################################\n",
      "Loss:  3.930833554477431e-05\n",
      "################################  1351  ################################\n",
      "Loss:  3.926747740479186e-05\n",
      "################################  1352  ################################\n",
      "Loss:  3.921796815120615e-05\n",
      "################################  1353  ################################\n",
      "Loss:  3.9161772292573005e-05\n",
      "################################  1354  ################################\n",
      "Loss:  3.909320003003813e-05\n",
      "################################  1355  ################################\n",
      "Loss:  3.904734330717474e-05\n",
      "################################  1356  ################################\n",
      "Loss:  3.899521834682673e-05\n",
      "################################  1357  ################################\n",
      "Loss:  3.8938022044021636e-05\n",
      "################################  1358  ################################\n",
      "Loss:  3.887601997121237e-05\n",
      "################################  1359  ################################\n",
      "Loss:  3.8814647268736735e-05\n",
      "################################  1360  ################################\n",
      "Loss:  3.875557013088837e-05\n",
      "################################  1361  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.869826832669787e-05\n",
      "################################  1362  ################################\n",
      "Loss:  3.864684913423844e-05\n",
      "################################  1363  ################################\n",
      "Loss:  3.8599930121563375e-05\n",
      "################################  1364  ################################\n",
      "Loss:  3.855093746096827e-05\n",
      "################################  1365  ################################\n",
      "Loss:  3.851393921650015e-05\n",
      "################################  1366  ################################\n",
      "Loss:  3.847908737952821e-05\n",
      "################################  1367  ################################\n",
      "Loss:  3.844232560368255e-05\n",
      "################################  1368  ################################\n",
      "Loss:  3.8405501982197165e-05\n",
      "################################  1369  ################################\n",
      "Loss:  3.837255644612014e-05\n",
      "################################  1370  ################################\n",
      "Loss:  3.8338541344273835e-05\n",
      "################################  1371  ################################\n",
      "Loss:  3.830613422906026e-05\n",
      "################################  1372  ################################\n",
      "Loss:  3.827427644864656e-05\n",
      "################################  1373  ################################\n",
      "Loss:  3.8237743865465745e-05\n",
      "################################  1374  ################################\n",
      "Loss:  3.820210258709267e-05\n",
      "################################  1375  ################################\n",
      "Loss:  3.816099706455134e-05\n",
      "################################  1376  ################################\n",
      "Loss:  3.812637442024425e-05\n",
      "################################  1377  ################################\n",
      "Loss:  3.808047767961398e-05\n",
      "################################  1378  ################################\n",
      "Loss:  3.8021025829948485e-05\n",
      "################################  1379  ################################\n",
      "Loss:  3.7968689866829664e-05\n",
      "################################  1380  ################################\n",
      "Loss:  3.791403287323192e-05\n",
      "################################  1381  ################################\n",
      "Loss:  3.7848425563424826e-05\n",
      "################################  1382  ################################\n",
      "Loss:  3.7787944165756926e-05\n",
      "################################  1383  ################################\n",
      "Loss:  3.7722711567766964e-05\n",
      "################################  1384  ################################\n",
      "Loss:  3.7664194678654894e-05\n",
      "################################  1385  ################################\n",
      "Loss:  3.759795072255656e-05\n",
      "################################  1386  ################################\n",
      "Loss:  3.7526355299632996e-05\n",
      "################################  1387  ################################\n",
      "Loss:  3.7451565731316805e-05\n",
      "################################  1388  ################################\n",
      "Loss:  3.73766160919331e-05\n",
      "################################  1389  ################################\n",
      "Loss:  3.7305533624021336e-05\n",
      "################################  1390  ################################\n",
      "Loss:  3.723189729498699e-05\n",
      "################################  1391  ################################\n",
      "Loss:  3.7166912079555914e-05\n",
      "################################  1392  ################################\n",
      "Loss:  3.710213059093803e-05\n",
      "################################  1393  ################################\n",
      "Loss:  3.704072514665313e-05\n",
      "################################  1394  ################################\n",
      "Loss:  3.6975608963984996e-05\n",
      "################################  1395  ################################\n",
      "Loss:  3.690755693241954e-05\n",
      "################################  1396  ################################\n",
      "Loss:  3.683986506075598e-05\n",
      "################################  1397  ################################\n",
      "Loss:  3.678277425933629e-05\n",
      "################################  1398  ################################\n",
      "Loss:  3.67296306649223e-05\n",
      "################################  1399  ################################\n",
      "Loss:  3.666565316962078e-05\n",
      "################################  1400  ################################\n",
      "Loss:  3.660252696136013e-05\n",
      "################################  1401  ################################\n",
      "Loss:  3.6532426747726277e-05\n",
      "################################  1402  ################################\n",
      "Loss:  3.645800825324841e-05\n",
      "################################  1403  ################################\n",
      "Loss:  3.638261114247143e-05\n",
      "################################  1404  ################################\n",
      "Loss:  3.630449646152556e-05\n",
      "################################  1405  ################################\n",
      "Loss:  3.622756412369199e-05\n",
      "################################  1406  ################################\n",
      "Loss:  3.6157834983896464e-05\n",
      "################################  1407  ################################\n",
      "Loss:  3.609005216276273e-05\n",
      "################################  1408  ################################\n",
      "Loss:  3.60230405931361e-05\n",
      "################################  1409  ################################\n",
      "Loss:  3.595654561650008e-05\n",
      "################################  1410  ################################\n",
      "Loss:  3.5893532185582444e-05\n",
      "################################  1411  ################################\n",
      "Loss:  3.5834720620187e-05\n",
      "################################  1412  ################################\n",
      "Loss:  3.577937968657352e-05\n",
      "################################  1413  ################################\n",
      "Loss:  3.572594141587615e-05\n",
      "################################  1414  ################################\n",
      "Loss:  3.5674609534908086e-05\n",
      "################################  1415  ################################\n",
      "Loss:  3.562447091098875e-05\n",
      "################################  1416  ################################\n",
      "Loss:  3.5576016671257094e-05\n",
      "################################  1417  ################################\n",
      "Loss:  3.553281567292288e-05\n",
      "################################  1418  ################################\n",
      "Loss:  3.54886578861624e-05\n",
      "################################  1419  ################################\n",
      "Loss:  3.54495678038802e-05\n",
      "################################  1420  ################################\n",
      "Loss:  3.541051046340726e-05\n",
      "################################  1421  ################################\n",
      "Loss:  3.536589065333828e-05\n",
      "################################  1422  ################################\n",
      "Loss:  3.5325334465596825e-05\n",
      "################################  1423  ################################\n",
      "Loss:  3.5285644116811454e-05\n",
      "################################  1424  ################################\n",
      "Loss:  3.524650674080476e-05\n",
      "################################  1425  ################################\n",
      "Loss:  3.5208962799515575e-05\n",
      "################################  1426  ################################\n",
      "Loss:  3.517041477607563e-05\n",
      "################################  1427  ################################\n",
      "Loss:  3.5132281482219696e-05\n",
      "################################  1428  ################################\n",
      "Loss:  3.509494126774371e-05\n",
      "################################  1429  ################################\n",
      "Loss:  3.505792119540274e-05\n",
      "################################  1430  ################################\n",
      "Loss:  3.5022352676605806e-05\n",
      "################################  1431  ################################\n",
      "Loss:  3.4987628168892115e-05\n",
      "################################  1432  ################################\n",
      "Loss:  3.494964767014608e-05\n",
      "################################  1433  ################################\n",
      "Loss:  3.4914613934233785e-05\n",
      "################################  1434  ################################\n",
      "Loss:  3.487514550215565e-05\n",
      "################################  1435  ################################\n",
      "Loss:  3.483922773739323e-05\n",
      "################################  1436  ################################\n",
      "Loss:  3.480138548184186e-05\n",
      "################################  1437  ################################\n",
      "Loss:  3.4753458749037236e-05\n",
      "################################  1438  ################################\n",
      "Loss:  3.471336094662547e-05\n",
      "################################  1439  ################################\n",
      "Loss:  3.466540510999039e-05\n",
      "################################  1440  ################################\n",
      "Loss:  3.461223968770355e-05\n",
      "################################  1441  ################################\n",
      "Loss:  3.4552740544313565e-05\n",
      "################################  1442  ################################\n",
      "Loss:  3.449355426710099e-05\n",
      "################################  1443  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.442759407334961e-05\n",
      "################################  1444  ################################\n",
      "Loss:  3.436015686020255e-05\n",
      "################################  1445  ################################\n",
      "Loss:  3.42915445799008e-05\n",
      "################################  1446  ################################\n",
      "Loss:  3.4223165130242705e-05\n",
      "################################  1447  ################################\n",
      "Loss:  3.416255640331656e-05\n",
      "################################  1448  ################################\n",
      "Loss:  3.409524288144894e-05\n",
      "################################  1449  ################################\n",
      "Loss:  3.4041899198200554e-05\n",
      "################################  1450  ################################\n",
      "Loss:  3.3991149393841624e-05\n",
      "################################  1451  ################################\n",
      "Loss:  3.393841689103283e-05\n",
      "################################  1452  ################################\n",
      "Loss:  3.389770063222386e-05\n",
      "################################  1453  ################################\n",
      "Loss:  3.3861429983517155e-05\n",
      "################################  1454  ################################\n",
      "Loss:  3.382443537702784e-05\n",
      "################################  1455  ################################\n",
      "Loss:  3.3791831810958683e-05\n",
      "################################  1456  ################################\n",
      "Loss:  3.375508094904944e-05\n",
      "################################  1457  ################################\n",
      "Loss:  3.372885112185031e-05\n",
      "################################  1458  ################################\n",
      "Loss:  3.370530248503201e-05\n",
      "################################  1459  ################################\n",
      "Loss:  3.368087709532119e-05\n",
      "################################  1460  ################################\n",
      "Loss:  3.365815064171329e-05\n",
      "################################  1461  ################################\n",
      "Loss:  3.3634652936598286e-05\n",
      "################################  1462  ################################\n",
      "Loss:  3.361052949912846e-05\n",
      "################################  1463  ################################\n",
      "Loss:  3.3585718483664095e-05\n",
      "################################  1464  ################################\n",
      "Loss:  3.356158413225785e-05\n",
      "################################  1465  ################################\n",
      "Loss:  3.353804277139716e-05\n",
      "################################  1466  ################################\n",
      "Loss:  3.3514792448841035e-05\n",
      "################################  1467  ################################\n",
      "Loss:  3.349195685586892e-05\n",
      "################################  1468  ################################\n",
      "Loss:  3.3468972105765715e-05\n",
      "################################  1469  ################################\n",
      "Loss:  3.344596552778967e-05\n",
      "################################  1470  ################################\n",
      "Loss:  3.342251147842035e-05\n",
      "################################  1471  ################################\n",
      "Loss:  3.339898103149608e-05\n",
      "################################  1472  ################################\n",
      "Loss:  3.337538873893209e-05\n",
      "################################  1473  ################################\n",
      "Loss:  3.335069413878955e-05\n",
      "################################  1474  ################################\n",
      "Loss:  3.332513369969092e-05\n",
      "################################  1475  ################################\n",
      "Loss:  3.329816900077276e-05\n",
      "################################  1476  ################################\n",
      "Loss:  3.327171361888759e-05\n",
      "################################  1477  ################################\n",
      "Loss:  3.3243333746213466e-05\n",
      "################################  1478  ################################\n",
      "Loss:  3.321758413221687e-05\n",
      "################################  1479  ################################\n",
      "Loss:  3.318813833175227e-05\n",
      "################################  1480  ################################\n",
      "Loss:  3.3151743991766125e-05\n",
      "################################  1481  ################################\n",
      "Loss:  3.311437103548087e-05\n",
      "################################  1482  ################################\n",
      "Loss:  3.307935912744142e-05\n",
      "################################  1483  ################################\n",
      "Loss:  3.304445635876618e-05\n",
      "################################  1484  ################################\n",
      "Loss:  3.30076327372808e-05\n",
      "################################  1485  ################################\n",
      "Loss:  3.297518924227916e-05\n",
      "################################  1486  ################################\n",
      "Loss:  3.2942982215899974e-05\n",
      "################################  1487  ################################\n",
      "Loss:  3.290486347395927e-05\n",
      "################################  1488  ################################\n",
      "Loss:  3.2868214475456625e-05\n",
      "################################  1489  ################################\n",
      "Loss:  3.283166006440297e-05\n",
      "################################  1490  ################################\n",
      "Loss:  3.279648080933839e-05\n",
      "################################  1491  ################################\n",
      "Loss:  3.276035931776278e-05\n",
      "################################  1492  ################################\n",
      "Loss:  3.272660615039058e-05\n",
      "################################  1493  ################################\n",
      "Loss:  3.269196531618945e-05\n",
      "################################  1494  ################################\n",
      "Loss:  3.265346822445281e-05\n",
      "################################  1495  ################################\n",
      "Loss:  3.2615371310384944e-05\n",
      "################################  1496  ################################\n",
      "Loss:  3.257837670389563e-05\n",
      "################################  1497  ################################\n",
      "Loss:  3.254306284361519e-05\n",
      "################################  1498  ################################\n",
      "Loss:  3.250469308113679e-05\n",
      "################################  1499  ################################\n",
      "Loss:  3.247061977162957e-05\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1500\n",
    "history = fit(my_network, training_set, interior, n_epochs, optimizer_, p=2, verbose=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading Model\n",
    "FILE = \"maxwell2D_2.pth\"\n",
    "torch.save(my_network, FILE)\n",
    "\n",
    "# uncomment below when you need to test for different points\n",
    "#my_network = torch.load(FILE)\n",
    "#my_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading Model (this model with reported results)\n",
    "#FILE = \"second.pth\"\n",
    "#torch.save(my_network, FILE)\n",
    "\n",
    "# uncomment below when you need to test for different points\n",
    "#my_network = torch.load(FILE)\n",
    "#my_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(0, 1, 10000).reshape(-1,1)\n",
    "y_test = torch.linspace(0, 1, 10000).reshape(-1,1)\n",
    "t_test = torch.ones((10000,1))\n",
    "test = torch.cat([x_test, y_test, t_test],1)\n",
    "h_test = exact_solution_h(x_test, y_test, t_test).reshape(-1,1)\n",
    "e1_test = exact_solution_e1(x_test, y_test, t_test).reshape(-1,1)\n",
    "e2_test = exact_solution_e2(x_test, y_test, t_test).reshape(-1,1)\n",
    "w_test_pred = my_network(test)\n",
    "h_test_pred = w_test_pred[:,0].reshape(-1,1)\n",
    "e1_test_pred = w_test_pred[:,1].reshape(-1,1)\n",
    "e2_test_pred = w_test_pred[:,2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcc61b16910>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(x_test, h_test, label=\"Ground Truth\",lw=2)\n",
    "plt.plot(x_test, h_test_pred.detach(), label=\"Network Prediction\",lw=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"u\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  0.011657299182843417 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((h_test_pred - h_test)**2)/torch.mean(h_test**2)\n",
    "#relative_error_test = torch.max(torch.abs(u_test_pred -u_test))/torch.max(torch.abs(u_test))\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcc61adc970>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(x_test, e1_test, label=\"Ground Truth\",lw=2)\n",
    "plt.plot(x_test, e1_test_pred.detach(), label=\"Network Prediction\",lw=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"u\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  0.0020642175513785332 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((e1_test_pred - e1_test)**2)/torch.mean(e1_test**2)\n",
    "#relative_error_test = torch.max(torch.abs(u_test_pred -u_test))/torch.max(torch.abs(u_test))\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  0.002686636616999749 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((e2_test_pred - e2_test)**2)/torch.mean(e2_test**2)\n",
    "#relative_error_test = torch.max(torch.abs(u_test_pred -u_test))/torch.max(torch.abs(u_test))\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exact_solution_u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m t_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m100000\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_test, t_test],\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m u_test \u001b[38;5;241m=\u001b[39m \u001b[43mexact_solution_u\u001b[49m(x_test,t_test)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m p_test \u001b[38;5;241m=\u001b[39m exact_solution_p(x_test,t_test)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m u_test_pred \u001b[38;5;241m=\u001b[39m my_network(test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exact_solution_u' is not defined"
     ]
    }
   ],
   "source": [
    "model = my_network\n",
    "x_test = pi*torch.rand(100000).reshape(-1,1)\n",
    "t_test = torch.rand(100000).reshape(-1,1)\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution_u(x_test,t_test).reshape(-1,1)\n",
    "p_test = exact_solution_p(x_test,t_test).reshape(-1,1)\n",
    "u_test_pred = my_network(test)\n",
    "u_pred = u_test_pred[:, 0].reshape(-1,1)\n",
    "\n",
    "u_pred1 = u_test_pred[:, 1].reshape(-1,1)\n",
    "\n",
    "\n",
    "relative_error = torch.abs(u_pred- u_test)\n",
    "\n",
    "relative_error1 = torch.abs(u_pred1- p_test)\n",
    "u_pred = u_pred.detach().numpy()\n",
    "x_test = x_test.detach().numpy()\n",
    "t_test = t_test.detach().numpy()\n",
    "p_pred = u_pred1.detach().numpy()\n",
    "relative_error = relative_error.detach().numpy()\n",
    "relative_error1 = relative_error1.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = x_test.reshape(-1,)\n",
    "t_test = t_test.reshape(-1,)\n",
    "\n",
    "u_pred = u_pred.reshape(-1,)\n",
    "p_pred = p_pred.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "\n",
    "CS = plt.tricontourf(x_test, t_test, u_pred, 20, cmap='turbo')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(CS)\n",
    "for t in cbar.ax.get_yticklabels():\n",
    "     t.set_fontsize(20)\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('t', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "\n",
    "#plt.savefig('timo_u.png', dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CS = plt.tricontourf(x_test, t_test, p_pred, 20, cmap='turbo')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(CS)\n",
    "for t in cbar.ax.get_yticklabels():\n",
    "     t.set_fontsize(20)\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('t', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "#plt.savefig('timo_p.png', dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = x_test.reshape(-1, )\n",
    "t_test = t_test.reshape(-1, )\n",
    "relative_error = relative_error.reshape(-1,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CS = plt.tricontourf(x_test, t_test, relative_error, 20, cmap='turbo')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(CS)\n",
    "for t in cbar.ax.get_yticklabels():\n",
    "     t.set_fontsize(20)\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('t', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "#plt.savefig('relative_error_timo_u.png', dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error1 = relative_error1.reshape(-1,)\n",
    "\n",
    "CS = plt.tricontourf(x_test, t_test, relative_error1, 20, cmap='turbo')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(CS)\n",
    "for t in cbar.ax.get_yticklabels():\n",
    "     t.set_fontsize(20)\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('t', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "#plt.savefig('relative_error_timo_p.png', dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
