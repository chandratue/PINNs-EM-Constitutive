{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exact solution u = H\n",
    "def exact_solution_h(x, y, t):\n",
    "    return -torch.sin(x)*torch.sin(y)*torch.cos(t)\n",
    "\n",
    "def initial_condition_h(x, y):\n",
    "    return -torch.sin(x)*torch.sin(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exact solution p = E\n",
    "def exact_solution_e1(x, y, t):\n",
    "    return -torch.sin(x)*torch.sin(t)*torch.cos(y)\n",
    "\n",
    "def initial_condition_e1(x, y):\n",
    "    return 0.0*torch.sin(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exact solution p = E\n",
    "def exact_solution_e2(x, y, t):\n",
    "    return torch.sin(y)*torch.sin(t)*torch.cos(x)\n",
    "\n",
    "def initial_condition_e2(x, y):\n",
    "    return 0.0*torch.sin(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning number of points\n",
    "initial_pts = 200 \n",
    "left_boundary_pts = 200 \n",
    "right_boundary_pts = 200\n",
    "back_boundary_pts = 200\n",
    "front_boundary_pts = 200\n",
    "residual_pts = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of optimizer (ADAM or LBFGS)\n",
    "opt_type = \"LBFGS\"\n",
    "\n",
    "x_init = torch.rand((initial_pts,1)) # initial pts\n",
    "y_init = torch.rand((initial_pts,1))\n",
    "t_init =  0*x_init\n",
    "init =  torch.cat([x_init, y_init, t_init],1)\n",
    "h_init = initial_condition_h(init[:,0], init[:, 1]).reshape(-1, 1)\n",
    "e1_init = initial_condition_e1(init[:,0], init[:, 1]).reshape(-1, 1)\n",
    "e2_init = initial_condition_e2(init[:,0], init[:, 1]).reshape(-1, 1)\n",
    "w_init = torch.cat([h_init, e1_init, e2_init],1)\n",
    "\n",
    "\n",
    "xb_left = torch.zeros((left_boundary_pts, 1)) # left spatial boundary\n",
    "yb_left = torch.rand((left_boundary_pts, 1)) # left spatial boundary\n",
    "tb_left = torch.rand((left_boundary_pts, 1)) # \n",
    "b_left = torch.cat([xb_left, yb_left, tb_left ],1)\n",
    "h_b_l = exact_solution_h(xb_left, yb_left, tb_left).reshape(-1, 1)\n",
    "e1_b_l = exact_solution_e1(xb_left, yb_left, tb_left).reshape(-1, 1)\n",
    "e2_b_l = exact_solution_e2(xb_left, yb_left, tb_left).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "xb_right = torch.ones((right_boundary_pts, 1)) # right spatial boundary\n",
    "yb_right = torch.rand((right_boundary_pts, 1)) # right spatial boundary\n",
    "tb_right = torch.rand((right_boundary_pts, 1)) # right boundary pts\n",
    "b_right = torch.cat([xb_right, yb_right, tb_right ],1)\n",
    "h_b_r = exact_solution_h(xb_right, yb_right, tb_right).reshape(-1, 1)\n",
    "e1_b_r = exact_solution_e1(xb_right, yb_right, tb_right).reshape(-1, 1)\n",
    "e2_b_r = exact_solution_e2(xb_right, yb_right, tb_right).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "xb_front = torch.rand((front_boundary_pts, 1)) # front spatial boundary\n",
    "yb_front = torch.zeros((front_boundary_pts, 1)) # front spatial boundary\n",
    "tb_front = torch.rand((front_boundary_pts, 1)) # \n",
    "b_front = torch.cat([xb_front, yb_front, tb_front ],1)\n",
    "h_b_f = exact_solution_h(xb_front, yb_front, tb_front).reshape(-1, 1)\n",
    "e1_b_f = exact_solution_e1(xb_front, yb_front, tb_front).reshape(-1, 1)\n",
    "e2_b_f = exact_solution_e2(xb_front, yb_front, tb_front).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "xb_back = torch.rand((back_boundary_pts, 1)) # back spatial boundary\n",
    "yb_back = torch.ones((back_boundary_pts, 1)) # back spatial boundary\n",
    "tb_back = torch.rand((back_boundary_pts, 1)) # back boundary pts\n",
    "b_back = torch.cat([xb_back, yb_back, tb_back ],1)\n",
    "h_b_b = exact_solution_h(xb_back, yb_back, tb_back).reshape(-1, 1)\n",
    "e1_b_b = exact_solution_e1(xb_back, yb_back, tb_back).reshape(-1, 1)\n",
    "e2_b_b = exact_solution_e2(xb_back, yb_back, tb_back).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "x_interior = torch.rand((residual_pts, 1))\n",
    "y_interior = torch.rand((residual_pts, 1))\n",
    "t_interior = torch.rand((residual_pts, 1))\n",
    "interior = torch.cat([x_interior, y_interior, t_interior],1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_set = DataLoader(torch.utils.data.TensorDataset(init, w_init, b_left,  b_right, b_front, b_back), batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, output_dimension, n_hidden_layers, neurons):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer \n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers \n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function \n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers)])\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network \n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "my_network = NeuralNet(input_dimension = init.shape[1], output_dimension = w_init.shape[1], n_hidden_layers=4, neurons=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_xavier(model, retrain_seed):\n",
    "    torch.manual_seed(retrain_seed)\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "            g = nn.init.calculate_gain('tanh')\n",
    "            torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "            #torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "            m.bias.data.fill_(0)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "# Random Seed for weight initialization\n",
    "retrain = 128\n",
    "# Xavier weight initialization\n",
    "init_xavier(my_network, retrain)\n",
    "#print(my_network(init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt_type == \"ADAM\":\n",
    "    optimizer_ = optim.Adam(my_network.parameters(), lr=0.001)\n",
    "elif opt_type == \"LBFGS\":\n",
    "    optimizer_ = optim.LBFGS(my_network.parameters(), lr=0.1, max_iter=1, max_eval=50000, tolerance_change=1.0 * np.finfo(float).eps)\n",
    "else:\n",
    "    raise ValueError(\"Optimizer not recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, training_set, interior, num_epochs, optimizer, p, verbose=True):\n",
    "    history = list()\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "        running_loss = list([0])\n",
    "        \n",
    "        # Loop over batches\n",
    "        for j, (initial, w_initial, bd_left,  bd_right, bd_front, bd_back) in enumerate(training_set):\n",
    "            \n",
    "            def closure():\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # for initial\n",
    "                initial.requires_grad = True\n",
    "                w_initial_pred_ = model(initial)\n",
    "                h_initial_pred_ = w_initial_pred_[:,0].reshape(-1,1)\n",
    "                e1_initial_pred_ = w_initial_pred_[:,1].reshape(-1,1)\n",
    "                e2_initial_pred_ = w_initial_pred_[:,2].reshape(-1,1)\n",
    "                \n",
    "                \n",
    "                # with derivative\n",
    "                inpu = torch.ones(initial_pts, 1 )\n",
    "                \n",
    "                grad_h_ini = torch.autograd.grad(h_initial_pred_, initial, grad_outputs=inpu, create_graph=True, allow_unused=True)[0]\n",
    "                h_initial_t = grad_h_ini[:, 2]\n",
    "                \n",
    "                grad_e1_ini = torch.autograd.grad(e1_initial_pred_, initial, grad_outputs=inpu, create_graph=True, allow_unused=True)[0]\n",
    "                e1_initial_t = grad_e1_ini[:, 2]\n",
    "                \n",
    "                grad_e2_ini = torch.autograd.grad(e2_initial_pred_, initial, grad_outputs=inpu, create_graph=True, allow_unused=True)[0]\n",
    "                e2_initial_t = grad_e2_ini[:, 2]\n",
    "                \n",
    "                \n",
    "                \n",
    "                # for left boundary\n",
    "                w_bd_left_pred_ = model(bd_left)\n",
    "                h_bd_left_pred_ = w_bd_left_pred_[:,0].reshape(-1,1)\n",
    "                e1_bd_left_pred_ = w_bd_left_pred_[:,1].reshape(-1,1)\n",
    "                e2_bd_left_pred_ = w_bd_left_pred_[:,2].reshape(-1,1)\n",
    "                \n",
    "                # for right boundary\n",
    "                w_bd_right_pred_ = model(bd_right)\n",
    "                h_bd_right_pred_ = w_bd_right_pred_[:,0].reshape(-1,1)\n",
    "                e1_bd_right_pred_ = w_bd_right_pred_[:,1].reshape(-1,1)\n",
    "                e2_bd_right_pred_ = w_bd_right_pred_[:,2].reshape(-1,1)\n",
    "                \n",
    "                # for front boundary\n",
    "                w_bd_front_pred_ = model(bd_front)\n",
    "                h_bd_front_pred_ = w_bd_front_pred_[:,0].reshape(-1,1)\n",
    "                e1_bd_front_pred_ = w_bd_front_pred_[:,1].reshape(-1,1)\n",
    "                e2_bd_front_pred_ = w_bd_front_pred_[:,2].reshape(-1,1)\n",
    "                \n",
    "                # for back boundary\n",
    "                w_bd_back_pred_ = model(bd_back)\n",
    "                h_bd_back_pred_ = w_bd_back_pred_[:,0].reshape(-1,1)\n",
    "                e1_bd_back_pred_ = w_bd_back_pred_[:,1].reshape(-1,1)\n",
    "                e2_bd_back_pred_ = w_bd_back_pred_[:,2].reshape(-1,1)\n",
    "                \n",
    "                # residual calculation\n",
    "                interior.requires_grad = True\n",
    "                w_hat = model(interior)\n",
    "                h_hat = w_hat[:,0].reshape(-1,1)\n",
    "                e1_hat = w_hat[:,1].reshape(-1,1)\n",
    "                e2_hat = w_hat[:,2].reshape(-1,1)\n",
    "                \n",
    "                inputs = torch.ones(residual_pts, 1 )\n",
    "                inputs2 = torch.ones(residual_pts, 1)\n",
    "                \n",
    "                grad_h_hat = torch.autograd.grad(h_hat.reshape(-1,1), interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                h_x = grad_h_hat[:, 0].reshape(-1,1)\n",
    "                h_y = grad_h_hat[:, 1].reshape(-1,1)\n",
    "                \n",
    "                grad_e1_hat = torch.autograd.grad(e1_hat, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                e1_x = grad_e1_hat[:, 0].reshape(-1,1)\n",
    "                e1_y = grad_e1_hat[:, 1].reshape(-1,1)\n",
    "                \n",
    "                grad_e2_hat = torch.autograd.grad(e2_hat, interior, grad_outputs=inputs, create_graph=True)[0]\n",
    "                e2_x = grad_e2_hat[:, 0].reshape(-1,1)\n",
    "                e2_y = grad_e2_hat[:, 1].reshape(-1,1)\n",
    "                \n",
    "                h_t = grad_h_hat[:, 2].reshape(-1,1)\n",
    "                e1_t = grad_e1_hat[:, 2].reshape(-1,1)\n",
    "                e2_t = grad_e2_hat[:, 2].reshape(-1,1)\n",
    "                \n",
    "                \n",
    "                # Item 1. below\n",
    "                loss1 = torch.mean((h_initial_pred_.reshape(-1, ) - w_initial[:,0].reshape(-1, ))**p) + torch.mean((2*h_t.reshape(-1, ) + e2_x.reshape(-1, ) - e1_y.reshape(-1, ))**p)+torch.mean((h_bd_left_pred_.reshape(-1,)- h_b_l.reshape(-1,))**p) + torch.mean((h_bd_right_pred_.reshape(-1,)- h_b_r.reshape(-1,))**p) +torch.mean((h_bd_front_pred_.reshape(-1,)- h_b_f.reshape(-1,))**p) + torch.mean((h_bd_back_pred_.reshape(-1,)- h_b_b.reshape(-1,))**p)\n",
    "                loss2 = torch.mean((e1_initial_pred_.reshape(-1, ) - w_initial[:,1].reshape(-1, ))**p)+ torch.mean((3*e1_t.reshape(-1, )  - h_y.reshape(-1, ) + 2*torch.sin(interior[:, 0])*torch.cos(interior[:, 2])*torch.cos(interior[:, 1]))**p) +torch.mean((e1_bd_left_pred_.reshape(-1,)- e1_b_l.reshape(-1,))**p) + torch.mean((e1_bd_right_pred_.reshape(-1,)- e1_b_r.reshape(-1,))**p) +torch.mean((e1_bd_front_pred_.reshape(-1,)- e1_b_f.reshape(-1,))**p) + torch.mean((e1_bd_back_pred_.reshape(-1,)- e1_b_b.reshape(-1,))**p)\n",
    "                loss3 = torch.mean((e2_initial_pred_.reshape(-1, ) - w_initial[:,2].reshape(-1, ))**p)+ torch.mean((2*e2_t.reshape(-1, )  + h_x.reshape(-1, ) - torch.cos(interior[:, 0])*torch.cos(interior[:, 2])*torch.sin(interior[:, 1]))**p) +torch.mean((e2_bd_left_pred_.reshape(-1,)- e2_b_l.reshape(-1,))**p) + torch.mean((e2_bd_right_pred_.reshape(-1,)- e2_b_r.reshape(-1,))**p) +torch.mean((e2_bd_front_pred_.reshape(-1,)- e2_b_f.reshape(-1,))**p) + torch.mean((e2_bd_back_pred_.reshape(-1,)- e2_b_b.reshape(-1,))**p)\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                \n",
    "                # Item 2. below\n",
    "                loss.backward()\n",
    "                # Compute average training loss over batches for the current epoch\n",
    "                running_loss[0] += loss.item()\n",
    "                return loss\n",
    "            \n",
    "            # Item 3. below\n",
    "            optimizer.step(closure=closure)\n",
    "            \n",
    "        print('Loss: ', (running_loss[0] / len(training_set)))\n",
    "        history.append(running_loss[0])\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  0  ################################\n",
      "Loss:  94.36968994140625\n",
      "################################  1  ################################\n",
      "Loss:  92.66423797607422\n",
      "################################  2  ################################\n",
      "Loss:  77.3872299194336\n",
      "################################  3  ################################\n",
      "Loss:  57.76629638671875\n",
      "################################  4  ################################\n",
      "Loss:  41.545021057128906\n",
      "################################  5  ################################\n",
      "Loss:  34.75366973876953\n",
      "################################  6  ################################\n",
      "Loss:  30.717031478881836\n",
      "################################  7  ################################\n",
      "Loss:  27.3055419921875\n",
      "################################  8  ################################\n",
      "Loss:  24.13738250732422\n",
      "################################  9  ################################\n",
      "Loss:  21.117130279541016\n",
      "################################  10  ################################\n",
      "Loss:  18.25105094909668\n",
      "################################  11  ################################\n",
      "Loss:  15.609806060791016\n",
      "################################  12  ################################\n",
      "Loss:  13.276817321777344\n",
      "################################  13  ################################\n",
      "Loss:  11.28818130493164\n",
      "################################  14  ################################\n",
      "Loss:  9.617715835571289\n",
      "################################  15  ################################\n",
      "Loss:  8.210168838500977\n",
      "################################  16  ################################\n",
      "Loss:  7.01313591003418\n",
      "################################  17  ################################\n",
      "Loss:  5.98707389831543\n",
      "################################  18  ################################\n",
      "Loss:  5.103943347930908\n",
      "################################  19  ################################\n",
      "Loss:  4.3437395095825195\n",
      "################################  20  ################################\n",
      "Loss:  3.691842794418335\n",
      "################################  21  ################################\n",
      "Loss:  3.1372649669647217\n",
      "################################  22  ################################\n",
      "Loss:  2.6711061000823975\n",
      "################################  23  ################################\n",
      "Loss:  2.2848806381225586\n",
      "################################  24  ################################\n",
      "Loss:  1.968973994255066\n",
      "################################  25  ################################\n",
      "Loss:  1.7121487855911255\n",
      "################################  26  ################################\n",
      "Loss:  1.5024640560150146\n",
      "################################  27  ################################\n",
      "Loss:  1.3288657665252686\n",
      "################################  28  ################################\n",
      "Loss:  1.1823738813400269\n",
      "################################  29  ################################\n",
      "Loss:  1.0563414096832275\n",
      "################################  30  ################################\n",
      "Loss:  0.9462361931800842\n",
      "################################  31  ################################\n",
      "Loss:  0.8476030826568604\n",
      "################################  32  ################################\n",
      "Loss:  0.7447311282157898\n",
      "################################  33  ################################\n",
      "Loss:  0.6643117070198059\n",
      "################################  34  ################################\n",
      "Loss:  0.6039459705352783\n",
      "################################  35  ################################\n",
      "Loss:  0.5459174513816833\n",
      "################################  36  ################################\n",
      "Loss:  0.49441638588905334\n",
      "################################  37  ################################\n",
      "Loss:  0.4536610543727875\n",
      "################################  38  ################################\n",
      "Loss:  0.4139377772808075\n",
      "################################  39  ################################\n",
      "Loss:  0.3771226406097412\n",
      "################################  40  ################################\n",
      "Loss:  0.34405598044395447\n",
      "################################  41  ################################\n",
      "Loss:  0.3180517256259918\n",
      "################################  42  ################################\n",
      "Loss:  0.29632362723350525\n",
      "################################  43  ################################\n",
      "Loss:  0.2736814618110657\n",
      "################################  44  ################################\n",
      "Loss:  0.2538240849971771\n",
      "################################  45  ################################\n",
      "Loss:  0.2379361391067505\n",
      "################################  46  ################################\n",
      "Loss:  0.21995772421360016\n",
      "################################  47  ################################\n",
      "Loss:  0.20763146877288818\n",
      "################################  48  ################################\n",
      "Loss:  0.19485414028167725\n",
      "################################  49  ################################\n",
      "Loss:  0.17975559830665588\n",
      "################################  50  ################################\n",
      "Loss:  0.16382962465286255\n",
      "################################  51  ################################\n",
      "Loss:  0.15093155205249786\n",
      "################################  52  ################################\n",
      "Loss:  0.1379086822271347\n",
      "################################  53  ################################\n",
      "Loss:  0.12642312049865723\n",
      "################################  54  ################################\n",
      "Loss:  0.1155119240283966\n",
      "################################  55  ################################\n",
      "Loss:  0.10649936646223068\n",
      "################################  56  ################################\n",
      "Loss:  0.09841904044151306\n",
      "################################  57  ################################\n",
      "Loss:  0.09093914926052094\n",
      "################################  58  ################################\n",
      "Loss:  0.08438030630350113\n",
      "################################  59  ################################\n",
      "Loss:  0.07795941829681396\n",
      "################################  60  ################################\n",
      "Loss:  0.07232442498207092\n",
      "################################  61  ################################\n",
      "Loss:  0.06696230918169022\n",
      "################################  62  ################################\n",
      "Loss:  0.061924584209918976\n",
      "################################  63  ################################\n",
      "Loss:  0.057671885937452316\n",
      "################################  64  ################################\n",
      "Loss:  0.05347788333892822\n",
      "################################  65  ################################\n",
      "Loss:  0.050023362040519714\n",
      "################################  66  ################################\n",
      "Loss:  0.046369925141334534\n",
      "################################  67  ################################\n",
      "Loss:  0.04332408308982849\n",
      "################################  68  ################################\n",
      "Loss:  0.040024612098932266\n",
      "################################  69  ################################\n",
      "Loss:  0.037211183458566666\n",
      "################################  70  ################################\n",
      "Loss:  0.03453141450881958\n",
      "################################  71  ################################\n",
      "Loss:  0.03173906356096268\n",
      "################################  72  ################################\n",
      "Loss:  0.029594220221042633\n",
      "################################  73  ################################\n",
      "Loss:  0.02730444259941578\n",
      "################################  74  ################################\n",
      "Loss:  0.02552986890077591\n",
      "################################  75  ################################\n",
      "Loss:  0.023536743596196175\n",
      "################################  76  ################################\n",
      "Loss:  0.022061487659811974\n",
      "################################  77  ################################\n",
      "Loss:  0.0203461404889822\n",
      "################################  78  ################################\n",
      "Loss:  0.019145607948303223\n",
      "################################  79  ################################\n",
      "Loss:  0.017946485430002213\n",
      "################################  80  ################################\n",
      "Loss:  0.016874736174941063\n",
      "################################  81  ################################\n",
      "Loss:  0.015944506973028183\n",
      "################################  82  ################################\n",
      "Loss:  0.015096556395292282\n",
      "################################  83  ################################\n",
      "Loss:  0.014355983585119247\n",
      "################################  84  ################################\n",
      "Loss:  0.013681176118552685\n",
      "################################  85  ################################\n",
      "Loss:  0.013082482852041721\n",
      "################################  86  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.012526980601251125\n",
      "################################  87  ################################\n",
      "Loss:  0.012023823335766792\n",
      "################################  88  ################################\n",
      "Loss:  0.01155991479754448\n",
      "################################  89  ################################\n",
      "Loss:  0.011148020625114441\n",
      "################################  90  ################################\n",
      "Loss:  0.010778160765767097\n",
      "################################  91  ################################\n",
      "Loss:  0.010422075167298317\n",
      "################################  92  ################################\n",
      "Loss:  0.010097319260239601\n",
      "################################  93  ################################\n",
      "Loss:  0.009779797866940498\n",
      "################################  94  ################################\n",
      "Loss:  0.009492699056863785\n",
      "################################  95  ################################\n",
      "Loss:  0.009199265390634537\n",
      "################################  96  ################################\n",
      "Loss:  0.008931094780564308\n",
      "################################  97  ################################\n",
      "Loss:  0.008619631640613079\n",
      "################################  98  ################################\n",
      "Loss:  0.008313320577144623\n",
      "################################  99  ################################\n",
      "Loss:  0.008034873753786087\n",
      "################################  100  ################################\n",
      "Loss:  0.0077231889590620995\n",
      "################################  101  ################################\n",
      "Loss:  0.007472928613424301\n",
      "################################  102  ################################\n",
      "Loss:  0.007200272753834724\n",
      "################################  103  ################################\n",
      "Loss:  0.006939610932022333\n",
      "################################  104  ################################\n",
      "Loss:  0.006689123343676329\n",
      "################################  105  ################################\n",
      "Loss:  0.006445793900638819\n",
      "################################  106  ################################\n",
      "Loss:  0.006227459758520126\n",
      "################################  107  ################################\n",
      "Loss:  0.00601627491414547\n",
      "################################  108  ################################\n",
      "Loss:  0.00581866130232811\n",
      "################################  109  ################################\n",
      "Loss:  0.005627608858048916\n",
      "################################  110  ################################\n",
      "Loss:  0.005435701459646225\n",
      "################################  111  ################################\n",
      "Loss:  0.0052565294317901134\n",
      "################################  112  ################################\n",
      "Loss:  0.005086889490485191\n",
      "################################  113  ################################\n",
      "Loss:  0.004932825453579426\n",
      "################################  114  ################################\n",
      "Loss:  0.004787370562553406\n",
      "################################  115  ################################\n",
      "Loss:  0.004654844291508198\n",
      "################################  116  ################################\n",
      "Loss:  0.004530678037554026\n",
      "################################  117  ################################\n",
      "Loss:  0.004415309056639671\n",
      "################################  118  ################################\n",
      "Loss:  0.004307258874177933\n",
      "################################  119  ################################\n",
      "Loss:  0.004205734468996525\n",
      "################################  120  ################################\n",
      "Loss:  0.00411131139844656\n",
      "################################  121  ################################\n",
      "Loss:  0.004024399444460869\n",
      "################################  122  ################################\n",
      "Loss:  0.003944641910493374\n",
      "################################  123  ################################\n",
      "Loss:  0.0038697568234056234\n",
      "################################  124  ################################\n",
      "Loss:  0.003799580968916416\n",
      "################################  125  ################################\n",
      "Loss:  0.003732766956090927\n",
      "################################  126  ################################\n",
      "Loss:  0.0036702274810522795\n",
      "################################  127  ################################\n",
      "Loss:  0.003610472194850445\n",
      "################################  128  ################################\n",
      "Loss:  0.003554712515324354\n",
      "################################  129  ################################\n",
      "Loss:  0.0035010280553251505\n",
      "################################  130  ################################\n",
      "Loss:  0.0034494600258767605\n",
      "################################  131  ################################\n",
      "Loss:  0.0033988303039222956\n",
      "################################  132  ################################\n",
      "Loss:  0.003348894417285919\n",
      "################################  133  ################################\n",
      "Loss:  0.0032987501472234726\n",
      "################################  134  ################################\n",
      "Loss:  0.0032493167091161013\n",
      "################################  135  ################################\n",
      "Loss:  0.0032014744356274605\n",
      "################################  136  ################################\n",
      "Loss:  0.003153502009809017\n",
      "################################  137  ################################\n",
      "Loss:  0.003107671160250902\n",
      "################################  138  ################################\n",
      "Loss:  0.003059898968786001\n",
      "################################  139  ################################\n",
      "Loss:  0.003015131689608097\n",
      "################################  140  ################################\n",
      "Loss:  0.0029640551656484604\n",
      "################################  141  ################################\n",
      "Loss:  0.0029138086829334497\n",
      "################################  142  ################################\n",
      "Loss:  0.0028514168225228786\n",
      "################################  143  ################################\n",
      "Loss:  0.0028039636090397835\n",
      "################################  144  ################################\n",
      "Loss:  0.002751769497990608\n",
      "################################  145  ################################\n",
      "Loss:  0.0026933355256915092\n",
      "################################  146  ################################\n",
      "Loss:  0.0026440320070832968\n",
      "################################  147  ################################\n",
      "Loss:  0.0025946858804672956\n",
      "################################  148  ################################\n",
      "Loss:  0.0025446692015975714\n",
      "################################  149  ################################\n",
      "Loss:  0.0024971896782517433\n",
      "################################  150  ################################\n",
      "Loss:  0.0024496254045516253\n",
      "################################  151  ################################\n",
      "Loss:  0.002405422041192651\n",
      "################################  152  ################################\n",
      "Loss:  0.0023644566535949707\n",
      "################################  153  ################################\n",
      "Loss:  0.002324735978618264\n",
      "################################  154  ################################\n",
      "Loss:  0.002289756666868925\n",
      "################################  155  ################################\n",
      "Loss:  0.0022559878416359425\n",
      "################################  156  ################################\n",
      "Loss:  0.002225361531600356\n",
      "################################  157  ################################\n",
      "Loss:  0.0021948963403701782\n",
      "################################  158  ################################\n",
      "Loss:  0.0021659070625901222\n",
      "################################  159  ################################\n",
      "Loss:  0.002137508476153016\n",
      "################################  160  ################################\n",
      "Loss:  0.00211027218028903\n",
      "################################  161  ################################\n",
      "Loss:  0.0020844447426497936\n",
      "################################  162  ################################\n",
      "Loss:  0.0020592045038938522\n",
      "################################  163  ################################\n",
      "Loss:  0.0020359994377940893\n",
      "################################  164  ################################\n",
      "Loss:  0.0020126381423324347\n",
      "################################  165  ################################\n",
      "Loss:  0.001990706194192171\n",
      "################################  166  ################################\n",
      "Loss:  0.0019682892598211765\n",
      "################################  167  ################################\n",
      "Loss:  0.0019458914175629616\n",
      "################################  168  ################################\n",
      "Loss:  0.0019241950940340757\n",
      "################################  169  ################################\n",
      "Loss:  0.0019018288003280759\n",
      "################################  170  ################################\n",
      "Loss:  0.0018803775310516357\n",
      "################################  171  ################################\n",
      "Loss:  0.0018583430210128427\n",
      "################################  172  ################################\n",
      "Loss:  0.0018361546099185944\n",
      "################################  173  ################################\n",
      "Loss:  0.0018124895868822932\n",
      "################################  174  ################################\n",
      "Loss:  0.0017882456304505467\n",
      "################################  175  ################################\n",
      "Loss:  0.0017642215825617313\n",
      "################################  176  ################################\n",
      "Loss:  0.0017390395514667034\n",
      "################################  177  ################################\n",
      "Loss:  0.001713886158540845\n",
      "################################  178  ################################\n",
      "Loss:  0.0016876752488315105\n",
      "################################  179  ################################\n",
      "Loss:  0.0016618662048131227\n",
      "################################  180  ################################\n",
      "Loss:  0.0016365553019568324\n",
      "################################  181  ################################\n",
      "Loss:  0.0016117971390485764\n",
      "################################  182  ################################\n",
      "Loss:  0.0015883082523941994\n",
      "################################  183  ################################\n",
      "Loss:  0.0015657292678952217\n",
      "################################  184  ################################\n",
      "Loss:  0.0015429630875587463\n",
      "################################  185  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0015210913261398673\n",
      "################################  186  ################################\n",
      "Loss:  0.0014991187490522861\n",
      "################################  187  ################################\n",
      "Loss:  0.001478042220696807\n",
      "################################  188  ################################\n",
      "Loss:  0.001457087229937315\n",
      "################################  189  ################################\n",
      "Loss:  0.0014366069808602333\n",
      "################################  190  ################################\n",
      "Loss:  0.001416540239006281\n",
      "################################  191  ################################\n",
      "Loss:  0.001396950799971819\n",
      "################################  192  ################################\n",
      "Loss:  0.0013778661377727985\n",
      "################################  193  ################################\n",
      "Loss:  0.0013598467921838164\n",
      "################################  194  ################################\n",
      "Loss:  0.001342832576483488\n",
      "################################  195  ################################\n",
      "Loss:  0.0013269813498482108\n",
      "################################  196  ################################\n",
      "Loss:  0.0013122594682499766\n",
      "################################  197  ################################\n",
      "Loss:  0.001298211282119155\n",
      "################################  198  ################################\n",
      "Loss:  0.0012855238746851683\n",
      "################################  199  ################################\n",
      "Loss:  0.0012731372844427824\n",
      "################################  200  ################################\n",
      "Loss:  0.0012614906299859285\n",
      "################################  201  ################################\n",
      "Loss:  0.0012496539857238531\n",
      "################################  202  ################################\n",
      "Loss:  0.0012376224622130394\n",
      "################################  203  ################################\n",
      "Loss:  0.0012253354070708156\n",
      "################################  204  ################################\n",
      "Loss:  0.0012136385776102543\n",
      "################################  205  ################################\n",
      "Loss:  0.0012031712103635073\n",
      "################################  206  ################################\n",
      "Loss:  0.0011921387631446123\n",
      "################################  207  ################################\n",
      "Loss:  0.0011810355354100466\n",
      "################################  208  ################################\n",
      "Loss:  0.0011701815528795123\n",
      "################################  209  ################################\n",
      "Loss:  0.0011595746036618948\n",
      "################################  210  ################################\n",
      "Loss:  0.0011495922226458788\n",
      "################################  211  ################################\n",
      "Loss:  0.0011397581547498703\n",
      "################################  212  ################################\n",
      "Loss:  0.001130291842855513\n",
      "################################  213  ################################\n",
      "Loss:  0.001120861154049635\n",
      "################################  214  ################################\n",
      "Loss:  0.001111470628529787\n",
      "################################  215  ################################\n",
      "Loss:  0.001102100359275937\n",
      "################################  216  ################################\n",
      "Loss:  0.0010929054114967585\n",
      "################################  217  ################################\n",
      "Loss:  0.0010839803144335747\n",
      "################################  218  ################################\n",
      "Loss:  0.0010751686058938503\n",
      "################################  219  ################################\n",
      "Loss:  0.0010665474692359567\n",
      "################################  220  ################################\n",
      "Loss:  0.0010579186491668224\n",
      "################################  221  ################################\n",
      "Loss:  0.0010493174195289612\n",
      "################################  222  ################################\n",
      "Loss:  0.0010406766086816788\n",
      "################################  223  ################################\n",
      "Loss:  0.0010321454610675573\n",
      "################################  224  ################################\n",
      "Loss:  0.0010238076793029904\n",
      "################################  225  ################################\n",
      "Loss:  0.001015583984553814\n",
      "################################  226  ################################\n",
      "Loss:  0.001007492421194911\n",
      "################################  227  ################################\n",
      "Loss:  0.0009994731517508626\n",
      "################################  228  ################################\n",
      "Loss:  0.0009915640112012625\n",
      "################################  229  ################################\n",
      "Loss:  0.000983787584118545\n",
      "################################  230  ################################\n",
      "Loss:  0.0009762283880263567\n",
      "################################  231  ################################\n",
      "Loss:  0.0009689327562227845\n",
      "################################  232  ################################\n",
      "Loss:  0.0009619875345379114\n",
      "################################  233  ################################\n",
      "Loss:  0.000955411815084517\n",
      "################################  234  ################################\n",
      "Loss:  0.0009493422112427652\n",
      "################################  235  ################################\n",
      "Loss:  0.0009437288390472531\n",
      "################################  236  ################################\n",
      "Loss:  0.0009386443998664618\n",
      "################################  237  ################################\n",
      "Loss:  0.0009338647359982133\n",
      "################################  238  ################################\n",
      "Loss:  0.0009291203459724784\n",
      "################################  239  ################################\n",
      "Loss:  0.0009243378881365061\n",
      "################################  240  ################################\n",
      "Loss:  0.000919747690204531\n",
      "################################  241  ################################\n",
      "Loss:  0.0009154616273008287\n",
      "################################  242  ################################\n",
      "Loss:  0.0009112240513786674\n",
      "################################  243  ################################\n",
      "Loss:  0.0009073724504560232\n",
      "################################  244  ################################\n",
      "Loss:  0.000903359497897327\n",
      "################################  245  ################################\n",
      "Loss:  0.0008993250667117536\n",
      "################################  246  ################################\n",
      "Loss:  0.0008945661247707903\n",
      "################################  247  ################################\n",
      "Loss:  0.0008898727828636765\n",
      "################################  248  ################################\n",
      "Loss:  0.0008837917121127248\n",
      "################################  249  ################################\n",
      "Loss:  0.0008781045326031744\n",
      "################################  250  ################################\n",
      "Loss:  0.0008724014041945338\n",
      "################################  251  ################################\n",
      "Loss:  0.0008662933832965791\n",
      "################################  252  ################################\n",
      "Loss:  0.0008603439200669527\n",
      "################################  253  ################################\n",
      "Loss:  0.0008544910233467817\n",
      "################################  254  ################################\n",
      "Loss:  0.0008486742153763771\n",
      "################################  255  ################################\n",
      "Loss:  0.0008430737070739269\n",
      "################################  256  ################################\n",
      "Loss:  0.0008375048637390137\n",
      "################################  257  ################################\n",
      "Loss:  0.0008321466157212853\n",
      "################################  258  ################################\n",
      "Loss:  0.0008270711405202746\n",
      "################################  259  ################################\n",
      "Loss:  0.0008223154582083225\n",
      "################################  260  ################################\n",
      "Loss:  0.0008179660653695464\n",
      "################################  261  ################################\n",
      "Loss:  0.0008137457189150155\n",
      "################################  262  ################################\n",
      "Loss:  0.0008098478429019451\n",
      "################################  263  ################################\n",
      "Loss:  0.0008061141707003117\n",
      "################################  264  ################################\n",
      "Loss:  0.000802505062893033\n",
      "################################  265  ################################\n",
      "Loss:  0.0007991684251464903\n",
      "################################  266  ################################\n",
      "Loss:  0.0007959083886817098\n",
      "################################  267  ################################\n",
      "Loss:  0.000792972685303539\n",
      "################################  268  ################################\n",
      "Loss:  0.0007899953052401543\n",
      "################################  269  ################################\n",
      "Loss:  0.0007871845155023038\n",
      "################################  270  ################################\n",
      "Loss:  0.0007843930507078767\n",
      "################################  271  ################################\n",
      "Loss:  0.0007814959390088916\n",
      "################################  272  ################################\n",
      "Loss:  0.0007785739726386964\n",
      "################################  273  ################################\n",
      "Loss:  0.0007756035192869604\n",
      "################################  274  ################################\n",
      "Loss:  0.0007724988972768188\n",
      "################################  275  ################################\n",
      "Loss:  0.0007696341490373015\n",
      "################################  276  ################################\n",
      "Loss:  0.0007667390746064484\n",
      "################################  277  ################################\n",
      "Loss:  0.0007635982474312186\n",
      "################################  278  ################################\n",
      "Loss:  0.0007600077660754323\n",
      "################################  279  ################################\n",
      "Loss:  0.0007563574472442269\n",
      "################################  280  ################################\n",
      "Loss:  0.0007522673113271594\n",
      "################################  281  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0007482603541575372\n",
      "################################  282  ################################\n",
      "Loss:  0.0007438117172569036\n",
      "################################  283  ################################\n",
      "Loss:  0.0007381921750493348\n",
      "################################  284  ################################\n",
      "Loss:  0.0007339970325119793\n",
      "################################  285  ################################\n",
      "Loss:  0.0007293048547580838\n",
      "################################  286  ################################\n",
      "Loss:  0.0007240347331389785\n",
      "################################  287  ################################\n",
      "Loss:  0.0007188530289568007\n",
      "################################  288  ################################\n",
      "Loss:  0.0007133287144824862\n",
      "################################  289  ################################\n",
      "Loss:  0.000708159408532083\n",
      "################################  290  ################################\n",
      "Loss:  0.000701945333275944\n",
      "################################  291  ################################\n",
      "Loss:  0.0006972883711569011\n",
      "################################  292  ################################\n",
      "Loss:  0.0006923725595697761\n",
      "################################  293  ################################\n",
      "Loss:  0.000687169493176043\n",
      "################################  294  ################################\n",
      "Loss:  0.0006816554814577103\n",
      "################################  295  ################################\n",
      "Loss:  0.000676793628372252\n",
      "################################  296  ################################\n",
      "Loss:  0.0006719022057950497\n",
      "################################  297  ################################\n",
      "Loss:  0.0006671814480796456\n",
      "################################  298  ################################\n",
      "Loss:  0.0006623957888223231\n",
      "################################  299  ################################\n",
      "Loss:  0.0006587192765437067\n",
      "################################  300  ################################\n",
      "Loss:  0.0006552594131790102\n",
      "################################  301  ################################\n",
      "Loss:  0.0006515774293802679\n",
      "################################  302  ################################\n",
      "Loss:  0.0006481556338258088\n",
      "################################  303  ################################\n",
      "Loss:  0.0006448888452723622\n",
      "################################  304  ################################\n",
      "Loss:  0.000641555932816118\n",
      "################################  305  ################################\n",
      "Loss:  0.0006383899599313736\n",
      "################################  306  ################################\n",
      "Loss:  0.0006355042569339275\n",
      "################################  307  ################################\n",
      "Loss:  0.0006329223979264498\n",
      "################################  308  ################################\n",
      "Loss:  0.0006304402486421168\n",
      "################################  309  ################################\n",
      "Loss:  0.0006280199158936739\n",
      "################################  310  ################################\n",
      "Loss:  0.0006254729814827442\n",
      "################################  311  ################################\n",
      "Loss:  0.0006229255232028663\n",
      "################################  312  ################################\n",
      "Loss:  0.0006203301600180566\n",
      "################################  313  ################################\n",
      "Loss:  0.0006177177419885993\n",
      "################################  314  ################################\n",
      "Loss:  0.000615196768194437\n",
      "################################  315  ################################\n",
      "Loss:  0.0006126163061708212\n",
      "################################  316  ################################\n",
      "Loss:  0.0006100265891291201\n",
      "################################  317  ################################\n",
      "Loss:  0.0006076151039451361\n",
      "################################  318  ################################\n",
      "Loss:  0.0006052052485756576\n",
      "################################  319  ################################\n",
      "Loss:  0.0006028914358466864\n",
      "################################  320  ################################\n",
      "Loss:  0.0006005594041198492\n",
      "################################  321  ################################\n",
      "Loss:  0.0005982853472232819\n",
      "################################  322  ################################\n",
      "Loss:  0.0005959453992545605\n",
      "################################  323  ################################\n",
      "Loss:  0.0005936055094935\n",
      "################################  324  ################################\n",
      "Loss:  0.00059111462906003\n",
      "################################  325  ################################\n",
      "Loss:  0.0005884884158149362\n",
      "################################  326  ################################\n",
      "Loss:  0.0005857686628587544\n",
      "################################  327  ################################\n",
      "Loss:  0.0005828897119499743\n",
      "################################  328  ################################\n",
      "Loss:  0.0005799779901280999\n",
      "################################  329  ################################\n",
      "Loss:  0.0005770852440036833\n",
      "################################  330  ################################\n",
      "Loss:  0.0005742814391851425\n",
      "################################  331  ################################\n",
      "Loss:  0.0005714347353205085\n",
      "################################  332  ################################\n",
      "Loss:  0.0005686000222340226\n",
      "################################  333  ################################\n",
      "Loss:  0.0005658779409714043\n",
      "################################  334  ################################\n",
      "Loss:  0.0005631078965961933\n",
      "################################  335  ################################\n",
      "Loss:  0.0005605466431006789\n",
      "################################  336  ################################\n",
      "Loss:  0.0005580817232839763\n",
      "################################  337  ################################\n",
      "Loss:  0.0005556385149247944\n",
      "################################  338  ################################\n",
      "Loss:  0.0005531258066184819\n",
      "################################  339  ################################\n",
      "Loss:  0.0005507648456841707\n",
      "################################  340  ################################\n",
      "Loss:  0.0005484669236466289\n",
      "################################  341  ################################\n",
      "Loss:  0.0005462893168441951\n",
      "################################  342  ################################\n",
      "Loss:  0.0005441877292469144\n",
      "################################  343  ################################\n",
      "Loss:  0.0005422442336566746\n",
      "################################  344  ################################\n",
      "Loss:  0.000540368549991399\n",
      "################################  345  ################################\n",
      "Loss:  0.0005385839031077921\n",
      "################################  346  ################################\n",
      "Loss:  0.0005369731225073338\n",
      "################################  347  ################################\n",
      "Loss:  0.0005353770684450865\n",
      "################################  348  ################################\n",
      "Loss:  0.0005339161725714803\n",
      "################################  349  ################################\n",
      "Loss:  0.0005324088269844651\n",
      "################################  350  ################################\n",
      "Loss:  0.0005310437409207225\n",
      "################################  351  ################################\n",
      "Loss:  0.0005295887822285295\n",
      "################################  352  ################################\n",
      "Loss:  0.0005279797478578985\n",
      "################################  353  ################################\n",
      "Loss:  0.0005262054619379342\n",
      "################################  354  ################################\n",
      "Loss:  0.0005247269291430712\n",
      "################################  355  ################################\n",
      "Loss:  0.0005231426330283284\n",
      "################################  356  ################################\n",
      "Loss:  0.0005213647382333875\n",
      "################################  357  ################################\n",
      "Loss:  0.0005194281693547964\n",
      "################################  358  ################################\n",
      "Loss:  0.0005176914273761213\n",
      "################################  359  ################################\n",
      "Loss:  0.0005159596330486238\n",
      "################################  360  ################################\n",
      "Loss:  0.0005141503643244505\n",
      "################################  361  ################################\n",
      "Loss:  0.0005125144962221384\n",
      "################################  362  ################################\n",
      "Loss:  0.0005108811892569065\n",
      "################################  363  ################################\n",
      "Loss:  0.0005092223873361945\n",
      "################################  364  ################################\n",
      "Loss:  0.0005074559594504535\n",
      "################################  365  ################################\n",
      "Loss:  0.0005056302761659026\n",
      "################################  366  ################################\n",
      "Loss:  0.0005038853851146996\n",
      "################################  367  ################################\n",
      "Loss:  0.0005021782126277685\n",
      "################################  368  ################################\n",
      "Loss:  0.0005005899583920836\n",
      "################################  369  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0004989744629710913\n",
      "################################  370  ################################\n",
      "Loss:  0.0004973760806024075\n",
      "################################  371  ################################\n",
      "Loss:  0.0004957836936227977\n",
      "################################  372  ################################\n",
      "Loss:  0.0004941857187077403\n",
      "################################  373  ################################\n",
      "Loss:  0.0004925838438794017\n",
      "################################  374  ################################\n",
      "Loss:  0.0004909379640594125\n",
      "################################  375  ################################\n",
      "Loss:  0.0004892498254776001\n",
      "################################  376  ################################\n",
      "Loss:  0.0004875858430750668\n",
      "################################  377  ################################\n",
      "Loss:  0.00048595425323583186\n",
      "################################  378  ################################\n",
      "Loss:  0.00048430723836645484\n",
      "################################  379  ################################\n",
      "Loss:  0.00048269645776599646\n",
      "################################  380  ################################\n",
      "Loss:  0.00048107869224622846\n",
      "################################  381  ################################\n",
      "Loss:  0.00047952745808288455\n",
      "################################  382  ################################\n",
      "Loss:  0.00047797508887015283\n",
      "################################  383  ################################\n",
      "Loss:  0.00047646291204728186\n",
      "################################  384  ################################\n",
      "Loss:  0.0004748943611048162\n",
      "################################  385  ################################\n",
      "Loss:  0.0004733368114102632\n",
      "################################  386  ################################\n",
      "Loss:  0.0004717733827419579\n",
      "################################  387  ################################\n",
      "Loss:  0.00047002441715449095\n",
      "################################  388  ################################\n",
      "Loss:  0.00046846800250932574\n",
      "################################  389  ################################\n",
      "Loss:  0.0004668996552936733\n",
      "################################  390  ################################\n",
      "Loss:  0.0004654073854908347\n",
      "################################  391  ################################\n",
      "Loss:  0.00046394849778153\n",
      "################################  392  ################################\n",
      "Loss:  0.0004623987479135394\n",
      "################################  393  ################################\n",
      "Loss:  0.00046095161815173924\n",
      "################################  394  ################################\n",
      "Loss:  0.0004594777128659189\n",
      "################################  395  ################################\n",
      "Loss:  0.0004579964152071625\n",
      "################################  396  ################################\n",
      "Loss:  0.0004565169510897249\n",
      "################################  397  ################################\n",
      "Loss:  0.00045495963422581553\n",
      "################################  398  ################################\n",
      "Loss:  0.0004534002800937742\n",
      "################################  399  ################################\n",
      "Loss:  0.00045176982530392706\n",
      "################################  400  ################################\n",
      "Loss:  0.00045047595631331205\n",
      "################################  401  ################################\n",
      "Loss:  0.0004492492589633912\n",
      "################################  402  ################################\n",
      "Loss:  0.00044788618106395006\n",
      "################################  403  ################################\n",
      "Loss:  0.00044658005936071277\n",
      "################################  404  ################################\n",
      "Loss:  0.00044526715646497905\n",
      "################################  405  ################################\n",
      "Loss:  0.0004439922922756523\n",
      "################################  406  ################################\n",
      "Loss:  0.000442775635747239\n",
      "################################  407  ################################\n",
      "Loss:  0.00044161779806017876\n",
      "################################  408  ################################\n",
      "Loss:  0.00044030376011505723\n",
      "################################  409  ################################\n",
      "Loss:  0.0004392677219584584\n",
      "################################  410  ################################\n",
      "Loss:  0.0004381908511277288\n",
      "################################  411  ################################\n",
      "Loss:  0.0004369557136669755\n",
      "################################  412  ################################\n",
      "Loss:  0.00043592462316155434\n",
      "################################  413  ################################\n",
      "Loss:  0.0004348975489847362\n",
      "################################  414  ################################\n",
      "Loss:  0.00043386738980188966\n",
      "################################  415  ################################\n",
      "Loss:  0.0004328281502239406\n",
      "################################  416  ################################\n",
      "Loss:  0.00043177028419449925\n",
      "################################  417  ################################\n",
      "Loss:  0.00043079169699922204\n",
      "################################  418  ################################\n",
      "Loss:  0.00042977603152394295\n",
      "################################  419  ################################\n",
      "Loss:  0.0004288486670702696\n",
      "################################  420  ################################\n",
      "Loss:  0.00042784324614331126\n",
      "################################  421  ################################\n",
      "Loss:  0.0004267444310244173\n",
      "################################  422  ################################\n",
      "Loss:  0.00042553554521873593\n",
      "################################  423  ################################\n",
      "Loss:  0.00042435157229192555\n",
      "################################  424  ################################\n",
      "Loss:  0.00042307988042011857\n",
      "################################  425  ################################\n",
      "Loss:  0.0004217353416606784\n",
      "################################  426  ################################\n",
      "Loss:  0.00042043719440698624\n",
      "################################  427  ################################\n",
      "Loss:  0.0004191506886854768\n",
      "################################  428  ################################\n",
      "Loss:  0.0004179245443083346\n",
      "################################  429  ################################\n",
      "Loss:  0.00041672177030704916\n",
      "################################  430  ################################\n",
      "Loss:  0.00041553194751031697\n",
      "################################  431  ################################\n",
      "Loss:  0.0004143176774960011\n",
      "################################  432  ################################\n",
      "Loss:  0.000413089815992862\n",
      "################################  433  ################################\n",
      "Loss:  0.0004118028446100652\n",
      "################################  434  ################################\n",
      "Loss:  0.0004105857515241951\n",
      "################################  435  ################################\n",
      "Loss:  0.00040940725011751056\n",
      "################################  436  ################################\n",
      "Loss:  0.00040820776484906673\n",
      "################################  437  ################################\n",
      "Loss:  0.00040695982170291245\n",
      "################################  438  ################################\n",
      "Loss:  0.0004058355698361993\n",
      "################################  439  ################################\n",
      "Loss:  0.00040467962389811873\n",
      "################################  440  ################################\n",
      "Loss:  0.00040340909617953\n",
      "################################  441  ################################\n",
      "Loss:  0.0004021774511784315\n",
      "################################  442  ################################\n",
      "Loss:  0.00040091463597491384\n",
      "################################  443  ################################\n",
      "Loss:  0.00039973994717001915\n",
      "################################  444  ################################\n",
      "Loss:  0.00039837032090872526\n",
      "################################  445  ################################\n",
      "Loss:  0.0003966519143432379\n",
      "################################  446  ################################\n",
      "Loss:  0.0003949849051423371\n",
      "################################  447  ################################\n",
      "Loss:  0.00039339327486231923\n",
      "################################  448  ################################\n",
      "Loss:  0.0003918713773600757\n",
      "################################  449  ################################\n",
      "Loss:  0.0003902694152202457\n",
      "################################  450  ################################\n",
      "Loss:  0.0003886802587658167\n",
      "################################  451  ################################\n",
      "Loss:  0.0003871409862767905\n",
      "################################  452  ################################\n",
      "Loss:  0.0003854984533973038\n",
      "################################  453  ################################\n",
      "Loss:  0.0003840089775621891\n",
      "################################  454  ################################\n",
      "Loss:  0.00038254959508776665\n",
      "################################  455  ################################\n",
      "Loss:  0.0003810198395512998\n",
      "################################  456  ################################\n",
      "Loss:  0.0003795295488089323\n",
      "################################  457  ################################\n",
      "Loss:  0.0003781970590353012\n",
      "################################  458  ################################\n",
      "Loss:  0.00037694466300308704\n",
      "################################  459  ################################\n",
      "Loss:  0.0003756341175176203\n",
      "################################  460  ################################\n",
      "Loss:  0.00037431743112392724\n",
      "################################  461  ################################\n",
      "Loss:  0.0003731340984813869\n",
      "################################  462  ################################\n",
      "Loss:  0.0003718430525623262\n",
      "################################  463  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.00037076009903103113\n",
      "################################  464  ################################\n",
      "Loss:  0.0003690790617838502\n",
      "################################  465  ################################\n",
      "Loss:  0.00036788033321499825\n",
      "################################  466  ################################\n",
      "Loss:  0.00036609056405723095\n",
      "################################  467  ################################\n",
      "Loss:  0.00036440411349758506\n",
      "################################  468  ################################\n",
      "Loss:  0.00036254897713661194\n",
      "################################  469  ################################\n",
      "Loss:  0.00036053964868187904\n",
      "################################  470  ################################\n",
      "Loss:  0.000358371326001361\n",
      "################################  471  ################################\n",
      "Loss:  0.00035627506440505385\n",
      "################################  472  ################################\n",
      "Loss:  0.0003541775222402066\n",
      "################################  473  ################################\n",
      "Loss:  0.00035231877700425684\n",
      "################################  474  ################################\n",
      "Loss:  0.00035067962016910315\n",
      "################################  475  ################################\n",
      "Loss:  0.00034910463728010654\n",
      "################################  476  ################################\n",
      "Loss:  0.00034767494071274996\n",
      "################################  477  ################################\n",
      "Loss:  0.0003463435859885067\n",
      "################################  478  ################################\n",
      "Loss:  0.0003450855438131839\n",
      "################################  479  ################################\n",
      "Loss:  0.0003438463609199971\n",
      "################################  480  ################################\n",
      "Loss:  0.00034271826734766364\n",
      "################################  481  ################################\n",
      "Loss:  0.00034180236980319023\n",
      "################################  482  ################################\n",
      "Loss:  0.00034094712464138865\n",
      "################################  483  ################################\n",
      "Loss:  0.0003401119029149413\n",
      "################################  484  ################################\n",
      "Loss:  0.00033935115789063275\n",
      "################################  485  ################################\n",
      "Loss:  0.0003384922456461936\n",
      "################################  486  ################################\n",
      "Loss:  0.0003375296073500067\n",
      "################################  487  ################################\n",
      "Loss:  0.0003363251453265548\n",
      "################################  488  ################################\n",
      "Loss:  0.00033498197444714606\n",
      "################################  489  ################################\n",
      "Loss:  0.0003333495114929974\n",
      "################################  490  ################################\n",
      "Loss:  0.00033169472590088844\n",
      "################################  491  ################################\n",
      "Loss:  0.0003299745440017432\n",
      "################################  492  ################################\n",
      "Loss:  0.0003279633237980306\n",
      "################################  493  ################################\n",
      "Loss:  0.000326279376167804\n",
      "################################  494  ################################\n",
      "Loss:  0.00032448465935885906\n",
      "################################  495  ################################\n",
      "Loss:  0.00032253796234726906\n",
      "################################  496  ################################\n",
      "Loss:  0.0003205427201464772\n",
      "################################  497  ################################\n",
      "Loss:  0.00031855126144364476\n",
      "################################  498  ################################\n",
      "Loss:  0.00031659918022342026\n",
      "################################  499  ################################\n",
      "Loss:  0.0003146700037177652\n",
      "################################  500  ################################\n",
      "Loss:  0.0003126413212157786\n",
      "################################  501  ################################\n",
      "Loss:  0.00031093112193048\n",
      "################################  502  ################################\n",
      "Loss:  0.000309537339489907\n",
      "################################  503  ################################\n",
      "Loss:  0.00030807036091573536\n",
      "################################  504  ################################\n",
      "Loss:  0.0003069278609473258\n",
      "################################  505  ################################\n",
      "Loss:  0.00030585372587665915\n",
      "################################  506  ################################\n",
      "Loss:  0.0003046708879992366\n",
      "################################  507  ################################\n",
      "Loss:  0.0003036642447113991\n",
      "################################  508  ################################\n",
      "Loss:  0.0003026962513104081\n",
      "################################  509  ################################\n",
      "Loss:  0.0003017012495547533\n",
      "################################  510  ################################\n",
      "Loss:  0.00030079486896283925\n",
      "################################  511  ################################\n",
      "Loss:  0.0002998893614858389\n",
      "################################  512  ################################\n",
      "Loss:  0.000299020204693079\n",
      "################################  513  ################################\n",
      "Loss:  0.0002982309670187533\n",
      "################################  514  ################################\n",
      "Loss:  0.00029746571090072393\n",
      "################################  515  ################################\n",
      "Loss:  0.00029671029187738895\n",
      "################################  516  ################################\n",
      "Loss:  0.000295986799756065\n",
      "################################  517  ################################\n",
      "Loss:  0.00029530684696510434\n",
      "################################  518  ################################\n",
      "Loss:  0.0002946372842416167\n",
      "################################  519  ################################\n",
      "Loss:  0.00029396655736491084\n",
      "################################  520  ################################\n",
      "Loss:  0.0002932357310783118\n",
      "################################  521  ################################\n",
      "Loss:  0.00029249454382807016\n",
      "################################  522  ################################\n",
      "Loss:  0.00029173772782087326\n",
      "################################  523  ################################\n",
      "Loss:  0.00029093812918290496\n",
      "################################  524  ################################\n",
      "Loss:  0.0002901392290368676\n",
      "################################  525  ################################\n",
      "Loss:  0.0002892467309720814\n",
      "################################  526  ################################\n",
      "Loss:  0.00028831203235313296\n",
      "################################  527  ################################\n",
      "Loss:  0.00028738362016156316\n",
      "################################  528  ################################\n",
      "Loss:  0.00028642971301451325\n",
      "################################  529  ################################\n",
      "Loss:  0.0002854714111890644\n",
      "################################  530  ################################\n",
      "Loss:  0.00028450277750380337\n",
      "################################  531  ################################\n",
      "Loss:  0.000283481931546703\n",
      "################################  532  ################################\n",
      "Loss:  0.00028255191864445806\n",
      "################################  533  ################################\n",
      "Loss:  0.0002816629712469876\n",
      "################################  534  ################################\n",
      "Loss:  0.0002807955606840551\n",
      "################################  535  ################################\n",
      "Loss:  0.0002799693320412189\n",
      "################################  536  ################################\n",
      "Loss:  0.00027917648549191654\n",
      "################################  537  ################################\n",
      "Loss:  0.000278401596006006\n",
      "################################  538  ################################\n",
      "Loss:  0.00027766183484345675\n",
      "################################  539  ################################\n",
      "Loss:  0.00027694617165252566\n",
      "################################  540  ################################\n",
      "Loss:  0.0002763096708804369\n",
      "################################  541  ################################\n",
      "Loss:  0.00027568559744395316\n",
      "################################  542  ################################\n",
      "Loss:  0.0002750800340436399\n",
      "################################  543  ################################\n",
      "Loss:  0.000274453719612211\n",
      "################################  544  ################################\n",
      "Loss:  0.00027378511731512845\n",
      "################################  545  ################################\n",
      "Loss:  0.0002731395943555981\n",
      "################################  546  ################################\n",
      "Loss:  0.00027246904210187495\n",
      "################################  547  ################################\n",
      "Loss:  0.00027181627228856087\n",
      "################################  548  ################################\n",
      "Loss:  0.000271165365120396\n",
      "################################  549  ################################\n",
      "Loss:  0.00027049038908444345\n",
      "################################  550  ################################\n",
      "Loss:  0.0002698131138458848\n",
      "################################  551  ################################\n",
      "Loss:  0.00026910999440588057\n",
      "################################  552  ################################\n",
      "Loss:  0.0002684015780687332\n",
      "################################  553  ################################\n",
      "Loss:  0.0002676662988960743\n",
      "################################  554  ################################\n",
      "Loss:  0.00026692566461861134\n",
      "################################  555  ################################\n",
      "Loss:  0.0002661776961758733\n",
      "################################  556  ################################\n",
      "Loss:  0.00026540830731391907\n",
      "################################  557  ################################\n",
      "Loss:  0.0002646148204803467\n",
      "################################  558  ################################\n",
      "Loss:  0.00026381516363471746\n",
      "################################  559  ################################\n",
      "Loss:  0.00026303998311050236\n",
      "################################  560  ################################\n",
      "Loss:  0.0002622040919959545\n",
      "################################  561  ################################\n",
      "Loss:  0.0002614494878798723\n",
      "################################  562  ################################\n",
      "Loss:  0.0002606399648357183\n",
      "################################  563  ################################\n",
      "Loss:  0.00025985733373090625\n",
      "################################  564  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.00025899027241393924\n",
      "################################  565  ################################\n",
      "Loss:  0.0002581869484856725\n",
      "################################  566  ################################\n",
      "Loss:  0.0002573218080215156\n",
      "################################  567  ################################\n",
      "Loss:  0.0002563440939411521\n",
      "################################  568  ################################\n",
      "Loss:  0.000255468359682709\n",
      "################################  569  ################################\n",
      "Loss:  0.00025461704353801906\n",
      "################################  570  ################################\n",
      "Loss:  0.0002537851978559047\n",
      "################################  571  ################################\n",
      "Loss:  0.000252970727160573\n",
      "################################  572  ################################\n",
      "Loss:  0.00025217427173629403\n",
      "################################  573  ################################\n",
      "Loss:  0.00025138951605185866\n",
      "################################  574  ################################\n",
      "Loss:  0.00025062222266569734\n",
      "################################  575  ################################\n",
      "Loss:  0.00024987885262817144\n",
      "################################  576  ################################\n",
      "Loss:  0.0002491497725713998\n",
      "################################  577  ################################\n",
      "Loss:  0.0002484332653693855\n",
      "################################  578  ################################\n",
      "Loss:  0.00024776856298558414\n",
      "################################  579  ################################\n",
      "Loss:  0.0002470986801199615\n",
      "################################  580  ################################\n",
      "Loss:  0.0002464592980686575\n",
      "################################  581  ################################\n",
      "Loss:  0.000245804141741246\n",
      "################################  582  ################################\n",
      "Loss:  0.0002450922620482743\n",
      "################################  583  ################################\n",
      "Loss:  0.0002443882403895259\n",
      "################################  584  ################################\n",
      "Loss:  0.00024363116244785488\n",
      "################################  585  ################################\n",
      "Loss:  0.00024293226306326687\n",
      "################################  586  ################################\n",
      "Loss:  0.00024220376508310437\n",
      "################################  587  ################################\n",
      "Loss:  0.0002414833870716393\n",
      "################################  588  ################################\n",
      "Loss:  0.00024074464454315603\n",
      "################################  589  ################################\n",
      "Loss:  0.000239831511862576\n",
      "################################  590  ################################\n",
      "Loss:  0.00023907935246825218\n",
      "################################  591  ################################\n",
      "Loss:  0.00023834468447603285\n",
      "################################  592  ################################\n",
      "Loss:  0.00023755560687277466\n",
      "################################  593  ################################\n",
      "Loss:  0.00023684362531639636\n",
      "################################  594  ################################\n",
      "Loss:  0.00023613186203874648\n",
      "################################  595  ################################\n",
      "Loss:  0.0002353855816181749\n",
      "################################  596  ################################\n",
      "Loss:  0.0002346841647522524\n",
      "################################  597  ################################\n",
      "Loss:  0.00023406511172652245\n",
      "################################  598  ################################\n",
      "Loss:  0.0002334985474590212\n",
      "################################  599  ################################\n",
      "Loss:  0.00023294537095353007\n",
      "################################  600  ################################\n",
      "Loss:  0.0002324288070667535\n",
      "################################  601  ################################\n",
      "Loss:  0.0002319144259672612\n",
      "################################  602  ################################\n",
      "Loss:  0.0002314326266059652\n",
      "################################  603  ################################\n",
      "Loss:  0.00023099087411537766\n",
      "################################  604  ################################\n",
      "Loss:  0.0002305138623341918\n",
      "################################  605  ################################\n",
      "Loss:  0.0002301348722539842\n",
      "################################  606  ################################\n",
      "Loss:  0.0002297758182976395\n",
      "################################  607  ################################\n",
      "Loss:  0.00022943763178773224\n",
      "################################  608  ################################\n",
      "Loss:  0.00022911434643901885\n",
      "################################  609  ################################\n",
      "Loss:  0.00022880316828377545\n",
      "################################  610  ################################\n",
      "Loss:  0.00022848433582112193\n",
      "################################  611  ################################\n",
      "Loss:  0.00022819737205281854\n",
      "################################  612  ################################\n",
      "Loss:  0.0002279150066897273\n",
      "################################  613  ################################\n",
      "Loss:  0.0002275550941703841\n",
      "################################  614  ################################\n",
      "Loss:  0.00022721986169926822\n",
      "################################  615  ################################\n",
      "Loss:  0.0002268290554638952\n",
      "################################  616  ################################\n",
      "Loss:  0.00022648625599686056\n",
      "################################  617  ################################\n",
      "Loss:  0.0002260527980979532\n",
      "################################  618  ################################\n",
      "Loss:  0.00022560034994967282\n",
      "################################  619  ################################\n",
      "Loss:  0.0002251043333671987\n",
      "################################  620  ################################\n",
      "Loss:  0.00022451634868048131\n",
      "################################  621  ################################\n",
      "Loss:  0.00022385914053302258\n",
      "################################  622  ################################\n",
      "Loss:  0.00022313676890917122\n",
      "################################  623  ################################\n",
      "Loss:  0.00022236521181184798\n",
      "################################  624  ################################\n",
      "Loss:  0.00022161478409543633\n",
      "################################  625  ################################\n",
      "Loss:  0.00022083261865191162\n",
      "################################  626  ################################\n",
      "Loss:  0.00022005298524163663\n",
      "################################  627  ################################\n",
      "Loss:  0.00021923155873082578\n",
      "################################  628  ################################\n",
      "Loss:  0.0002184347395086661\n",
      "################################  629  ################################\n",
      "Loss:  0.00021764264965895563\n",
      "################################  630  ################################\n",
      "Loss:  0.0002168510400224477\n",
      "################################  631  ################################\n",
      "Loss:  0.0002161039155907929\n",
      "################################  632  ################################\n",
      "Loss:  0.0002153534151148051\n",
      "################################  633  ################################\n",
      "Loss:  0.0002146734914276749\n",
      "################################  634  ################################\n",
      "Loss:  0.00021401573030743748\n",
      "################################  635  ################################\n",
      "Loss:  0.00021339277736842632\n",
      "################################  636  ################################\n",
      "Loss:  0.00021273925085552037\n",
      "################################  637  ################################\n",
      "Loss:  0.00021207047393545508\n",
      "################################  638  ################################\n",
      "Loss:  0.00021141476463526487\n",
      "################################  639  ################################\n",
      "Loss:  0.00021079671569168568\n",
      "################################  640  ################################\n",
      "Loss:  0.00021018131519667804\n",
      "################################  641  ################################\n",
      "Loss:  0.00020960172696504742\n",
      "################################  642  ################################\n",
      "Loss:  0.0002090065972879529\n",
      "################################  643  ################################\n",
      "Loss:  0.00020842996309511364\n",
      "################################  644  ################################\n",
      "Loss:  0.00020782704814337194\n",
      "################################  645  ################################\n",
      "Loss:  0.00020723638590425253\n",
      "################################  646  ################################\n",
      "Loss:  0.00020662570022977889\n",
      "################################  647  ################################\n",
      "Loss:  0.00020588809275068343\n",
      "################################  648  ################################\n",
      "Loss:  0.00020518951350823045\n",
      "################################  649  ################################\n",
      "Loss:  0.00020444518304429948\n",
      "################################  650  ################################\n",
      "Loss:  0.00020359446352813393\n",
      "################################  651  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.00020269509695935994\n",
      "################################  652  ################################\n",
      "Loss:  0.0002017646620515734\n",
      "################################  653  ################################\n",
      "Loss:  0.0002008424635278061\n",
      "################################  654  ################################\n",
      "Loss:  0.0001999256492126733\n",
      "################################  655  ################################\n",
      "Loss:  0.00019895816512871534\n",
      "################################  656  ################################\n",
      "Loss:  0.0001980193774215877\n",
      "################################  657  ################################\n",
      "Loss:  0.00019705924205482006\n",
      "################################  658  ################################\n",
      "Loss:  0.00019619686645455658\n",
      "################################  659  ################################\n",
      "Loss:  0.0001953563914867118\n",
      "################################  660  ################################\n",
      "Loss:  0.00019446064834482968\n",
      "################################  661  ################################\n",
      "Loss:  0.0001936637854669243\n",
      "################################  662  ################################\n",
      "Loss:  0.00019286206224933267\n",
      "################################  663  ################################\n",
      "Loss:  0.0001920145732583478\n",
      "################################  664  ################################\n",
      "Loss:  0.00019116289331577718\n",
      "################################  665  ################################\n",
      "Loss:  0.0001903700758703053\n",
      "################################  666  ################################\n",
      "Loss:  0.00018963809998240322\n",
      "################################  667  ################################\n",
      "Loss:  0.0001889128179755062\n",
      "################################  668  ################################\n",
      "Loss:  0.0001882464566733688\n",
      "################################  669  ################################\n",
      "Loss:  0.00018761042156256735\n",
      "################################  670  ################################\n",
      "Loss:  0.0001869821862783283\n",
      "################################  671  ################################\n",
      "Loss:  0.00018638258916325867\n",
      "################################  672  ################################\n",
      "Loss:  0.0001858387258835137\n",
      "################################  673  ################################\n",
      "Loss:  0.0001853136345744133\n",
      "################################  674  ################################\n",
      "Loss:  0.00018481160805094987\n",
      "################################  675  ################################\n",
      "Loss:  0.00018435443053022027\n",
      "################################  676  ################################\n",
      "Loss:  0.00018390166223980486\n",
      "################################  677  ################################\n",
      "Loss:  0.00018348556477576494\n",
      "################################  678  ################################\n",
      "Loss:  0.0001830897235777229\n",
      "################################  679  ################################\n",
      "Loss:  0.00018269667634740472\n",
      "################################  680  ################################\n",
      "Loss:  0.00018232052389066666\n",
      "################################  681  ################################\n",
      "Loss:  0.00018195787561126053\n",
      "################################  682  ################################\n",
      "Loss:  0.0001815997966332361\n",
      "################################  683  ################################\n",
      "Loss:  0.00018125746282748878\n",
      "################################  684  ################################\n",
      "Loss:  0.0001809288514778018\n",
      "################################  685  ################################\n",
      "Loss:  0.00018059840658679605\n",
      "################################  686  ################################\n",
      "Loss:  0.00018029073544312268\n",
      "################################  687  ################################\n",
      "Loss:  0.00017996538372244686\n",
      "################################  688  ################################\n",
      "Loss:  0.00017966001178137958\n",
      "################################  689  ################################\n",
      "Loss:  0.00017930709873326123\n",
      "################################  690  ################################\n",
      "Loss:  0.00017891309107653797\n",
      "################################  691  ################################\n",
      "Loss:  0.0001784591586329043\n",
      "################################  692  ################################\n",
      "Loss:  0.00017800375644583255\n",
      "################################  693  ################################\n",
      "Loss:  0.00017745126388035715\n",
      "################################  694  ################################\n",
      "Loss:  0.00017692493565846235\n",
      "################################  695  ################################\n",
      "Loss:  0.00017633891548030078\n",
      "################################  696  ################################\n",
      "Loss:  0.0001755790872266516\n",
      "################################  697  ################################\n",
      "Loss:  0.00017503570416010916\n",
      "################################  698  ################################\n",
      "Loss:  0.0001744748733472079\n",
      "################################  699  ################################\n",
      "Loss:  0.00017389640561304986\n",
      "################################  700  ################################\n",
      "Loss:  0.00017333932919427752\n",
      "################################  701  ################################\n",
      "Loss:  0.0001727804192341864\n",
      "################################  702  ################################\n",
      "Loss:  0.00017223147733602673\n",
      "################################  703  ################################\n",
      "Loss:  0.00017167977057397366\n",
      "################################  704  ################################\n",
      "Loss:  0.0001711480726953596\n",
      "################################  705  ################################\n",
      "Loss:  0.00017061541439034045\n",
      "################################  706  ################################\n",
      "Loss:  0.00017010178999044\n",
      "################################  707  ################################\n",
      "Loss:  0.00016960437642410398\n",
      "################################  708  ################################\n",
      "Loss:  0.00016913737636059523\n",
      "################################  709  ################################\n",
      "Loss:  0.0001686788455117494\n",
      "################################  710  ################################\n",
      "Loss:  0.00016821267490740865\n",
      "################################  711  ################################\n",
      "Loss:  0.0001677424879744649\n",
      "################################  712  ################################\n",
      "Loss:  0.00016723426233511418\n",
      "################################  713  ################################\n",
      "Loss:  0.00016675333608873188\n",
      "################################  714  ################################\n",
      "Loss:  0.00016621866961941123\n",
      "################################  715  ################################\n",
      "Loss:  0.00016574248729739338\n",
      "################################  716  ################################\n",
      "Loss:  0.0001651891798246652\n",
      "################################  717  ################################\n",
      "Loss:  0.0001646179734962061\n",
      "################################  718  ################################\n",
      "Loss:  0.0001640402479097247\n",
      "################################  719  ################################\n",
      "Loss:  0.0001633926876820624\n",
      "################################  720  ################################\n",
      "Loss:  0.00016278395196422935\n",
      "################################  721  ################################\n",
      "Loss:  0.00016221703845076263\n",
      "################################  722  ################################\n",
      "Loss:  0.00016166808200068772\n",
      "################################  723  ################################\n",
      "Loss:  0.00016111982404254377\n",
      "################################  724  ################################\n",
      "Loss:  0.00016056443564593792\n",
      "################################  725  ################################\n",
      "Loss:  0.0001599934184923768\n",
      "################################  726  ################################\n",
      "Loss:  0.00015943693870212883\n",
      "################################  727  ################################\n",
      "Loss:  0.00015888686175458133\n",
      "################################  728  ################################\n",
      "Loss:  0.00015836060629226267\n",
      "################################  729  ################################\n",
      "Loss:  0.0001578668161528185\n",
      "################################  730  ################################\n",
      "Loss:  0.00015737867215648293\n",
      "################################  731  ################################\n",
      "Loss:  0.00015695484762545675\n",
      "################################  732  ################################\n",
      "Loss:  0.0001565647980896756\n",
      "################################  733  ################################\n",
      "Loss:  0.00015617487952113152\n",
      "################################  734  ################################\n",
      "Loss:  0.00015580860781483352\n",
      "################################  735  ################################\n",
      "Loss:  0.0001554580230731517\n",
      "################################  736  ################################\n",
      "Loss:  0.0001550999586470425\n",
      "################################  737  ################################\n",
      "Loss:  0.0001547491701785475\n",
      "################################  738  ################################\n",
      "Loss:  0.0001544020342407748\n",
      "################################  739  ################################\n",
      "Loss:  0.00015406703460030258\n",
      "################################  740  ################################\n",
      "Loss:  0.00015374126087408513\n",
      "################################  741  ################################\n",
      "Loss:  0.0001534060575067997\n",
      "################################  742  ################################\n",
      "Loss:  0.00015307043213397264\n",
      "################################  743  ################################\n",
      "Loss:  0.00015271709708031267\n",
      "################################  744  ################################\n",
      "Loss:  0.00015233701560646296\n",
      "################################  745  ################################\n",
      "Loss:  0.00015193744911812246\n",
      "################################  746  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.00015152007108554244\n",
      "################################  747  ################################\n",
      "Loss:  0.00015111902030184865\n",
      "################################  748  ################################\n",
      "Loss:  0.00015070792869664729\n",
      "################################  749  ################################\n",
      "Loss:  0.00015031121438369155\n",
      "################################  750  ################################\n",
      "Loss:  0.00014991519856266677\n",
      "################################  751  ################################\n",
      "Loss:  0.00014950775948818773\n",
      "################################  752  ################################\n",
      "Loss:  0.00014910433674231172\n",
      "################################  753  ################################\n",
      "Loss:  0.00014870084123685956\n",
      "################################  754  ################################\n",
      "Loss:  0.00014830895815975964\n",
      "################################  755  ################################\n",
      "Loss:  0.00014792774163652211\n",
      "################################  756  ################################\n",
      "Loss:  0.00014755681331735104\n",
      "################################  757  ################################\n",
      "Loss:  0.00014719468890689313\n",
      "################################  758  ################################\n",
      "Loss:  0.00014683821063954383\n",
      "################################  759  ################################\n",
      "Loss:  0.00014649814693257213\n",
      "################################  760  ################################\n",
      "Loss:  0.00014616854605264962\n",
      "################################  761  ################################\n",
      "Loss:  0.00014585518511012197\n",
      "################################  762  ################################\n",
      "Loss:  0.00014554547669831663\n",
      "################################  763  ################################\n",
      "Loss:  0.00014525030564982444\n",
      "################################  764  ################################\n",
      "Loss:  0.00014497849042527378\n",
      "################################  765  ################################\n",
      "Loss:  0.00014469806046690792\n",
      "################################  766  ################################\n",
      "Loss:  0.00014446696150116622\n",
      "################################  767  ################################\n",
      "Loss:  0.00014423613902181387\n",
      "################################  768  ################################\n",
      "Loss:  0.00014402135275304317\n",
      "################################  769  ################################\n",
      "Loss:  0.000143808574648574\n",
      "################################  770  ################################\n",
      "Loss:  0.0001435713202226907\n",
      "################################  771  ################################\n",
      "Loss:  0.0001433492434443906\n",
      "################################  772  ################################\n",
      "Loss:  0.0001431258860975504\n",
      "################################  773  ################################\n",
      "Loss:  0.0001429056574124843\n",
      "################################  774  ################################\n",
      "Loss:  0.00014268577797338367\n",
      "################################  775  ################################\n",
      "Loss:  0.00014247334911487997\n",
      "################################  776  ################################\n",
      "Loss:  0.00014227152860257775\n",
      "################################  777  ################################\n",
      "Loss:  0.0001420726184733212\n",
      "################################  778  ################################\n",
      "Loss:  0.0001418775791535154\n",
      "################################  779  ################################\n",
      "Loss:  0.00014168246707413346\n",
      "################################  780  ################################\n",
      "Loss:  0.00014148402260616422\n",
      "################################  781  ################################\n",
      "Loss:  0.00014130029012449086\n",
      "################################  782  ################################\n",
      "Loss:  0.00014111993368715048\n",
      "################################  783  ################################\n",
      "Loss:  0.00014093727804720402\n",
      "################################  784  ################################\n",
      "Loss:  0.000140753312734887\n",
      "################################  785  ################################\n",
      "Loss:  0.00014056669897399843\n",
      "################################  786  ################################\n",
      "Loss:  0.0001403782080160454\n",
      "################################  787  ################################\n",
      "Loss:  0.0001401833724230528\n",
      "################################  788  ################################\n",
      "Loss:  0.00013998342910781503\n",
      "################################  789  ################################\n",
      "Loss:  0.0001397827873006463\n",
      "################################  790  ################################\n",
      "Loss:  0.00013958716590423137\n",
      "################################  791  ################################\n",
      "Loss:  0.00013938650954514742\n",
      "################################  792  ################################\n",
      "Loss:  0.00013918246258981526\n",
      "################################  793  ################################\n",
      "Loss:  0.0001389732351526618\n",
      "################################  794  ################################\n",
      "Loss:  0.00013874527940060943\n",
      "################################  795  ################################\n",
      "Loss:  0.00013851365656591952\n",
      "################################  796  ################################\n",
      "Loss:  0.00013826754002366215\n",
      "################################  797  ################################\n",
      "Loss:  0.0001380115281790495\n",
      "################################  798  ################################\n",
      "Loss:  0.00013775320257991552\n",
      "################################  799  ################################\n",
      "Loss:  0.0001374965941067785\n",
      "################################  800  ################################\n",
      "Loss:  0.00013724812015425414\n",
      "################################  801  ################################\n",
      "Loss:  0.00013700562703888863\n",
      "################################  802  ################################\n",
      "Loss:  0.00013676813978236169\n",
      "################################  803  ################################\n",
      "Loss:  0.0001365192438242957\n",
      "################################  804  ################################\n",
      "Loss:  0.0001362619805149734\n",
      "################################  805  ################################\n",
      "Loss:  0.0001360076857963577\n",
      "################################  806  ################################\n",
      "Loss:  0.00013575921184383333\n",
      "################################  807  ################################\n",
      "Loss:  0.00013552216114476323\n",
      "################################  808  ################################\n",
      "Loss:  0.00013528244744520634\n",
      "################################  809  ################################\n",
      "Loss:  0.00013503943046089262\n",
      "################################  810  ################################\n",
      "Loss:  0.00013478417531587183\n",
      "################################  811  ################################\n",
      "Loss:  0.00013453126302920282\n",
      "################################  812  ################################\n",
      "Loss:  0.00013429790851660073\n",
      "################################  813  ################################\n",
      "Loss:  0.0001340675662504509\n",
      "################################  814  ################################\n",
      "Loss:  0.00013383988698478788\n",
      "################################  815  ################################\n",
      "Loss:  0.00013360980665311217\n",
      "################################  816  ################################\n",
      "Loss:  0.00013336249685380608\n",
      "################################  817  ################################\n",
      "Loss:  0.00013314788520801812\n",
      "################################  818  ################################\n",
      "Loss:  0.00013292736548464745\n",
      "################################  819  ################################\n",
      "Loss:  0.00013270995987113565\n",
      "################################  820  ################################\n",
      "Loss:  0.0001324801705777645\n",
      "################################  821  ################################\n",
      "Loss:  0.00013225874863564968\n",
      "################################  822  ################################\n",
      "Loss:  0.0001320478186244145\n",
      "################################  823  ################################\n",
      "Loss:  0.00013181402755435556\n",
      "################################  824  ################################\n",
      "Loss:  0.00013158682850189507\n",
      "################################  825  ################################\n",
      "Loss:  0.00013134675100445747\n",
      "################################  826  ################################\n",
      "Loss:  0.00013110003783367574\n",
      "################################  827  ################################\n",
      "Loss:  0.00013085888349451125\n",
      "################################  828  ################################\n",
      "Loss:  0.000130610671476461\n",
      "################################  829  ################################\n",
      "Loss:  0.00013037135067861527\n",
      "################################  830  ################################\n",
      "Loss:  0.00013012849376536906\n",
      "################################  831  ################################\n",
      "Loss:  0.0001298869028687477\n",
      "################################  832  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0001296379487030208\n",
      "################################  833  ################################\n",
      "Loss:  0.000129402382299304\n",
      "################################  834  ################################\n",
      "Loss:  0.00012915898696519434\n",
      "################################  835  ################################\n",
      "Loss:  0.00012889920617453754\n",
      "################################  836  ################################\n",
      "Loss:  0.00012862816220149398\n",
      "################################  837  ################################\n",
      "Loss:  0.0001283259771298617\n",
      "################################  838  ################################\n",
      "Loss:  0.0001280162250623107\n",
      "################################  839  ################################\n",
      "Loss:  0.00012769372551701963\n",
      "################################  840  ################################\n",
      "Loss:  0.00012737588258460164\n",
      "################################  841  ################################\n",
      "Loss:  0.0001270513457711786\n",
      "################################  842  ################################\n",
      "Loss:  0.00012671499280259013\n",
      "################################  843  ################################\n",
      "Loss:  0.00012637309555429965\n",
      "################################  844  ################################\n",
      "Loss:  0.00012602523202076554\n",
      "################################  845  ################################\n",
      "Loss:  0.00012565962970256805\n",
      "################################  846  ################################\n",
      "Loss:  0.0001252664951607585\n",
      "################################  847  ################################\n",
      "Loss:  0.00012487388448789716\n",
      "################################  848  ################################\n",
      "Loss:  0.00012451899237930775\n",
      "################################  849  ################################\n",
      "Loss:  0.00012417143443599343\n",
      "################################  850  ################################\n",
      "Loss:  0.00012382245040498674\n",
      "################################  851  ################################\n",
      "Loss:  0.00012347659503575414\n",
      "################################  852  ################################\n",
      "Loss:  0.00012311422324273735\n",
      "################################  853  ################################\n",
      "Loss:  0.00012272666208446026\n",
      "################################  854  ################################\n",
      "Loss:  0.00012233643792569637\n",
      "################################  855  ################################\n",
      "Loss:  0.00012199416232760996\n",
      "################################  856  ################################\n",
      "Loss:  0.00012159546895418316\n",
      "################################  857  ################################\n",
      "Loss:  0.00012125130888307467\n",
      "################################  858  ################################\n",
      "Loss:  0.00012088534276699647\n",
      "################################  859  ################################\n",
      "Loss:  0.00012044272443745285\n",
      "################################  860  ################################\n",
      "Loss:  0.00012004112068098038\n",
      "################################  861  ################################\n",
      "Loss:  0.00011963189172092825\n",
      "################################  862  ################################\n",
      "Loss:  0.00011917908705072477\n",
      "################################  863  ################################\n",
      "Loss:  0.00011871958122355863\n",
      "################################  864  ################################\n",
      "Loss:  0.00011822392116300762\n",
      "################################  865  ################################\n",
      "Loss:  0.0001176932273665443\n",
      "################################  866  ################################\n",
      "Loss:  0.00011709643877111375\n",
      "################################  867  ################################\n",
      "Loss:  0.0001165455614682287\n",
      "################################  868  ################################\n",
      "Loss:  0.00011600536527112126\n",
      "################################  869  ################################\n",
      "Loss:  0.00011543727305252105\n",
      "################################  870  ################################\n",
      "Loss:  0.00011485158029245213\n",
      "################################  871  ################################\n",
      "Loss:  0.00011422789975767955\n",
      "################################  872  ################################\n",
      "Loss:  0.00011357022594893351\n",
      "################################  873  ################################\n",
      "Loss:  0.00011300404730718583\n",
      "################################  874  ################################\n",
      "Loss:  0.00011246161011513323\n",
      "################################  875  ################################\n",
      "Loss:  0.00011192185775144026\n",
      "################################  876  ################################\n",
      "Loss:  0.00011133824591524899\n",
      "################################  877  ################################\n",
      "Loss:  0.000110830798803363\n",
      "################################  878  ################################\n",
      "Loss:  0.00011038314551115036\n",
      "################################  879  ################################\n",
      "Loss:  0.00010993095929734409\n",
      "################################  880  ################################\n",
      "Loss:  0.00010948308045044541\n",
      "################################  881  ################################\n",
      "Loss:  0.00010905716044362634\n",
      "################################  882  ################################\n",
      "Loss:  0.00010866955562960356\n",
      "################################  883  ################################\n",
      "Loss:  0.00010830804239958525\n",
      "################################  884  ################################\n",
      "Loss:  0.00010795351408887655\n",
      "################################  885  ################################\n",
      "Loss:  0.00010760856093838811\n",
      "################################  886  ################################\n",
      "Loss:  0.00010730253416113555\n",
      "################################  887  ################################\n",
      "Loss:  0.00010701913561206311\n",
      "################################  888  ################################\n",
      "Loss:  0.00010675800149329007\n",
      "################################  889  ################################\n",
      "Loss:  0.00010651074262568727\n",
      "################################  890  ################################\n",
      "Loss:  0.00010626848961692303\n",
      "################################  891  ################################\n",
      "Loss:  0.00010604182898532599\n",
      "################################  892  ################################\n",
      "Loss:  0.00010583321272861212\n",
      "################################  893  ################################\n",
      "Loss:  0.00010563988325884566\n",
      "################################  894  ################################\n",
      "Loss:  0.00010545159602770582\n",
      "################################  895  ################################\n",
      "Loss:  0.00010527043923502788\n",
      "################################  896  ################################\n",
      "Loss:  0.00010509487765375525\n",
      "################################  897  ################################\n",
      "Loss:  0.00010492049477761611\n",
      "################################  898  ################################\n",
      "Loss:  0.00010474800365045667\n",
      "################################  899  ################################\n",
      "Loss:  0.00010457614553160965\n",
      "################################  900  ################################\n",
      "Loss:  0.00010439634206704795\n",
      "################################  901  ################################\n",
      "Loss:  0.00010420934995636344\n",
      "################################  902  ################################\n",
      "Loss:  0.00010401648614788428\n",
      "################################  903  ################################\n",
      "Loss:  0.0001038414120557718\n",
      "################################  904  ################################\n",
      "Loss:  0.00010366184869781137\n",
      "################################  905  ################################\n",
      "Loss:  0.00010348772775614634\n",
      "################################  906  ################################\n",
      "Loss:  0.00010331040539313108\n",
      "################################  907  ################################\n",
      "Loss:  0.00010312111407984048\n",
      "################################  908  ################################\n",
      "Loss:  0.00010292889783158898\n",
      "################################  909  ################################\n",
      "Loss:  0.0001027228354359977\n",
      "################################  910  ################################\n",
      "Loss:  0.00010251012281514704\n",
      "################################  911  ################################\n",
      "Loss:  0.00010228769679088145\n",
      "################################  912  ################################\n",
      "Loss:  0.0001020580020849593\n",
      "################################  913  ################################\n",
      "Loss:  0.00010183011181652546\n",
      "################################  914  ################################\n",
      "Loss:  0.0001015933376038447\n",
      "################################  915  ################################\n",
      "Loss:  0.00010136728087672964\n",
      "################################  916  ################################\n",
      "Loss:  0.0001011500135064125\n",
      "################################  917  ################################\n",
      "Loss:  0.00010092853335663676\n",
      "################################  918  ################################\n",
      "Loss:  0.00010071572614833713\n",
      "################################  919  ################################\n",
      "Loss:  0.00010051550634671003\n",
      "################################  920  ################################\n",
      "Loss:  0.00010031845886260271\n",
      "################################  921  ################################\n",
      "Loss:  0.00010012539860326797\n",
      "################################  922  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  9.991900878958404e-05\n",
      "################################  923  ################################\n",
      "Loss:  9.973865962820128e-05\n",
      "################################  924  ################################\n",
      "Loss:  9.957971633411944e-05\n",
      "################################  925  ################################\n",
      "Loss:  9.94268775684759e-05\n",
      "################################  926  ################################\n",
      "Loss:  9.927316568791866e-05\n",
      "################################  927  ################################\n",
      "Loss:  9.909929940477014e-05\n",
      "################################  928  ################################\n",
      "Loss:  9.893960668705404e-05\n",
      "################################  929  ################################\n",
      "Loss:  9.877213597064838e-05\n",
      "################################  930  ################################\n",
      "Loss:  9.858814155450091e-05\n",
      "################################  931  ################################\n",
      "Loss:  9.840268467087299e-05\n",
      "################################  932  ################################\n",
      "Loss:  9.821639832807705e-05\n",
      "################################  933  ################################\n",
      "Loss:  9.803169086808339e-05\n",
      "################################  934  ################################\n",
      "Loss:  9.784405119717121e-05\n",
      "################################  935  ################################\n",
      "Loss:  9.76609589997679e-05\n",
      "################################  936  ################################\n",
      "Loss:  9.746984869707376e-05\n",
      "################################  937  ################################\n",
      "Loss:  9.728004806675017e-05\n",
      "################################  938  ################################\n",
      "Loss:  9.708023571874946e-05\n",
      "################################  939  ################################\n",
      "Loss:  9.687110286904499e-05\n",
      "################################  940  ################################\n",
      "Loss:  9.664516255725175e-05\n",
      "################################  941  ################################\n",
      "Loss:  9.642887744121253e-05\n",
      "################################  942  ################################\n",
      "Loss:  9.619654156267643e-05\n",
      "################################  943  ################################\n",
      "Loss:  9.595324809197336e-05\n",
      "################################  944  ################################\n",
      "Loss:  9.569906978867948e-05\n",
      "################################  945  ################################\n",
      "Loss:  9.542290354147553e-05\n",
      "################################  946  ################################\n",
      "Loss:  9.514283738099039e-05\n",
      "################################  947  ################################\n",
      "Loss:  9.484545444138348e-05\n",
      "################################  948  ################################\n",
      "Loss:  9.454904648009688e-05\n",
      "################################  949  ################################\n",
      "Loss:  9.424325980944559e-05\n",
      "################################  950  ################################\n",
      "Loss:  9.395421511726454e-05\n",
      "################################  951  ################################\n",
      "Loss:  9.366226731799543e-05\n",
      "################################  952  ################################\n",
      "Loss:  9.33622577576898e-05\n",
      "################################  953  ################################\n",
      "Loss:  9.306325955549255e-05\n",
      "################################  954  ################################\n",
      "Loss:  9.276440687244758e-05\n",
      "################################  955  ################################\n",
      "Loss:  9.246720583178103e-05\n",
      "################################  956  ################################\n",
      "Loss:  9.217052138410509e-05\n",
      "################################  957  ################################\n",
      "Loss:  9.186429088003933e-05\n",
      "################################  958  ################################\n",
      "Loss:  9.156707528745756e-05\n",
      "################################  959  ################################\n",
      "Loss:  9.130304533755407e-05\n",
      "################################  960  ################################\n",
      "Loss:  9.105712524615228e-05\n",
      "################################  961  ################################\n",
      "Loss:  9.081533789867535e-05\n",
      "################################  962  ################################\n",
      "Loss:  9.057737770490348e-05\n",
      "################################  963  ################################\n",
      "Loss:  9.033558308146894e-05\n",
      "################################  964  ################################\n",
      "Loss:  9.01151797734201e-05\n",
      "################################  965  ################################\n",
      "Loss:  8.989530761027709e-05\n",
      "################################  966  ################################\n",
      "Loss:  8.966367749962956e-05\n",
      "################################  967  ################################\n",
      "Loss:  8.942882413975894e-05\n",
      "################################  968  ################################\n",
      "Loss:  8.919126412365586e-05\n",
      "################################  969  ################################\n",
      "Loss:  8.896026702132076e-05\n",
      "################################  970  ################################\n",
      "Loss:  8.873284241417423e-05\n",
      "################################  971  ################################\n",
      "Loss:  8.850006270222366e-05\n",
      "################################  972  ################################\n",
      "Loss:  8.826466364553198e-05\n",
      "################################  973  ################################\n",
      "Loss:  8.802268712315708e-05\n",
      "################################  974  ################################\n",
      "Loss:  8.777457696851343e-05\n",
      "################################  975  ################################\n",
      "Loss:  8.752250141697004e-05\n",
      "################################  976  ################################\n",
      "Loss:  8.726499800104648e-05\n",
      "################################  977  ################################\n",
      "Loss:  8.701693150214851e-05\n",
      "################################  978  ################################\n",
      "Loss:  8.676770085003227e-05\n",
      "################################  979  ################################\n",
      "Loss:  8.651339885545895e-05\n",
      "################################  980  ################################\n",
      "Loss:  8.625653572380543e-05\n",
      "################################  981  ################################\n",
      "Loss:  8.60045402077958e-05\n",
      "################################  982  ################################\n",
      "Loss:  8.577501284889877e-05\n",
      "################################  983  ################################\n",
      "Loss:  8.553365478292108e-05\n",
      "################################  984  ################################\n",
      "Loss:  8.529749902663752e-05\n",
      "################################  985  ################################\n",
      "Loss:  8.504168363288045e-05\n",
      "################################  986  ################################\n",
      "Loss:  8.478458039462566e-05\n",
      "################################  987  ################################\n",
      "Loss:  8.452197653241456e-05\n",
      "################################  988  ################################\n",
      "Loss:  8.421104575973004e-05\n",
      "################################  989  ################################\n",
      "Loss:  8.391297160414979e-05\n",
      "################################  990  ################################\n",
      "Loss:  8.362514927284792e-05\n",
      "################################  991  ################################\n",
      "Loss:  8.33164231153205e-05\n",
      "################################  992  ################################\n",
      "Loss:  8.301516936626285e-05\n",
      "################################  993  ################################\n",
      "Loss:  8.270886610262096e-05\n",
      "################################  994  ################################\n",
      "Loss:  8.239915041485801e-05\n",
      "################################  995  ################################\n",
      "Loss:  8.20895511424169e-05\n",
      "################################  996  ################################\n",
      "Loss:  8.181142038665712e-05\n",
      "################################  997  ################################\n",
      "Loss:  8.155270188581198e-05\n",
      "################################  998  ################################\n",
      "Loss:  8.128480112645775e-05\n",
      "################################  999  ################################\n",
      "Loss:  8.103848085738719e-05\n",
      "################################  1000  ################################\n",
      "Loss:  8.081033593043685e-05\n",
      "################################  1001  ################################\n",
      "Loss:  8.057126251515001e-05\n",
      "################################  1002  ################################\n",
      "Loss:  8.035350765567273e-05\n",
      "################################  1003  ################################\n",
      "Loss:  8.012682519620284e-05\n",
      "################################  1004  ################################\n",
      "Loss:  7.99196568550542e-05\n",
      "################################  1005  ################################\n",
      "Loss:  7.96902968431823e-05\n",
      "################################  1006  ################################\n",
      "Loss:  7.947850826894864e-05\n",
      "################################  1007  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  7.925455429358408e-05\n",
      "################################  1008  ################################\n",
      "Loss:  7.90166377555579e-05\n",
      "################################  1009  ################################\n",
      "Loss:  7.879448094172403e-05\n",
      "################################  1010  ################################\n",
      "Loss:  7.857446325942874e-05\n",
      "################################  1011  ################################\n",
      "Loss:  7.835992437321693e-05\n",
      "################################  1012  ################################\n",
      "Loss:  7.814827404217795e-05\n",
      "################################  1013  ################################\n",
      "Loss:  7.793303666403517e-05\n",
      "################################  1014  ################################\n",
      "Loss:  7.771377568133175e-05\n",
      "################################  1015  ################################\n",
      "Loss:  7.751243538223207e-05\n",
      "################################  1016  ################################\n",
      "Loss:  7.732296944595873e-05\n",
      "################################  1017  ################################\n",
      "Loss:  7.712449587415904e-05\n",
      "################################  1018  ################################\n",
      "Loss:  7.693599036429077e-05\n",
      "################################  1019  ################################\n",
      "Loss:  7.675429515074939e-05\n",
      "################################  1020  ################################\n",
      "Loss:  7.658069080207497e-05\n",
      "################################  1021  ################################\n",
      "Loss:  7.640795229235664e-05\n",
      "################################  1022  ################################\n",
      "Loss:  7.623963756486773e-05\n",
      "################################  1023  ################################\n",
      "Loss:  7.606369035784155e-05\n",
      "################################  1024  ################################\n",
      "Loss:  7.589509186800569e-05\n",
      "################################  1025  ################################\n",
      "Loss:  7.572556933155283e-05\n",
      "################################  1026  ################################\n",
      "Loss:  7.553834439022467e-05\n",
      "################################  1027  ################################\n",
      "Loss:  7.535612530773506e-05\n",
      "################################  1028  ################################\n",
      "Loss:  7.51712141209282e-05\n",
      "################################  1029  ################################\n",
      "Loss:  7.498366176150739e-05\n",
      "################################  1030  ################################\n",
      "Loss:  7.478046609321609e-05\n",
      "################################  1031  ################################\n",
      "Loss:  7.456964522134513e-05\n",
      "################################  1032  ################################\n",
      "Loss:  7.43564814911224e-05\n",
      "################################  1033  ################################\n",
      "Loss:  7.414669380523264e-05\n",
      "################################  1034  ################################\n",
      "Loss:  7.394437125185505e-05\n",
      "################################  1035  ################################\n",
      "Loss:  7.372221443802118e-05\n",
      "################################  1036  ################################\n",
      "Loss:  7.351381646003574e-05\n",
      "################################  1037  ################################\n",
      "Loss:  7.32900807633996e-05\n",
      "################################  1038  ################################\n",
      "Loss:  7.308424392249435e-05\n",
      "################################  1039  ################################\n",
      "Loss:  7.286493200808764e-05\n",
      "################################  1040  ################################\n",
      "Loss:  7.263381849043071e-05\n",
      "################################  1041  ################################\n",
      "Loss:  7.239971455419436e-05\n",
      "################################  1042  ################################\n",
      "Loss:  7.218264363473281e-05\n",
      "################################  1043  ################################\n",
      "Loss:  7.196947990451008e-05\n",
      "################################  1044  ################################\n",
      "Loss:  7.174763595685363e-05\n",
      "################################  1045  ################################\n",
      "Loss:  7.153714977903292e-05\n",
      "################################  1046  ################################\n",
      "Loss:  7.133309554774314e-05\n",
      "################################  1047  ################################\n",
      "Loss:  7.11487082298845e-05\n",
      "################################  1048  ################################\n",
      "Loss:  7.097396883182228e-05\n",
      "################################  1049  ################################\n",
      "Loss:  7.080263458192348e-05\n",
      "################################  1050  ################################\n",
      "Loss:  7.064157398417592e-05\n",
      "################################  1051  ################################\n",
      "Loss:  7.049022678984329e-05\n",
      "################################  1052  ################################\n",
      "Loss:  7.035440648905933e-05\n",
      "################################  1053  ################################\n",
      "Loss:  7.021734199952334e-05\n",
      "################################  1054  ################################\n",
      "Loss:  7.008286047494039e-05\n",
      "################################  1055  ################################\n",
      "Loss:  6.994909199420363e-05\n",
      "################################  1056  ################################\n",
      "Loss:  6.981027399888262e-05\n",
      "################################  1057  ################################\n",
      "Loss:  6.967320950934663e-05\n",
      "################################  1058  ################################\n",
      "Loss:  6.953655247343704e-05\n",
      "################################  1059  ################################\n",
      "Loss:  6.939357990631834e-05\n",
      "################################  1060  ################################\n",
      "Loss:  6.925100751686841e-05\n",
      "################################  1061  ################################\n",
      "Loss:  6.910620868438855e-05\n",
      "################################  1062  ################################\n",
      "Loss:  6.896494596730918e-05\n",
      "################################  1063  ################################\n",
      "Loss:  6.883048627059907e-05\n",
      "################################  1064  ################################\n",
      "Loss:  6.869468779768795e-05\n",
      "################################  1065  ################################\n",
      "Loss:  6.856274558231235e-05\n",
      "################################  1066  ################################\n",
      "Loss:  6.842828588560224e-05\n",
      "################################  1067  ################################\n",
      "Loss:  6.829272024333477e-05\n",
      "################################  1068  ################################\n",
      "Loss:  6.815706728957593e-05\n",
      "################################  1069  ################################\n",
      "Loss:  6.802154530305415e-05\n",
      "################################  1070  ################################\n",
      "Loss:  6.788722384953871e-05\n",
      "################################  1071  ################################\n",
      "Loss:  6.774967914680019e-05\n",
      "################################  1072  ################################\n",
      "Loss:  6.760591350030154e-05\n",
      "################################  1073  ################################\n",
      "Loss:  6.74595867167227e-05\n",
      "################################  1074  ################################\n",
      "Loss:  6.730621680617332e-05\n",
      "################################  1075  ################################\n",
      "Loss:  6.715670315315947e-05\n",
      "################################  1076  ################################\n",
      "Loss:  6.700211088173091e-05\n",
      "################################  1077  ################################\n",
      "Loss:  6.684382242383435e-05\n",
      "################################  1078  ################################\n",
      "Loss:  6.6684304329101e-05\n",
      "################################  1079  ################################\n",
      "Loss:  6.652822776231915e-05\n",
      "################################  1080  ################################\n",
      "Loss:  6.637601472903043e-05\n",
      "################################  1081  ################################\n",
      "Loss:  6.622490036534145e-05\n",
      "################################  1082  ################################\n",
      "Loss:  6.608515832340345e-05\n",
      "################################  1083  ################################\n",
      "Loss:  6.594803562620655e-05\n",
      "################################  1084  ################################\n",
      "Loss:  6.582020432688296e-05\n",
      "################################  1085  ################################\n",
      "Loss:  6.569868855876848e-05\n",
      "################################  1086  ################################\n",
      "Loss:  6.558484892593697e-05\n",
      "################################  1087  ################################\n",
      "Loss:  6.547280645463616e-05\n",
      "################################  1088  ################################\n",
      "Loss:  6.53711031191051e-05\n",
      "################################  1089  ################################\n",
      "Loss:  6.526787183247507e-05\n",
      "################################  1090  ################################\n",
      "Loss:  6.516374560305849e-05\n",
      "################################  1091  ################################\n",
      "Loss:  6.506704085040838e-05\n",
      "################################  1092  ################################\n",
      "Loss:  6.496808782685548e-05\n",
      "################################  1093  ################################\n",
      "Loss:  6.487792416010052e-05\n",
      "################################  1094  ################################\n",
      "Loss:  6.478612340288237e-05\n",
      "################################  1095  ################################\n",
      "Loss:  6.468580977525562e-05\n",
      "################################  1096  ################################\n",
      "Loss:  6.457070412579924e-05\n",
      "################################  1097  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  6.447279884014279e-05\n",
      "################################  1098  ################################\n",
      "Loss:  6.437717820517719e-05\n",
      "################################  1099  ################################\n",
      "Loss:  6.427619518944994e-05\n",
      "################################  1100  ################################\n",
      "Loss:  6.417389522539452e-05\n",
      "################################  1101  ################################\n",
      "Loss:  6.407337059499696e-05\n",
      "################################  1102  ################################\n",
      "Loss:  6.396595563273877e-05\n",
      "################################  1103  ################################\n",
      "Loss:  6.386481982190162e-05\n",
      "################################  1104  ################################\n",
      "Loss:  6.374669465003535e-05\n",
      "################################  1105  ################################\n",
      "Loss:  6.359864346450195e-05\n",
      "################################  1106  ################################\n",
      "Loss:  6.342408596538007e-05\n",
      "################################  1107  ################################\n",
      "Loss:  6.32852825219743e-05\n",
      "################################  1108  ################################\n",
      "Loss:  6.313460471574217e-05\n",
      "################################  1109  ################################\n",
      "Loss:  6.295605271589011e-05\n",
      "################################  1110  ################################\n",
      "Loss:  6.277707143453881e-05\n",
      "################################  1111  ################################\n",
      "Loss:  6.259298243094236e-05\n",
      "################################  1112  ################################\n",
      "Loss:  6.241030496312305e-05\n",
      "################################  1113  ################################\n",
      "Loss:  6.222535012057051e-05\n",
      "################################  1114  ################################\n",
      "Loss:  6.204601959325373e-05\n",
      "################################  1115  ################################\n",
      "Loss:  6.187510734889656e-05\n",
      "################################  1116  ################################\n",
      "Loss:  6.170140113681555e-05\n",
      "################################  1117  ################################\n",
      "Loss:  6.15380922681652e-05\n",
      "################################  1118  ################################\n",
      "Loss:  6.137616583146155e-05\n",
      "################################  1119  ################################\n",
      "Loss:  6.12232179264538e-05\n",
      "################################  1120  ################################\n",
      "Loss:  6.106591899879277e-05\n",
      "################################  1121  ################################\n",
      "Loss:  6.090596434660256e-05\n",
      "################################  1122  ################################\n",
      "Loss:  6.0759175539715216e-05\n",
      "################################  1123  ################################\n",
      "Loss:  6.06140420131851e-05\n",
      "################################  1124  ################################\n",
      "Loss:  6.048080467735417e-05\n",
      "################################  1125  ################################\n",
      "Loss:  6.035175465513021e-05\n",
      "################################  1126  ################################\n",
      "Loss:  6.0233309341128916e-05\n",
      "################################  1127  ################################\n",
      "Loss:  6.012280937284231e-05\n",
      "################################  1128  ################################\n",
      "Loss:  6.001352448947728e-05\n",
      "################################  1129  ################################\n",
      "Loss:  5.9907724789809436e-05\n",
      "################################  1130  ################################\n",
      "Loss:  5.97984399064444e-05\n",
      "################################  1131  ################################\n",
      "Loss:  5.969398989691399e-05\n",
      "################################  1132  ################################\n",
      "Loss:  5.959003101452254e-05\n",
      "################################  1133  ################################\n",
      "Loss:  5.948031321167946e-05\n",
      "################################  1134  ################################\n",
      "Loss:  5.937095556873828e-05\n",
      "################################  1135  ################################\n",
      "Loss:  5.9271606005495414e-05\n",
      "################################  1136  ################################\n",
      "Loss:  5.9172896726522595e-05\n",
      "################################  1137  ################################\n",
      "Loss:  5.908148887101561e-05\n",
      "################################  1138  ################################\n",
      "Loss:  5.898579547647387e-05\n",
      "################################  1139  ################################\n",
      "Loss:  5.8889854699373245e-05\n",
      "################################  1140  ################################\n",
      "Loss:  5.879194213775918e-05\n",
      "################################  1141  ################################\n",
      "Loss:  5.868588050361723e-05\n",
      "################################  1142  ################################\n",
      "Loss:  5.8578516473062336e-05\n",
      "################################  1143  ################################\n",
      "Loss:  5.8477708080317825e-05\n",
      "################################  1144  ################################\n",
      "Loss:  5.838122160639614e-05\n",
      "################################  1145  ################################\n",
      "Loss:  5.827888526255265e-05\n",
      "################################  1146  ################################\n",
      "Loss:  5.818192221340723e-05\n",
      "################################  1147  ################################\n",
      "Loss:  5.808824425912462e-05\n",
      "################################  1148  ################################\n",
      "Loss:  5.799442806164734e-05\n",
      "################################  1149  ################################\n",
      "Loss:  5.790793147752993e-05\n",
      "################################  1150  ################################\n",
      "Loss:  5.7824789109872654e-05\n",
      "################################  1151  ################################\n",
      "Loss:  5.774842429673299e-05\n",
      "################################  1152  ################################\n",
      "Loss:  5.767440961790271e-05\n",
      "################################  1153  ################################\n",
      "Loss:  5.76020720473025e-05\n",
      "################################  1154  ################################\n",
      "Loss:  5.753114237450063e-05\n",
      "################################  1155  ################################\n",
      "Loss:  5.746394890593365e-05\n",
      "################################  1156  ################################\n",
      "Loss:  5.739842526963912e-05\n",
      "################################  1157  ################################\n",
      "Loss:  5.733368743676692e-05\n",
      "################################  1158  ################################\n",
      "Loss:  5.72713470319286e-05\n",
      "################################  1159  ################################\n",
      "Loss:  5.720636545447633e-05\n",
      "################################  1160  ################################\n",
      "Loss:  5.714370490750298e-05\n",
      "################################  1161  ################################\n",
      "Loss:  5.7078861573245376e-05\n",
      "################################  1162  ################################\n",
      "Loss:  5.7007226132554933e-05\n",
      "################################  1163  ################################\n",
      "Loss:  5.693371349480003e-05\n",
      "################################  1164  ################################\n",
      "Loss:  5.685347059625201e-05\n",
      "################################  1165  ################################\n",
      "Loss:  5.677597437170334e-05\n",
      "################################  1166  ################################\n",
      "Loss:  5.6696138926781714e-05\n",
      "################################  1167  ################################\n",
      "Loss:  5.660863462253474e-05\n",
      "################################  1168  ################################\n",
      "Loss:  5.6521850638091564e-05\n",
      "################################  1169  ################################\n",
      "Loss:  5.6426004448439926e-05\n",
      "################################  1170  ################################\n",
      "Loss:  5.6334214605158195e-05\n",
      "################################  1171  ################################\n",
      "Loss:  5.6229873734992e-05\n",
      "################################  1172  ################################\n",
      "Loss:  5.611130109173246e-05\n",
      "################################  1173  ################################\n",
      "Loss:  5.598439020104706e-05\n",
      "################################  1174  ################################\n",
      "Loss:  5.5856671679066494e-05\n",
      "################################  1175  ################################\n",
      "Loss:  5.5728054576320574e-05\n",
      "################################  1176  ################################\n",
      "Loss:  5.5578184401383623e-05\n",
      "################################  1177  ################################\n",
      "Loss:  5.5444535973947495e-05\n",
      "################################  1178  ################################\n",
      "Loss:  5.530583075596951e-05\n",
      "################################  1179  ################################\n",
      "Loss:  5.515319207916036e-05\n",
      "################################  1180  ################################\n",
      "Loss:  5.50020340597257e-05\n",
      "################################  1181  ################################\n",
      "Loss:  5.484790017362684e-05\n",
      "################################  1182  ################################\n",
      "Loss:  5.470053292810917e-05\n",
      "################################  1183  ################################\n",
      "Loss:  5.4551142966374755e-05\n",
      "################################  1184  ################################\n",
      "Loss:  5.441018220153637e-05\n",
      "################################  1185  ################################\n",
      "Loss:  5.427848009276204e-05\n",
      "################################  1186  ################################\n",
      "Loss:  5.413472536019981e-05\n",
      "################################  1187  ################################\n",
      "Loss:  5.4012769396649674e-05\n",
      "################################  1188  ################################\n",
      "Loss:  5.3892690630164e-05\n",
      "################################  1189  ################################\n",
      "Loss:  5.379066715249792e-05\n",
      "################################  1190  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  5.369157588575035e-05\n",
      "################################  1191  ################################\n",
      "Loss:  5.35730468982365e-05\n",
      "################################  1192  ################################\n",
      "Loss:  5.346965554053895e-05\n",
      "################################  1193  ################################\n",
      "Loss:  5.3345487685874104e-05\n",
      "################################  1194  ################################\n",
      "Loss:  5.323793084244244e-05\n",
      "################################  1195  ################################\n",
      "Loss:  5.310142660164274e-05\n",
      "################################  1196  ################################\n",
      "Loss:  5.296649760566652e-05\n",
      "################################  1197  ################################\n",
      "Loss:  5.280019831843674e-05\n",
      "################################  1198  ################################\n",
      "Loss:  5.26652074768208e-05\n",
      "################################  1199  ################################\n",
      "Loss:  5.2506144129438326e-05\n",
      "################################  1200  ################################\n",
      "Loss:  5.2314095228211954e-05\n",
      "################################  1201  ################################\n",
      "Loss:  5.2116025472059846e-05\n",
      "################################  1202  ################################\n",
      "Loss:  5.190499723539688e-05\n",
      "################################  1203  ################################\n",
      "Loss:  5.1694347348529845e-05\n",
      "################################  1204  ################################\n",
      "Loss:  5.1490449550328776e-05\n",
      "################################  1205  ################################\n",
      "Loss:  5.129002238390967e-05\n",
      "################################  1206  ################################\n",
      "Loss:  5.1095379603793845e-05\n",
      "################################  1207  ################################\n",
      "Loss:  5.090066770208068e-05\n",
      "################################  1208  ################################\n",
      "Loss:  5.071091072750278e-05\n",
      "################################  1209  ################################\n",
      "Loss:  5.0521826779004186e-05\n",
      "################################  1210  ################################\n",
      "Loss:  5.035211506765336e-05\n",
      "################################  1211  ################################\n",
      "Loss:  5.01854156027548e-05\n",
      "################################  1212  ################################\n",
      "Loss:  5.0014474254567176e-05\n",
      "################################  1213  ################################\n",
      "Loss:  4.985388295608573e-05\n",
      "################################  1214  ################################\n",
      "Loss:  4.969280416844413e-05\n",
      "################################  1215  ################################\n",
      "Loss:  4.95495623908937e-05\n",
      "################################  1216  ################################\n",
      "Loss:  4.940529470331967e-05\n",
      "################################  1217  ################################\n",
      "Loss:  4.926698966301046e-05\n",
      "################################  1218  ################################\n",
      "Loss:  4.9126825615530834e-05\n",
      "################################  1219  ################################\n",
      "Loss:  4.898636325378902e-05\n",
      "################################  1220  ################################\n",
      "Loss:  4.8844682169146836e-05\n",
      "################################  1221  ################################\n",
      "Loss:  4.870642442256212e-05\n",
      "################################  1222  ################################\n",
      "Loss:  4.856173472944647e-05\n",
      "################################  1223  ################################\n",
      "Loss:  4.842722410103306e-05\n",
      "################################  1224  ################################\n",
      "Loss:  4.824691859539598e-05\n",
      "################################  1225  ################################\n",
      "Loss:  4.810906102648005e-05\n",
      "################################  1226  ################################\n",
      "Loss:  4.79142545373179e-05\n",
      "################################  1227  ################################\n",
      "Loss:  4.7753474063938484e-05\n",
      "################################  1228  ################################\n",
      "Loss:  4.7556866775266826e-05\n",
      "################################  1229  ################################\n",
      "Loss:  4.7334044211311266e-05\n",
      "################################  1230  ################################\n",
      "Loss:  4.707785046775825e-05\n",
      "################################  1231  ################################\n",
      "Loss:  4.68722173536662e-05\n",
      "################################  1232  ################################\n",
      "Loss:  4.664601874537766e-05\n",
      "################################  1233  ################################\n",
      "Loss:  4.6407003537751734e-05\n",
      "################################  1234  ################################\n",
      "Loss:  4.6184955863282084e-05\n",
      "################################  1235  ################################\n",
      "Loss:  4.596376675181091e-05\n",
      "################################  1236  ################################\n",
      "Loss:  4.5772707380820066e-05\n",
      "################################  1237  ################################\n",
      "Loss:  4.560224260785617e-05\n",
      "################################  1238  ################################\n",
      "Loss:  4.543639442999847e-05\n",
      "################################  1239  ################################\n",
      "Loss:  4.5292450522538275e-05\n",
      "################################  1240  ################################\n",
      "Loss:  4.515310865826905e-05\n",
      "################################  1241  ################################\n",
      "Loss:  4.504054595599882e-05\n",
      "################################  1242  ################################\n",
      "Loss:  4.494313907343894e-05\n",
      "################################  1243  ################################\n",
      "Loss:  4.484245437197387e-05\n",
      "################################  1244  ################################\n",
      "Loss:  4.4755477574653924e-05\n",
      "################################  1245  ################################\n",
      "Loss:  4.467176768230274e-05\n",
      "################################  1246  ################################\n",
      "Loss:  4.45967452833429e-05\n",
      "################################  1247  ################################\n",
      "Loss:  4.4526746933115646e-05\n",
      "################################  1248  ################################\n",
      "Loss:  4.444360820343718e-05\n",
      "################################  1249  ################################\n",
      "Loss:  4.437772076926194e-05\n",
      "################################  1250  ################################\n",
      "Loss:  4.430577246239409e-05\n",
      "################################  1251  ################################\n",
      "Loss:  4.4226020690985024e-05\n",
      "################################  1252  ################################\n",
      "Loss:  4.413876376929693e-05\n",
      "################################  1253  ################################\n",
      "Loss:  4.4067743147024885e-05\n",
      "################################  1254  ################################\n",
      "Loss:  4.399582394398749e-05\n",
      "################################  1255  ################################\n",
      "Loss:  4.391834227135405e-05\n",
      "################################  1256  ################################\n",
      "Loss:  4.384169733384624e-05\n",
      "################################  1257  ################################\n",
      "Loss:  4.37664712080732e-05\n",
      "################################  1258  ################################\n",
      "Loss:  4.369242014945485e-05\n",
      "################################  1259  ################################\n",
      "Loss:  4.362095569376834e-05\n",
      "################################  1260  ################################\n",
      "Loss:  4.3548570829443634e-05\n",
      "################################  1261  ################################\n",
      "Loss:  4.3485684727784246e-05\n",
      "################################  1262  ################################\n",
      "Loss:  4.3422191083664075e-05\n",
      "################################  1263  ################################\n",
      "Loss:  4.3360483687138185e-05\n",
      "################################  1264  ################################\n",
      "Loss:  4.330000592744909e-05\n",
      "################################  1265  ################################\n",
      "Loss:  4.323483517509885e-05\n",
      "################################  1266  ################################\n",
      "Loss:  4.3172996811335906e-05\n",
      "################################  1267  ################################\n",
      "Loss:  4.3112617277074605e-05\n",
      "################################  1268  ################################\n",
      "Loss:  4.305368202039972e-05\n",
      "################################  1269  ################################\n",
      "Loss:  4.299635475035757e-05\n",
      "################################  1270  ################################\n",
      "Loss:  4.294102473068051e-05\n",
      "################################  1271  ################################\n",
      "Loss:  4.2887721065199e-05\n",
      "################################  1272  ################################\n",
      "Loss:  4.283632733859122e-05\n",
      "################################  1273  ################################\n",
      "Loss:  4.278816413716413e-05\n",
      "################################  1274  ################################\n",
      "Loss:  4.274089587852359e-05\n",
      "################################  1275  ################################\n",
      "Loss:  4.269554847269319e-05\n",
      "################################  1276  ################################\n",
      "Loss:  4.265113238943741e-05\n",
      "################################  1277  ################################\n",
      "Loss:  4.2605370254023e-05\n",
      "################################  1278  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  4.256076135789044e-05\n",
      "################################  1279  ################################\n",
      "Loss:  4.2514409869909286e-05\n",
      "################################  1280  ################################\n",
      "Loss:  4.247164906701073e-05\n",
      "################################  1281  ################################\n",
      "Loss:  4.24247446062509e-05\n",
      "################################  1282  ################################\n",
      "Loss:  4.23685232817661e-05\n",
      "################################  1283  ################################\n",
      "Loss:  4.231233469909057e-05\n",
      "################################  1284  ################################\n",
      "Loss:  4.225364682497457e-05\n",
      "################################  1285  ################################\n",
      "Loss:  4.21984150307253e-05\n",
      "################################  1286  ################################\n",
      "Loss:  4.2145133193116635e-05\n",
      "################################  1287  ################################\n",
      "Loss:  4.2090658098459244e-05\n",
      "################################  1288  ################################\n",
      "Loss:  4.203840944683179e-05\n",
      "################################  1289  ################################\n",
      "Loss:  4.1985131247201934e-05\n",
      "################################  1290  ################################\n",
      "Loss:  4.193883069092408e-05\n",
      "################################  1291  ################################\n",
      "Loss:  4.189603350823745e-05\n",
      "################################  1292  ################################\n",
      "Loss:  4.184967838227749e-05\n",
      "################################  1293  ################################\n",
      "Loss:  4.180874384474009e-05\n",
      "################################  1294  ################################\n",
      "Loss:  4.1765335481613874e-05\n",
      "################################  1295  ################################\n",
      "Loss:  4.171623368165456e-05\n",
      "################################  1296  ################################\n",
      "Loss:  4.166646976955235e-05\n",
      "################################  1297  ################################\n",
      "Loss:  4.1618204704718664e-05\n",
      "################################  1298  ################################\n",
      "Loss:  4.157072544330731e-05\n",
      "################################  1299  ################################\n",
      "Loss:  4.1524323023622856e-05\n",
      "################################  1300  ################################\n",
      "Loss:  4.148154039285146e-05\n",
      "################################  1301  ################################\n",
      "Loss:  4.143566548009403e-05\n",
      "################################  1302  ################################\n",
      "Loss:  4.1393592255190015e-05\n",
      "################################  1303  ################################\n",
      "Loss:  4.134752452955581e-05\n",
      "################################  1304  ################################\n",
      "Loss:  4.1300299926660955e-05\n",
      "################################  1305  ################################\n",
      "Loss:  4.125200939597562e-05\n",
      "################################  1306  ################################\n",
      "Loss:  4.120328230783343e-05\n",
      "################################  1307  ################################\n",
      "Loss:  4.115691626793705e-05\n",
      "################################  1308  ################################\n",
      "Loss:  4.1108487494057044e-05\n",
      "################################  1309  ################################\n",
      "Loss:  4.106380220036954e-05\n",
      "################################  1310  ################################\n",
      "Loss:  4.10186912631616e-05\n",
      "################################  1311  ################################\n",
      "Loss:  4.0974315197672695e-05\n",
      "################################  1312  ################################\n",
      "Loss:  4.092954259249382e-05\n",
      "################################  1313  ################################\n",
      "Loss:  4.08850064559374e-05\n",
      "################################  1314  ################################\n",
      "Loss:  4.084580723429099e-05\n",
      "################################  1315  ################################\n",
      "Loss:  4.079724749317393e-05\n",
      "################################  1316  ################################\n",
      "Loss:  4.075817923876457e-05\n",
      "################################  1317  ################################\n",
      "Loss:  4.070728755323216e-05\n",
      "################################  1318  ################################\n",
      "Loss:  4.065916800755076e-05\n",
      "################################  1319  ################################\n",
      "Loss:  4.0602699300507084e-05\n",
      "################################  1320  ################################\n",
      "Loss:  4.0554376028012484e-05\n",
      "################################  1321  ################################\n",
      "Loss:  4.0504441130906343e-05\n",
      "################################  1322  ################################\n",
      "Loss:  4.0449351217830554e-05\n",
      "################################  1323  ################################\n",
      "Loss:  4.039660416310653e-05\n",
      "################################  1324  ################################\n",
      "Loss:  4.034254379803315e-05\n",
      "################################  1325  ################################\n",
      "Loss:  4.029014962725341e-05\n",
      "################################  1326  ################################\n",
      "Loss:  4.0231156162917614e-05\n",
      "################################  1327  ################################\n",
      "Loss:  4.0189850551541895e-05\n",
      "################################  1328  ################################\n",
      "Loss:  4.014919977635145e-05\n",
      "################################  1329  ################################\n",
      "Loss:  4.0107603126671165e-05\n",
      "################################  1330  ################################\n",
      "Loss:  4.00717617594637e-05\n",
      "################################  1331  ################################\n",
      "Loss:  4.003599678981118e-05\n",
      "################################  1332  ################################\n",
      "Loss:  4.000246917712502e-05\n",
      "################################  1333  ################################\n",
      "Loss:  3.997242311015725e-05\n",
      "################################  1334  ################################\n",
      "Loss:  3.99416167056188e-05\n",
      "################################  1335  ################################\n",
      "Loss:  3.991471749031916e-05\n",
      "################################  1336  ################################\n",
      "Loss:  3.9885744627099484e-05\n",
      "################################  1337  ################################\n",
      "Loss:  3.985210423707031e-05\n",
      "################################  1338  ################################\n",
      "Loss:  3.981719783041626e-05\n",
      "################################  1339  ################################\n",
      "Loss:  3.978349195676856e-05\n",
      "################################  1340  ################################\n",
      "Loss:  3.9751706935931e-05\n",
      "################################  1341  ################################\n",
      "Loss:  3.9714694139547646e-05\n",
      "################################  1342  ################################\n",
      "Loss:  3.9686514355707914e-05\n",
      "################################  1343  ################################\n",
      "Loss:  3.965477299061604e-05\n",
      "################################  1344  ################################\n",
      "Loss:  3.961914262617938e-05\n",
      "################################  1345  ################################\n",
      "Loss:  3.9582526369486004e-05\n",
      "################################  1346  ################################\n",
      "Loss:  3.954736894229427e-05\n",
      "################################  1347  ################################\n",
      "Loss:  3.951320468331687e-05\n",
      "################################  1348  ################################\n",
      "Loss:  3.947962977690622e-05\n",
      "################################  1349  ################################\n",
      "Loss:  3.9444239519070834e-05\n",
      "################################  1350  ################################\n",
      "Loss:  3.9411690522683784e-05\n",
      "################################  1351  ################################\n",
      "Loss:  3.938140071113594e-05\n",
      "################################  1352  ################################\n",
      "Loss:  3.93522423109971e-05\n",
      "################################  1353  ################################\n",
      "Loss:  3.9323440432781354e-05\n",
      "################################  1354  ################################\n",
      "Loss:  3.929271406377666e-05\n",
      "################################  1355  ################################\n",
      "Loss:  3.926691715605557e-05\n",
      "################################  1356  ################################\n",
      "Loss:  3.9240105252247304e-05\n",
      "################################  1357  ################################\n",
      "Loss:  3.921548341168091e-05\n",
      "################################  1358  ################################\n",
      "Loss:  3.918886432074942e-05\n",
      "################################  1359  ################################\n",
      "Loss:  3.91575631510932e-05\n",
      "################################  1360  ################################\n",
      "Loss:  3.9126724004745483e-05\n",
      "################################  1361  ################################\n",
      "Loss:  3.90950299333781e-05\n",
      "################################  1362  ################################\n",
      "Loss:  3.90630702895578e-05\n",
      "################################  1363  ################################\n",
      "Loss:  3.902939715771936e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  1364  ################################\n",
      "Loss:  3.899752118741162e-05\n",
      "################################  1365  ################################\n",
      "Loss:  3.8965717976680025e-05\n",
      "################################  1366  ################################\n",
      "Loss:  3.8932845200179145e-05\n",
      "################################  1367  ################################\n",
      "Loss:  3.89001434086822e-05\n",
      "################################  1368  ################################\n",
      "Loss:  3.8865458918735385e-05\n",
      "################################  1369  ################################\n",
      "Loss:  3.8830457924632356e-05\n",
      "################################  1370  ################################\n",
      "Loss:  3.87936488550622e-05\n",
      "################################  1371  ################################\n",
      "Loss:  3.8756494177505374e-05\n",
      "################################  1372  ################################\n",
      "Loss:  3.8717666029697284e-05\n",
      "################################  1373  ################################\n",
      "Loss:  3.867773193633184e-05\n",
      "################################  1374  ################################\n",
      "Loss:  3.863745951093733e-05\n",
      "################################  1375  ################################\n",
      "Loss:  3.859762364299968e-05\n",
      "################################  1376  ################################\n",
      "Loss:  3.855757677229121e-05\n",
      "################################  1377  ################################\n",
      "Loss:  3.8516758650075644e-05\n",
      "################################  1378  ################################\n",
      "Loss:  3.847693369607441e-05\n",
      "################################  1379  ################################\n",
      "Loss:  3.843711601803079e-05\n",
      "################################  1380  ################################\n",
      "Loss:  3.839728742605075e-05\n",
      "################################  1381  ################################\n",
      "Loss:  3.835602183244191e-05\n",
      "################################  1382  ################################\n",
      "Loss:  3.831692083622329e-05\n",
      "################################  1383  ################################\n",
      "Loss:  3.8276371924439445e-05\n",
      "################################  1384  ################################\n",
      "Loss:  3.823514634859748e-05\n",
      "################################  1385  ################################\n",
      "Loss:  3.819219637080096e-05\n",
      "################################  1386  ################################\n",
      "Loss:  3.8149293686728925e-05\n",
      "################################  1387  ################################\n",
      "Loss:  3.810707858065143e-05\n",
      "################################  1388  ################################\n",
      "Loss:  3.806295353570022e-05\n",
      "################################  1389  ################################\n",
      "Loss:  3.802102946792729e-05\n",
      "################################  1390  ################################\n",
      "Loss:  3.797814133577049e-05\n",
      "################################  1391  ################################\n",
      "Loss:  3.793685755226761e-05\n",
      "################################  1392  ################################\n",
      "Loss:  3.7892670661676675e-05\n",
      "################################  1393  ################################\n",
      "Loss:  3.7848451029276475e-05\n",
      "################################  1394  ################################\n",
      "Loss:  3.780334009206854e-05\n",
      "################################  1395  ################################\n",
      "Loss:  3.775494042201899e-05\n",
      "################################  1396  ################################\n",
      "Loss:  3.770850889850408e-05\n",
      "################################  1397  ################################\n",
      "Loss:  3.765818473766558e-05\n",
      "################################  1398  ################################\n",
      "Loss:  3.761121479328722e-05\n",
      "################################  1399  ################################\n",
      "Loss:  3.7562382203759626e-05\n",
      "################################  1400  ################################\n",
      "Loss:  3.751337499124929e-05\n",
      "################################  1401  ################################\n",
      "Loss:  3.746574657270685e-05\n",
      "################################  1402  ################################\n",
      "Loss:  3.7414713006000966e-05\n",
      "################################  1403  ################################\n",
      "Loss:  3.7367379263741896e-05\n",
      "################################  1404  ################################\n",
      "Loss:  3.7319772673072293e-05\n",
      "################################  1405  ################################\n",
      "Loss:  3.72747854271438e-05\n",
      "################################  1406  ################################\n",
      "Loss:  3.722952533280477e-05\n",
      "################################  1407  ################################\n",
      "Loss:  3.717995423357934e-05\n",
      "################################  1408  ################################\n",
      "Loss:  3.7131609133211896e-05\n",
      "################################  1409  ################################\n",
      "Loss:  3.7083438655827194e-05\n",
      "################################  1410  ################################\n",
      "Loss:  3.70369634765666e-05\n",
      "################################  1411  ################################\n",
      "Loss:  3.6992645618738607e-05\n",
      "################################  1412  ################################\n",
      "Loss:  3.694615588756278e-05\n",
      "################################  1413  ################################\n",
      "Loss:  3.690518860821612e-05\n",
      "################################  1414  ################################\n",
      "Loss:  3.685997944558039e-05\n",
      "################################  1415  ################################\n",
      "Loss:  3.6822588299401104e-05\n",
      "################################  1416  ################################\n",
      "Loss:  3.679079236462712e-05\n",
      "################################  1417  ################################\n",
      "Loss:  3.675562402349897e-05\n",
      "################################  1418  ################################\n",
      "Loss:  3.671826925710775e-05\n",
      "################################  1419  ################################\n",
      "Loss:  3.667664714157581e-05\n",
      "################################  1420  ################################\n",
      "Loss:  3.663072129711509e-05\n",
      "################################  1421  ################################\n",
      "Loss:  3.658120840555057e-05\n",
      "################################  1422  ################################\n",
      "Loss:  3.6528665077639744e-05\n",
      "################################  1423  ################################\n",
      "Loss:  3.647959601948969e-05\n",
      "################################  1424  ################################\n",
      "Loss:  3.643352829385549e-05\n",
      "################################  1425  ################################\n",
      "Loss:  3.638866473920643e-05\n",
      "################################  1426  ################################\n",
      "Loss:  3.634706081356853e-05\n",
      "################################  1427  ################################\n",
      "Loss:  3.6307239497546107e-05\n",
      "################################  1428  ################################\n",
      "Loss:  3.6269739212002605e-05\n",
      "################################  1429  ################################\n",
      "Loss:  3.623884549597278e-05\n",
      "################################  1430  ################################\n",
      "Loss:  3.6204415664542466e-05\n",
      "################################  1431  ################################\n",
      "Loss:  3.617737820604816e-05\n",
      "################################  1432  ################################\n",
      "Loss:  3.614932211348787e-05\n",
      "################################  1433  ################################\n",
      "Loss:  3.612027649069205e-05\n",
      "################################  1434  ################################\n",
      "Loss:  3.609342820709571e-05\n",
      "################################  1435  ################################\n",
      "Loss:  3.606358586694114e-05\n",
      "################################  1436  ################################\n",
      "Loss:  3.6037959944223985e-05\n",
      "################################  1437  ################################\n",
      "Loss:  3.60104204446543e-05\n",
      "################################  1438  ################################\n",
      "Loss:  3.5981356631964445e-05\n",
      "################################  1439  ################################\n",
      "Loss:  3.59418336302042e-05\n",
      "################################  1440  ################################\n",
      "Loss:  3.591195127228275e-05\n",
      "################################  1441  ################################\n",
      "Loss:  3.586681486922316e-05\n",
      "################################  1442  ################################\n",
      "Loss:  3.58211254933849e-05\n",
      "################################  1443  ################################\n",
      "Loss:  3.5766228393185884e-05\n",
      "################################  1444  ################################\n",
      "Loss:  3.571885827113874e-05\n",
      "################################  1445  ################################\n",
      "Loss:  3.5668370401253924e-05\n",
      "################################  1446  ################################\n",
      "Loss:  3.560987170203589e-05\n",
      "################################  1447  ################################\n",
      "Loss:  3.554631257429719e-05\n",
      "################################  1448  ################################\n",
      "Loss:  3.548083623172715e-05\n",
      "################################  1449  ################################\n",
      "Loss:  3.541930709616281e-05\n",
      "################################  1450  ################################\n",
      "Loss:  3.536073927534744e-05\n",
      "################################  1451  ################################\n",
      "Loss:  3.530597314238548e-05\n",
      "################################  1452  ################################\n",
      "Loss:  3.5253306123195216e-05\n",
      "################################  1453  ################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.520537939039059e-05\n",
      "################################  1454  ################################\n",
      "Loss:  3.515887146932073e-05\n",
      "################################  1455  ################################\n",
      "Loss:  3.511147224344313e-05\n",
      "################################  1456  ################################\n",
      "Loss:  3.5063920222455636e-05\n",
      "################################  1457  ################################\n",
      "Loss:  3.50198351952713e-05\n",
      "################################  1458  ################################\n",
      "Loss:  3.497519719530828e-05\n",
      "################################  1459  ################################\n",
      "Loss:  3.493182884994894e-05\n",
      "################################  1460  ################################\n",
      "Loss:  3.488742368062958e-05\n",
      "################################  1461  ################################\n",
      "Loss:  3.483679029159248e-05\n",
      "################################  1462  ################################\n",
      "Loss:  3.478355938568711e-05\n",
      "################################  1463  ################################\n",
      "Loss:  3.473037213552743e-05\n",
      "################################  1464  ################################\n",
      "Loss:  3.467557326075621e-05\n",
      "################################  1465  ################################\n",
      "Loss:  3.4620275982888415e-05\n",
      "################################  1466  ################################\n",
      "Loss:  3.456546983215958e-05\n",
      "################################  1467  ################################\n",
      "Loss:  3.450722215347923e-05\n",
      "################################  1468  ################################\n",
      "Loss:  3.445254696998745e-05\n",
      "################################  1469  ################################\n",
      "Loss:  3.4394626709399745e-05\n",
      "################################  1470  ################################\n",
      "Loss:  3.4340024285484105e-05\n",
      "################################  1471  ################################\n",
      "Loss:  3.4276818041689694e-05\n",
      "################################  1472  ################################\n",
      "Loss:  3.4213422622997314e-05\n",
      "################################  1473  ################################\n",
      "Loss:  3.412525984458625e-05\n",
      "################################  1474  ################################\n",
      "Loss:  3.406785617698915e-05\n",
      "################################  1475  ################################\n",
      "Loss:  3.4003031032625586e-05\n",
      "################################  1476  ################################\n",
      "Loss:  3.39227553922683e-05\n",
      "################################  1477  ################################\n",
      "Loss:  3.386020762263797e-05\n",
      "################################  1478  ################################\n",
      "Loss:  3.379067129571922e-05\n",
      "################################  1479  ################################\n",
      "Loss:  3.371445200173184e-05\n",
      "################################  1480  ################################\n",
      "Loss:  3.363657378940843e-05\n",
      "################################  1481  ################################\n",
      "Loss:  3.357059904374182e-05\n",
      "################################  1482  ################################\n",
      "Loss:  3.349944745423272e-05\n",
      "################################  1483  ################################\n",
      "Loss:  3.344251308590174e-05\n",
      "################################  1484  ################################\n",
      "Loss:  3.338079113746062e-05\n",
      "################################  1485  ################################\n",
      "Loss:  3.331373955006711e-05\n",
      "################################  1486  ################################\n",
      "Loss:  3.324929639347829e-05\n",
      "################################  1487  ################################\n",
      "Loss:  3.318785456940532e-05\n",
      "################################  1488  ################################\n",
      "Loss:  3.313174602226354e-05\n",
      "################################  1489  ################################\n",
      "Loss:  3.307762017357163e-05\n",
      "################################  1490  ################################\n",
      "Loss:  3.302622644696385e-05\n",
      "################################  1491  ################################\n",
      "Loss:  3.297656076028943e-05\n",
      "################################  1492  ################################\n",
      "Loss:  3.292977635283023e-05\n",
      "################################  1493  ################################\n",
      "Loss:  3.2887353881960735e-05\n",
      "################################  1494  ################################\n",
      "Loss:  3.284247577539645e-05\n",
      "################################  1495  ################################\n",
      "Loss:  3.280318924225867e-05\n",
      "################################  1496  ################################\n",
      "Loss:  3.276465577073395e-05\n",
      "################################  1497  ################################\n",
      "Loss:  3.272827234468423e-05\n",
      "################################  1498  ################################\n",
      "Loss:  3.269349326728843e-05\n",
      "################################  1499  ################################\n",
      "Loss:  3.265884151915088e-05\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1500\n",
    "history = fit(my_network, training_set, interior, n_epochs, optimizer_, p=2, verbose=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading Model\n",
    "FILE = \"maxwell2D_1.pth\"\n",
    "torch.save(my_network, FILE)\n",
    "\n",
    "# uncomment below when you need to test for different points\n",
    "#my_network = torch.load(FILE)\n",
    "#my_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading Model (this model with reported results)\n",
    "#FILE = \"second.pth\"\n",
    "#torch.save(my_network, FILE)\n",
    "\n",
    "# uncomment below when you need to test for different points\n",
    "#my_network = torch.load(FILE)\n",
    "#my_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(0, 1, 10000).reshape(-1,1)\n",
    "y_test = torch.linspace(0, 1, 10000).reshape(-1,1)\n",
    "t_test = torch.ones((10000,1))\n",
    "test = torch.cat([x_test, y_test, t_test],1)\n",
    "h_test = exact_solution_h(x_test, y_test, t_test).reshape(-1,1)\n",
    "e1_test = exact_solution_e1(x_test, y_test, t_test).reshape(-1,1)\n",
    "e2_test = exact_solution_e2(x_test, y_test, t_test).reshape(-1,1)\n",
    "w_test_pred = my_network(test)\n",
    "h_test_pred = w_test_pred[:,0].reshape(-1,1)\n",
    "e1_test_pred = w_test_pred[:,1].reshape(-1,1)\n",
    "e2_test_pred = w_test_pred[:,2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f42d015e850>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(x_test, h_test, label=\"Ground Truth\",lw=2)\n",
    "plt.plot(x_test, h_test_pred.detach(), label=\"Network Prediction\",lw=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"u\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  0.004954184259986505 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((h_test_pred - h_test)**2)/torch.mean(h_test**2)\n",
    "#relative_error_test = torch.max(torch.abs(u_test_pred -u_test))/torch.max(torch.abs(u_test))\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f42d0093220>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(x_test, e1_test, label=\"Ground Truth\",lw=2)\n",
    "plt.plot(x_test, e1_test_pred.detach(), label=\"Network Prediction\",lw=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"u\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  0.0004488900231081061 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((e1_test_pred - e1_test)**2)/torch.mean(e1_test**2)\n",
    "#relative_error_test = torch.max(torch.abs(u_test_pred -u_test))/torch.max(torch.abs(u_test))\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Error Test:  0.0015908222849247977 %\n"
     ]
    }
   ],
   "source": [
    "# Compute the relative L2 error norm (generalization error)\n",
    "relative_error_test = torch.mean((e2_test_pred - e2_test)**2)/torch.mean(e2_test**2)\n",
    "#relative_error_test = torch.max(torch.abs(u_test_pred -u_test))/torch.max(torch.abs(u_test))\n",
    "print(\"Relative Error Test: \", relative_error_test.detach().numpy()*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exact_solution_u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m t_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m100000\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_test, t_test],\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m u_test \u001b[38;5;241m=\u001b[39m \u001b[43mexact_solution_u\u001b[49m(x_test,t_test)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m p_test \u001b[38;5;241m=\u001b[39m exact_solution_p(x_test,t_test)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m u_test_pred \u001b[38;5;241m=\u001b[39m my_network(test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exact_solution_u' is not defined"
     ]
    }
   ],
   "source": [
    "model = my_network\n",
    "x_test = pi*torch.rand(100000).reshape(-1,1)\n",
    "t_test = torch.rand(100000).reshape(-1,1)\n",
    "test = torch.cat([x_test, t_test],1)\n",
    "u_test = exact_solution_u(x_test,t_test).reshape(-1,1)\n",
    "p_test = exact_solution_p(x_test,t_test).reshape(-1,1)\n",
    "u_test_pred = my_network(test)\n",
    "u_pred = u_test_pred[:, 0].reshape(-1,1)\n",
    "\n",
    "u_pred1 = u_test_pred[:, 1].reshape(-1,1)\n",
    "\n",
    "\n",
    "relative_error = torch.abs(u_pred- u_test)\n",
    "\n",
    "relative_error1 = torch.abs(u_pred1- p_test)\n",
    "u_pred = u_pred.detach().numpy()\n",
    "x_test = x_test.detach().numpy()\n",
    "t_test = t_test.detach().numpy()\n",
    "p_pred = u_pred1.detach().numpy()\n",
    "relative_error = relative_error.detach().numpy()\n",
    "relative_error1 = relative_error1.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = x_test.reshape(-1,)\n",
    "t_test = t_test.reshape(-1,)\n",
    "\n",
    "u_pred = u_pred.reshape(-1,)\n",
    "p_pred = p_pred.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "\n",
    "\n",
    "CS = plt.tricontourf(x_test, t_test, u_pred, 20, cmap='turbo')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(CS)\n",
    "for t in cbar.ax.get_yticklabels():\n",
    "     t.set_fontsize(20)\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('t', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "\n",
    "#plt.savefig('timo_u.png', dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CS = plt.tricontourf(x_test, t_test, p_pred, 20, cmap='turbo')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(CS)\n",
    "for t in cbar.ax.get_yticklabels():\n",
    "     t.set_fontsize(20)\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('t', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "#plt.savefig('timo_p.png', dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_test = x_test.reshape(-1, )\n",
    "t_test = t_test.reshape(-1, )\n",
    "relative_error = relative_error.reshape(-1,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CS = plt.tricontourf(x_test, t_test, relative_error, 20, cmap='turbo')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(CS)\n",
    "for t in cbar.ax.get_yticklabels():\n",
    "     t.set_fontsize(20)\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('t', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "#plt.savefig('relative_error_timo_u.png', dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error1 = relative_error1.reshape(-1,)\n",
    "\n",
    "CS = plt.tricontourf(x_test, t_test, relative_error1, 20, cmap='turbo')\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(CS)\n",
    "for t in cbar.ax.get_yticklabels():\n",
    "     t.set_fontsize(20)\n",
    "\n",
    "\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('t', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "#plt.savefig('relative_error_timo_p.png', dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
